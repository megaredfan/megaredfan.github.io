<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-10-27T18:44:59+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">熊纪元的博客</title><subtitle>think digital and be human</subtitle><entry><title type="html">TCP拥塞控制</title><link href="http://localhost:4000/2019/10/27/TCP-congestion.html" rel="alternate" type="text/html" title="TCP拥塞控制" /><published>2019-10-27T00:00:00+08:00</published><updated>2019-10-27T00:00:00+08:00</updated><id>http://localhost:4000/2019/10/27/TCP%20congestion</id><content type="html" xml:base="http://localhost:4000/2019/10/27/TCP-congestion.html">&lt;h2 id=&quot;tcp拥塞控制的起源&quot;&gt;TCP拥塞控制的起源&lt;/h2&gt;

&lt;p&gt;1986年，从LBL到UC Berkeley的网络吞吐因为拥塞出现了从32Kbps到40bps的急剧下降，Van Jacobson 在1988年的论文《Congestion Avoidance and Control》从这个问题出发，提出了数据包守恒定律以及慢启动、拥塞控制和快重传的算法，在1990年又提出了快恢复算法。&lt;/p&gt;

&lt;h2 id=&quot;数据包守恒原则&quot;&gt;数据包守恒原则&lt;/h2&gt;

&lt;p&gt;在一个运行平稳的TCP连接中流动的数据包应该是守恒的，意思是当只有旧的数据包被成功传输到对端后，新的数据包才能加入到连接中。在TCP协议中，我们可以使用ack来作为判断数据包是否已经成功到达对端的依据，就是说当发送端收到good ack（大于发送端当前已经收到的最大ack的ack）时，它就可以发送新的数据包了。这种根据ack来决定继续发送数据包的机制就叫做self clocking（也叫做ack clocking）。&lt;/p&gt;

&lt;h2 id=&quot;慢启动&quot;&gt;慢启动&lt;/h2&gt;

&lt;p&gt;通过数据包守恒原则，我们知道可以通过ack来决定是否发送新的数据，而要收到ack就要先发送数据。慢启动就开始发送数据时的行为控制。慢启动的总体思路就是从一个很低的初始值开始，逐渐增加数据发送的速度，直到达到超时或者丢包为止。慢启动的实现思路如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;每个连接维护一个变量cwnd（congestion window）&lt;/li&gt;
  &lt;li&gt;当连接刚建立或者遇到丢包时，将cwnd设置为1，单位为MSS（maximum segment size）&lt;/li&gt;
  &lt;li&gt;每收到一个新的ack，cwnd加一&lt;/li&gt;
  &lt;li&gt;当发送数据时，能够发送的数据包数量为min(cwnd,awnd)，awnd为接收端的滑动窗口大小(reciver’s advertised window)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;可以预见，在没有出现超时或者丢包时，慢启动增长的速度是指数级的，所以慢启动实际上并没有那么“慢”，“慢”是慢在它的起点只有1个MSS。&lt;/p&gt;

&lt;h2 id=&quot;拥塞避免&quot;&gt;拥塞避免&lt;/h2&gt;

&lt;p&gt;前面提到，慢启动的目的是逐渐增加发送速度进行试探，直到出现网络拥塞，而真正出现拥塞时又该怎么做呢，就是“拥塞避免”所做的事情了。拥塞避免主要由两部分组成：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;一个判断当前网路出现拥塞的机制&lt;/li&gt;
  &lt;li&gt;在出现拥塞时降低发送速度的机制&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;而拥塞避免的实现思路如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;当出现超时时，将cwnd设置为当前值的一半（即当出现超时时就认为是出现了拥塞）&lt;/li&gt;
  &lt;li&gt;每收到一个新的ack，cwnd加1/cwnd（即当传输成功cwnd个数据包时，窗口大小加一，也就是随着RTT线性增加）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这里的两个变更cwnd的行为通常称为“乘法减小”和“加法增大”。&lt;/p&gt;

&lt;h2 id=&quot;结合慢启动和拥塞避免的算法&quot;&gt;结合慢启动和拥塞避免的算法&lt;/h2&gt;

&lt;p&gt;值得注意的是，慢启动和拥塞避免实际上是两个不同的算法，它们一个用于试探网络资源的上限，另一个用于资源使用率达到或者接近上限时的行为。在1988年的论文中给出了一个结合了慢启动和拥塞避免的算法，具体实现思路如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;发送端维持两个变量：拥塞窗口cwnd（congestion window）和慢启动门限ssthresh（slow start threshold），通过这两个变量来决定当前应该执行慢启动还是拥塞避免算法。&lt;/li&gt;
  &lt;li&gt;发送数据时，能够发送的数据包数量为min(cwnd,awnd)&lt;/li&gt;
  &lt;li&gt;出现超时时，ssthresh更新为cwnd/2（但不能小于2），cwnd设置为1&lt;/li&gt;
  &lt;li&gt;每收到一个新的zck，发送端的行为是:
    &lt;ul&gt;
      &lt;li&gt;如果&lt;code class=&quot;highlighter-rouge&quot;&gt;cwnd&amp;lt;ssthresh&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;cwnd+=1&lt;/code&gt;（慢启动阶段，窗口指数级别增加）&lt;/li&gt;
      &lt;li&gt;否则&lt;code class=&quot;highlighter-rouge&quot;&gt;cwnd+=1/cwdn&lt;/code&gt;（拥塞避免阶段，窗口线性增加）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;快重传&quot;&gt;快重传&lt;/h2&gt;

&lt;p&gt;快速重传的目的就是让发送端尽快感知到丢包。TCP发送方在每发送一个分段时会启动一个计时器，如果相应的数据包确认没在特定时间内被送回，发送方就假设这个分段在网络上丢失了，需要重发。这也是TCP用来估计RTT的测量方法。&lt;/p&gt;

&lt;h3 id=&quot;重复确认&quot;&gt;重复确认&lt;/h3&gt;

&lt;p&gt;重复确认基于以下过程：如果接收方接收到一个数据分段，就会将该分段的序列号加上数据字节长的值，作为分段确认的确认号，发送回发送方，表示期望发送方发送下一个序列号的分段。但是如果接收方提前收到更大的序列号的分段，或者说接收到无序到达的分段，接收方需要&lt;strong&gt;立即&lt;/strong&gt;使用之前的确认号发送分段确认。此时如果发送方收到接收方相同确认号的分段确认超过1次，并且该对应序列号的分段超时计时器仍没超时的话，则这就是出现重复确认，需要进入快速重传。&lt;/p&gt;

&lt;p&gt;快送重传就是基于以下机制：如果假设重复阈值为3，当发送方收到4次相同确认号的分段确认（第1次收到确认期望序列号，加3次重复的期望序列号确认）时，则可以认为继续发送更高序列号的分段将会被接受方丢弃，而且会无法有序送达。发送方应该忽略超时计时器的等待重发，立即重发重复分段确认中确认号对应序列号的分段。&lt;/p&gt;

&lt;h2 id=&quot;tcp拥塞控制的各个实现&quot;&gt;TCP拥塞控制的各个实现&lt;/h2&gt;

&lt;p&gt;这里先列举各个TCP拥塞控制的实现，具体接受后续再补上了。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TCP Tahoe/Reno&lt;/li&gt;
  &lt;li&gt;TCP Vegas&lt;/li&gt;
  &lt;li&gt;TCP New Reno&lt;/li&gt;
  &lt;li&gt;TCP BIC/CUBIC&lt;/li&gt;
  &lt;li&gt;TCP Westwood/Westwood+&lt;/li&gt;
  &lt;li&gt;Compound TCP&lt;/li&gt;
  &lt;li&gt;TCP PRR&lt;/li&gt;
  &lt;li&gt;TCP BBR&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">TCP拥塞控制的起源</summary></entry><entry><title type="html">java内存模型与volatile</title><link href="http://localhost:4000/2019/10/20/volatile.html" rel="alternate" type="text/html" title="java内存模型与volatile" /><published>2019-10-20T00:00:00+08:00</published><updated>2019-10-20T00:00:00+08:00</updated><id>http://localhost:4000/2019/10/20/volatile</id><content type="html" xml:base="http://localhost:4000/2019/10/20/volatile.html">&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;在计算机硬件结构中，为了平衡cpu和内存之间由于速度带来的差距，cpu中引入了cache作为处理器与内存之间的缓冲。在多核的处理器中，每个核都有属于自己的cache，这就带来了cache一致性的问题。前面提到的MESI协议就是用于处理cache一致性问题的一个协议，它将cache的内容分成几个状态，并要求每个核监听总线上传来的其他核发出的事件，根据这些外部事件以及自身操作cache的内部事件来维护cache的内容和状态，以达到cache一致性。但MESI协议中特定的优化有时会导致cache中存在临时的不一致的数据，所以引入了内存屏障来规避这个问题。&lt;/p&gt;

&lt;p&gt;即使有cache的存在，当处理器等待cache的载入时仍然会浪费时间。所以处理器会在当前指令因等待数据阻塞时尝试执行其他不依赖这个数据的指令，来尽可能提高处理速度，这称为乱序执行。处理器会保证乱序执行的结果与顺序执行的结果一致，但仅在当前处理器范围内。如果有其他任务的计算依赖当前任务的中间结果，就有可能出现不符合预期的结果，这个问题同样可以通过内存屏障来规避。&lt;/p&gt;

&lt;h2 id=&quot;java的内存模型&quot;&gt;java的内存模型&lt;/h2&gt;

&lt;p&gt;java虚拟机规范中定义了java自身的内存模型，通过这个内存模型来屏蔽不同的操作系统和硬件带来的差异，达到各个平台运行效果一致的目标。java内存模型规定所有的变量都存储在主内存中，每个线程有自己的工作内存，线程在访问变量时都直接从工作内存中访问，而不能访问主内存。一个线程不能访问其他的线程的工作内存，线程之间的变量传递都需要经过主内存来完成。这里的线程、工作内存和主内存有有点类似计算机硬件结构中的处理器、cache和内存的关系。此外，java虚拟机中的即时编译中也有类似指令重排序的优化。java内存模型的介绍比较多，这里就不详细展开了。&lt;/p&gt;

&lt;h2 id=&quot;volatile变量&quot;&gt;volatile变量&lt;/h2&gt;

&lt;p&gt;在java中有一个用于实现单例模式的方式，叫做“双成例检查”。双成例检查利用了synchronized和volatile关键词保证了在并发执行的情况下单例模式的正确性。但是在jdk1.5以前（不包括1.5）的版本是存在问题的，其中具体的原因就是volatile关键词底层实现在jdk1.5才完全正确。&lt;/p&gt;

&lt;p&gt;根据volatile的特性，如果一个变量被标记为volatile，那么它将获得两个额外的属性：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;在一个线程中对于volatile变量的修改会立即被其他线程感知到，也就是可见性。前面提到，在java内存模型中，各个线程之间的变量传递都需要先经过主内存，所以为了性能考虑，线程不会总是从主内存获取最新的变量的值，而是在特定的时机才从主内存同步最新的内容。而volatile关键词则能够强制触发其他线程同步主内存的内容。&lt;/li&gt;
  &lt;li&gt;禁止指令重排序。对于一个普通的变量，只会保证所有依赖这个变量的地方都能获得正确的结果，而并不会保证对这个变量赋值的顺序和实际的代码执行顺序一致，比如不依赖这个变量的代码可能会被挪到之前或者之后执行，也就是“看起来就像是顺序执行一样”。而volatile关键词能够禁止指令重排序。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在jdk1.5之前的版本，volatile并没有禁止指令重排序的作用，所以即使把变量声明为volatile也会存在volatile变量前后的代码重排序的情况，这也是在jdk1.5之前不能使用双成例检查来实现单例的原因。&lt;/p&gt;

&lt;h2 id=&quot;volatile的实现&quot;&gt;volatile的实现&lt;/h2&gt;

&lt;p&gt;前面提到内存屏障能够避免cache中存在过期数据以及避免乱序执行，而volatile自身也是通过内存屏障来实现上述的2个特性的。&lt;/p&gt;

&lt;p&gt;内存屏障通常分为几个级别：读写（保证屏障前的读写操作都早于屏障后的读写操作）、读（只保证读操作）以及写（只保证写操作）。不同体系结构的硬件对内存屏障的实现都不一样，比如在x86中内存屏障的指令是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;lfence 读操作屏障&lt;/li&gt;
  &lt;li&gt;sfence 写操作屏障&lt;/li&gt;
  &lt;li&gt;mfence 读写操作屏障&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;而当我们把实际的java字节码反汇编成汇编指令时，可以看到并没有这几个屏障，而是在写入volatile变量之后添加一条&lt;code class=&quot;highlighter-rouge&quot;&gt;lock addl $0, 0 (%esp)&lt;/code&gt;指令。lock指令的作用是可以使当前处理器的cache内容被写入内存，同时使其他处理器的cache失效，这种操作相当于将本线程的工作内存的内容同步到主内存，也就保证了可见性。而在指令重排序的角度，由于lock指令之前的操作的结果都同步到了内存，也就相当于lock之前的操作都已经完成，这样就相当于“屏障后边的操作无法穿越到屏障前面”的效果。&lt;/p&gt;

&lt;h3 id=&quot;lock实际的作用&quot;&gt;lock实际的作用&lt;/h3&gt;

&lt;p&gt;可以看到，lock实际上具备了内存屏障的语义，那lock具体的作用是什么呢。lock是一个指令前缀，在它后面的指令会保证原子执行。其实现方式就是在指令执行期间设置处理器的&lt;code class=&quot;highlighter-rouge&quot;&gt;LOCK#&lt;/code&gt;信号，这样就能确保处理器能够互斥的操作内存（通过锁定总线来实现），当指令执行完毕之后&lt;code class=&quot;highlighter-rouge&quot;&gt;LOCK#信号&lt;/code&gt;会自动取消。从intel奔腾Pro处理器开始，当要锁定的内存地址已经被加载到cache时，会直接锁定对应的cache而不是设置&lt;code class=&quot;highlighter-rouge&quot;&gt;LOCK#信号&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;也就是说，volatile的实现中通过lock前缀+一条空的指令来锁定cache，实现了可见性和禁止重排序的功能。至于为什么要用&lt;code class=&quot;highlighter-rouge&quot;&gt;addl $0, 0 (%esp)&lt;/code&gt;配合lock前缀是因为lock前缀只支持内存操作类的指令，所以不能直接用lock前缀加空指令nop。&lt;/p&gt;</content><author><name></name></author><summary type="html">前言</summary></entry><entry><title type="html">cache一致性里的MESI协议</title><link href="http://localhost:4000/2019/10/13/MESI.html" rel="alternate" type="text/html" title="cache一致性里的MESI协议" /><published>2019-10-13T00:00:00+08:00</published><updated>2019-10-13T00:00:00+08:00</updated><id>http://localhost:4000/2019/10/13/MESI</id><content type="html" xml:base="http://localhost:4000/2019/10/13/MESI.html">&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;在有多个核的处理器的处理器中，每个核都有自己的cache，而如何确保多个核的cache内容的一致则是一个很容易遇到的问题，MESI协议就是一个专门用来解决cache一致性的协议。很多处理器使用的都是MESI协议或者MESI协议的变体，而MESI协议其实也是MSI协议的变种。MESI协议采用了回写（write-back）的策略来更新cache，使得其性能进一步提高，但也带来了额外的风险，回写带来的问题可以在编写程序时使用内存屏障来规避。&lt;/p&gt;

&lt;h2 id=&quot;mesi协议简介&quot;&gt;MESI协议简介&lt;/h2&gt;

&lt;p&gt;MESI协议名字的由来是由其描述的四个cache状态组成的，分别是M(modified)、E(exclusive)、S(shared)和I(invalid)。各个状态的描述具体如下：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;状态&lt;/th&gt;
      &lt;th&gt;描述&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Modified&lt;/td&gt;
      &lt;td&gt;当前cache的内容有效，数据已被修改而且与内存中的数据不一致，数据只在当前cache里存在&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Exclusive&lt;/td&gt;
      &lt;td&gt;当前cache的内容有效，数据与内存中的数据一致，数据只在当前cache里存在&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Shared&lt;/td&gt;
      &lt;td&gt;当前cache的内容有效，数据与内存中的数据一致，数据在多个cache里存在&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Invalid&lt;/td&gt;
      &lt;td&gt;当前cache无效&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;状态转移&quot;&gt;状态转移&lt;/h3&gt;

&lt;p&gt;MESI协议其实是一个状态机，cache的状态会跟根据外部事件的刺激而发生转移，具体的事件分为两类：处理器对cache的请求和总线对cache的请求：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;PrRd: 处理器请求读一个缓存块&lt;/li&gt;
  &lt;li&gt;PrWr: 处理器请求写一个缓存块&lt;/li&gt;
  &lt;li&gt;BusRd: 窥探器请求指出其他处理器请求读一个缓存块&lt;/li&gt;
  &lt;li&gt;BusRdX: 窥探器请求指出其他处理器请求写一个该处理器不拥有的缓存块&lt;/li&gt;
  &lt;li&gt;BusUpgr: 窥探器请求指出其他处理器请求写一个该处理器拥有的缓存块&lt;/li&gt;
  &lt;li&gt;Flush: 窥探器请求指出请求回写整个缓存到主存&lt;/li&gt;
  &lt;li&gt;FlushOpt: 窥探器请求指出整个缓存块被发到总线以发送给另外一个处理器（缓存到缓存的复制）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;而状态之间的转换如下图：
&lt;img src=&quot;/img/Diagrama_MESI.gif&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
   &lt;caption&gt;处理器操作带来的状态转化&lt;/caption&gt;
   &lt;thead&gt;
      &lt;tr&gt;
         &lt;th&gt;初始状态&lt;/th&gt;
         &lt;th&gt;操作&lt;/th&gt;
         &lt;th&gt;响应&lt;/th&gt;
      &lt;/tr&gt;
   &lt;/thead&gt;
   &lt;tbody&gt;
      &lt;tr&gt;
         &lt;td rowspan=&quot;2&quot;&gt;Invalid(I)&lt;/td&gt;
         &lt;td&gt;PrRd&lt;/td&gt;
         &lt;td&gt;
            &lt;ul&gt;
               &lt;li&gt;给总线发BusRd信号&lt;/li&gt;
               &lt;li&gt;其他处理器看到BusRd，检查自己是否有有效的数据副本，通知发出请求的缓存&lt;/li&gt;
               &lt;li&gt;如果其他缓存有有效的副本，状态转换为(S)&lt;b&gt;Shared&lt;/b&gt;&lt;/li&gt;
               &lt;li&gt;如果其他缓存都没有有效的副本，状态转换为(E)&lt;b&gt;Exclusive&lt;/b&gt;&lt;/li&gt;
               &lt;li&gt;如果其他缓存有有效的副本, 其中一个缓存发出数据；否则从主存获得数据&lt;/li&gt;
            &lt;/ul&gt;
         &lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
         &lt;td&gt;PrWr&lt;/td&gt;
         &lt;td&gt;
            &lt;ul&gt;
               &lt;li&gt;给总线发BusRdX信号&lt;/li&gt;
               &lt;li&gt;状态转换为(M)&lt;b&gt;Modified&lt;/b&gt;&lt;/li&gt;
               &lt;li&gt;如果其他缓存有有效的副本, 其中一个缓存发出数据；否则从主存获得数据&lt;/li&gt;
               &lt;li&gt;如果其他缓存有有效的副本, 见到BusRdX信号后无效其副本&lt;/li&gt;
               &lt;li&gt;向缓存块中写入修改后的值&lt;/li&gt;
            &lt;/ul&gt;
         &lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
         &lt;td rowspan=&quot;2&quot;&gt;Exclusive(E)&lt;/td&gt;
         &lt;td&gt;PrRd&lt;/td&gt;
         &lt;td&gt;
            &lt;ul&gt;
               &lt;li&gt;无总线事务生成&lt;/li&gt;
               &lt;li&gt;状态保持不变&lt;/li&gt;
               &lt;li&gt;读操作为缓存命中&lt;/li&gt;
            &lt;/ul&gt;
         &lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
         &lt;td&gt;PrWr&lt;/td&gt;
         &lt;td&gt;
            &lt;ul&gt;
               &lt;li&gt;无总线事务生成&lt;/li&gt;
               &lt;li&gt;状态转换为(M)&lt;b&gt;Modified&lt;/b&gt;&lt;/li&gt;
               &lt;li&gt;向缓存块中写入修改后的值&lt;/li&gt;
            &lt;/ul&gt;
         &lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
         &lt;td rowspan=&quot;2&quot;&gt;Shared(S)&lt;/td&gt;
         &lt;td&gt;PrRd&lt;/td&gt;
         &lt;td&gt;
            &lt;ul&gt;
               &lt;li&gt;无总线事务生成&lt;/li&gt;
               &lt;li&gt;状态保持不变&lt;/li&gt;
               &lt;li&gt;读操作为缓存命中&lt;/li&gt;
            &lt;/ul&gt;
         &lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
         &lt;td&gt;PrWr&lt;/td&gt;
         &lt;td&gt;
            &lt;ul&gt;
               &lt;li&gt;发出总线事务BusUpgr信号&lt;/li&gt;
               &lt;li&gt;状态转换为(M)&lt;b&gt;Modified&lt;/b&gt;&lt;/li&gt;
               &lt;li&gt;其他缓存看到BusUpgr总线信号，标记其副本为(I)Invalid.&lt;/li&gt;
            &lt;/ul&gt;
         &lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
         &lt;td rowspan=&quot;2&quot;&gt;Modified(M)&lt;/td&gt;
         &lt;td&gt;PrRd&lt;/td&gt;
         &lt;td&gt;
            &lt;ul&gt;
               &lt;li&gt;无总线事务生成&lt;/li&gt;
               &lt;li&gt;状态保持不变&lt;/li&gt;
               &lt;li&gt;读操作为缓存命中&lt;/li&gt;
            &lt;/ul&gt;
         &lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
         &lt;td&gt;PrWr&lt;/td&gt;
         &lt;td&gt;
            &lt;ul&gt;
               &lt;li&gt;无总线事务生成&lt;/li&gt;
               &lt;li&gt;状态保持不变&lt;/li&gt;
               &lt;li&gt;写操作为缓存命中&lt;/li&gt;
            &lt;/ul&gt;
         &lt;/td&gt;
      &lt;/tr&gt;
   &lt;/tbody&gt;
   &lt;tfoot&gt;&lt;/tfoot&gt;
&lt;/table&gt;

&lt;table&gt;
   &lt;caption&gt;不同总线操作带来的状态转化&lt;/caption&gt;
   &lt;thead&gt;
      &lt;tr&gt;
         &lt;th&gt;初始状态&lt;/th&gt;
         &lt;th&gt;操作&lt;/th&gt;
         &lt;th&gt;响应&lt;/th&gt;
      &lt;/tr&gt;
   &lt;/thead&gt;
   &lt;tbody&gt;
      &lt;tr&gt;
         &lt;td rowspan=&quot;2&quot;&gt;Invalid(I)&lt;/td&gt;
         &lt;td&gt;BusRd&lt;/td&gt;
         &lt;td&gt;
            &lt;ul&gt;
               &lt;li&gt;状态保持不变，信号忽略&lt;/li&gt;
            &lt;/ul&gt;
         &lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
         &lt;td&gt;BusRdX/BusUpgr&lt;/td&gt;
         &lt;td&gt;
            &lt;ul&gt;
               &lt;li&gt;状态保持不变，信号忽略&lt;/li&gt;
            &lt;/ul&gt;
         &lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
         &lt;td rowspan=&quot;2&quot;&gt;Exclusive(E)&lt;/td&gt;
         &lt;td&gt;BusRd&lt;/td&gt;
         &lt;td&gt;
            &lt;ul&gt;
               &lt;li&gt;状态变为共享&lt;/li&gt;
               &lt;li&gt;发出总线FlushOpt信号并发出块的内容&lt;/li&gt;
            &lt;/ul&gt;
         &lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
         &lt;td&gt;BusRdX&lt;/td&gt;
         &lt;td&gt;
            &lt;ul&gt;
               &lt;li&gt;状态变为无效&lt;/li&gt;
               &lt;li&gt;发出总线FlushOpt信号并发出块的内容&lt;/li&gt;
            &lt;/ul&gt;
         &lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
         &lt;td rowspan=&quot;2&quot;&gt;Shared(S)&lt;/td&gt;
         &lt;td&gt;BusRd&lt;/td&gt;
         &lt;td&gt;
            &lt;ul&gt;
               &lt;li&gt;状态变为共享&lt;/li&gt;
               &lt;li&gt;可能发出总线FlushOpt信号并发出块的内容（设计时决定那个共享的缓存发出数据）&lt;/li&gt;
            &lt;/ul&gt;
         &lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
         &lt;td&gt;BusRdX&lt;/td&gt;
         &lt;td&gt;
            &lt;ul&gt;
               &lt;li&gt;状态变为无效&lt;/li&gt;
               &lt;li&gt;可能发出总线FlushOpt信号并发出块的内容（设计时决定那个共享的缓存发出数据）&lt;/li&gt;
            &lt;/ul&gt;
         &lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
         &lt;td rowspan=&quot;2&quot;&gt;Modified(M)&lt;/td&gt;
         &lt;td&gt;BusRd&lt;/td&gt;
         &lt;td&gt;
            &lt;ul&gt;
               &lt;li&gt;状态变为共享&lt;/li&gt;
               &lt;li&gt;发出总线FlushOpt信号并发出块的内容，接收者为最初发出BusRd的缓存与主存控制器（回写主存）&lt;/li&gt;
            &lt;/ul&gt;
         &lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
         &lt;td&gt;BusRdX&lt;/td&gt;
         &lt;td&gt;
            &lt;ul&gt;
               &lt;li&gt;状态变为无效&lt;/li&gt;
               &lt;li&gt;发出总线FlushOpt信号并发出块的内容，接收者为最初发出BusRd的缓存与主存控制器（回写主存）&lt;/li&gt;
            &lt;/ul&gt;
         &lt;/td&gt;
      &lt;/tr&gt;
   &lt;/tbody&gt;
   &lt;tfoot&gt;&lt;/tfoot&gt;
&lt;/table&gt;

&lt;h3 id=&quot;内存屏障的引入&quot;&gt;内存屏障的引入&lt;/h3&gt;

&lt;p&gt;MESI的设计比较简单直接，但是其中有两个地方会导致性能下降：一是更新invalidate状态的cache时，需要尝试从其他cpu甚至是内存获取最新的数据；二是使一个cache变为invalidate时需要等待其他cpu的确认；这两个操作都是比较耗时的，如果cpu在这两个过程中一直等待的话，就会形成浪费。&lt;/p&gt;

&lt;h3 id=&quot;store-buffer&quot;&gt;store buffer&lt;/h3&gt;

&lt;p&gt;为了降低写入invalidate状态的cache的延时，可以引入store buffer。既然写入操作无论如何一定会发生，那么cpu就先发出信号通知其他cpu这个cache已经失效，然后再将本次的写操作更新到store buffer中，等到其他cpu都确认收到信号后再将结果写到内存中。&lt;/p&gt;

&lt;p&gt;这样就避免了更新cache时阻塞等待其他cpu确认的耗时，但是也会导致cpu的更新并没有及时写入cache，所以当cpu需要读取cache时，它需要先确认store buffer中是否有所需的数据，这个机制成为store forwarding。值得注意的是，当cpu在读写自己的store buffer时，对应的数据变更其他cpu是感知不到的。&lt;/p&gt;

&lt;h3 id=&quot;invalidate-queue&quot;&gt;invalidate queue&lt;/h3&gt;

&lt;p&gt;当cpu收到使某个cache失效的消息时，预期的行为是cpu马上执行这个失效操作。但实际上cpu并不会马上执行失效操作，而是先发送确认收到的消息，然后将失效操作加入到invalidate queue中，queue中的操作随后会在适当的时刻执行（并不一定是马上）。之所以需要invalidate queue同样是因为invalidate操作开销比较大，cpu为了执行invalidate操作必须丢弃cache，导致cache命中率下降。这样的好处是能够提高cpu的性能，但同时也导致cache中可能存在过期的数据。&lt;/p&gt;

&lt;h3 id=&quot;内存屏障&quot;&gt;内存屏障&lt;/h3&gt;

&lt;p&gt;针对store buffer和invalidate queue这两个优化带来的问题，我们又提供了内存屏障作为解决方案。内存屏障交给了编写程序的人的手里，利用它就可以规避上面提到的问题。&lt;/p&gt;

&lt;p&gt;内存屏障分为写屏障和读屏障，编写程序时可以在期望的地方加入内存屏障。写屏障会强制cpu清空store buffer的内容，也就是将所有的变更都写入cache，随后变更也就写入了内存，使其对其他cpu可见；读屏障会强制cpu执行invalidate queue中的所有invalidate操作，使自身的cache内容失效，从而使cpu从内存或者其他cpu中获取最新的cache数据。&lt;/p&gt;

&lt;h2 id=&quot;其他&quot;&gt;其他&lt;/h2&gt;

&lt;p&gt;MESI协议乍一看和java里的内存模型以及volatile关键字有些相似，后续再详细展开了。&lt;/p&gt;</content><author><name></name></author><summary type="html">前言</summary></entry><entry><title type="html">linux perf</title><link href="http://localhost:4000/2019/09/22/Linux-perf.html" rel="alternate" type="text/html" title="linux perf" /><published>2019-09-22T00:00:00+08:00</published><updated>2019-09-22T00:00:00+08:00</updated><id>http://localhost:4000/2019/09/22/Linux%20perf</id><content type="html" xml:base="http://localhost:4000/2019/09/22/Linux-perf.html">&lt;h2 id=&quot;简介&quot;&gt;简介&lt;/h2&gt;

&lt;p&gt;perf是linux系统中提供的性能分析工具，它基于一个叫“Performance counters”的内核子系统实现，同时支持硬件（CPU、PMU(Performance Monitoring Unit)）和软件(软件计数器、tracepoint)层面的性能分析。&lt;/p&gt;

&lt;h3 id=&quot;perf中的事件&quot;&gt;perf中的事件&lt;/h3&gt;

&lt;p&gt;perf与其他性能调优工具一样，都是通过对监测对象进行采样，根据采样点的分布来推断整个程序的行为。通过perf list命令我们可以看到perf支持很多的采样事件，比如branch-misses、cpu-clock等等。perf中预定义的事件属于不同的类型，比如硬件产生的事件（cache 命中/分支miss）和软件产生的事件（context switch/page fault)等等。&lt;/p&gt;

&lt;h3 id=&quot;tracepoint&quot;&gt;tracepoint&lt;/h3&gt;

&lt;p&gt;tracepoint是linux内核中定义的一些hook，如果被开启，它们就会在执行到特定逻辑时被触发，方便其他工具获取系统内部的运行状态等信息，perf就是利用了tracepoint，它会记录和统计tracepoint的各个事件，生成分析报告。&lt;/p&gt;

&lt;h2 id=&quot;使用方式&quot;&gt;使用方式&lt;/h2&gt;

&lt;p&gt;perf 工具的具体使用方式如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;perf [--version] [--help] COMMAND [ARGS]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;其中的COMMAND列表可以通过执行perf –help查看，下面列举几个常用的command。&lt;/p&gt;

&lt;h3 id=&quot;perf-stat&quot;&gt;perf stat&lt;/h3&gt;

&lt;p&gt;perf stat的作用是执行一个命令并收集其运行过程中的各个数据，它可以提供一个程序运行情况的总体概览。比如：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;user@localhost:~$ perf stat hostname
localhost

 Performance counter stats for 'hostname':

          0.313464      task-clock (msec)         #    0.481 CPUs utilized          
                 2      context-switches          #    0.006 M/sec                  
                 0      cpu-migrations            #    0.000 K/sec                  
               153      page-faults               #    0.488 M/sec                  
           896,723      cycles                    #    2.861 GHz                    
           620,709      instructions              #    0.69  insn per cycle         
           121,143      branches                  #  386.465 M/sec                  
             6,247      branch-misses             #    5.16% of all branches        

       0.000651441 seconds time elapsed
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;上面这个例子，通过perf stat运行了hostname命令，并将其运行过程中的一些指标汇总显示了出来，比如task-clock、context-switches等待。默认情况下，perf stat 会输出几个常用的事件的统计，比如：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;task-clock-msecs：cpu 使用率&lt;/li&gt;
  &lt;li&gt;context-switches：进程切换次数&lt;/li&gt;
  &lt;li&gt;page-faults：发生缺页的次数&lt;/li&gt;
  &lt;li&gt;cpu-migrations：表示进程运行过程中发生了多少次CPU迁移，即被调度器从一个CPU转移到另外一个CPU上运行&lt;/li&gt;
  &lt;li&gt;cycles：处理器时钟，一条机器指令可能需要多个cycles&lt;/li&gt;
  &lt;li&gt;instructions: 机器指令数目&lt;/li&gt;
  &lt;li&gt;branches：遇到的分支指令数&lt;/li&gt;
  &lt;li&gt;branch-misses是预测错误的分支指令数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除此之外，我们可以使用-e参数来指定我们感兴趣的事件，比如：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;user@localhost:~$ perf stat -e cache-misses hostname
localhost

 Performance counter stats for 'hostname':

          682      cache-misses                                                

       0.000646676 seconds time elapsed
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;perf-top&quot;&gt;perf top&lt;/h3&gt;

&lt;p&gt;perf top的作用是实时地显示系统当前的性能统计信息。前面的perf stat用于对一个特定的程序进行分析，而某些时候我们可能并不知道是哪个程序影响了系统性能，这时候就可以用perf top来查找可疑的程序。比如：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Samples: 775  of event 'cpu-clock', Event count (approx.): 92931021
Overhead  Shared Object       Symbol
   8.93%  [kernel]            [k] vsnprintf
   7.73%  perf                [.] rb_next
   5.92%  [kernel]            [k] kallsyms_expand_symbol.clone.0
   5.07%  [kernel]            [k] format_decode
   4.59%  [kernel]            [k] number
   3.40%  perf                [.] symbols__insert
   3.03%  libslang.so.2.2.1   [.] SLtt_smart_puts
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;上面的例子显示perf统计了cpu-clock事件的数据，根据比例进行了排序。和perf stat一样，我们可以通过-e参数指定统计其他的事件，比如perf top -e context-switches可以查看进程切换最多的top N个进程。&lt;/p&gt;

&lt;h3 id=&quot;perf-record--perf-report&quot;&gt;perf record &amp;amp; perf report&lt;/h3&gt;

&lt;p&gt;perf record的作用和perf stat类似，它可以运行一个命令并生成统计信息，不过perf record不会将结果显示出来，而是将结果输出到文件中。perf record生成的文件可以用perf report来进行解析。&lt;/p&gt;

&lt;p&gt;perf record还可以通过-g参数，在分析时生成calling graph，帮助定位更上层的逻辑分布。&lt;/p&gt;

&lt;h2 id=&quot;其他&quot;&gt;其他&lt;/h2&gt;

&lt;p&gt;通过例子我们可以发现，perf的分析结果中的Symbol一列显示的都是c语言函数的名字。对于java来说，jit编译产生的函数就会直接显示在symbol里，而不是java的函数名，这时要定位问题就不是那么容易了，我们需要通过额外的手段将symbol和java程序的符号表对应起来，具体后续再讨论了。&lt;/p&gt;</content><author><name></name></author><summary type="html">简介</summary></entry><entry><title type="html">Lettuce学习笔记</title><link href="http://localhost:4000/2019/09/08/Lettuce.html" rel="alternate" type="text/html" title="Lettuce学习笔记" /><published>2019-09-08T00:00:00+08:00</published><updated>2019-09-08T00:00:00+08:00</updated><id>http://localhost:4000/2019/09/08/Lettuce</id><content type="html" xml:base="http://localhost:4000/2019/09/08/Lettuce.html">&lt;h2 id=&quot;lettuce简介&quot;&gt;Lettuce简介&lt;/h2&gt;

&lt;p&gt;Lettuce是一个开源的redis client，由javg编写。其主要的宣传特性是线程安全、支持同步异步以及reactive api、支持单实例redis、redis sentinel、redis cluster等。&lt;/p&gt;

&lt;p&gt;Lettuce的实现主要是基于netty实现了网络层逻辑，通过专门的设计支持在同一个连接上并发的处理多个请求响应。&lt;/p&gt;

&lt;h2 id=&quot;使用实例&quot;&gt;使用实例&lt;/h2&gt;

&lt;p&gt;1、 添加依赖&lt;/p&gt;

&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;dependency&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;groupId&amp;gt;&lt;/span&gt;io.lettuce&lt;span class=&quot;nt&quot;&gt;&amp;lt;/groupId&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;artifactId&amp;gt;&lt;/span&gt;lettuce-core&lt;span class=&quot;nt&quot;&gt;&amp;lt;/artifactId&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;version&amp;gt;&lt;/span&gt;${lettuce.version}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/version&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/dependency&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;2、创建RedisClient并执行命令&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;//创建客户端，也可以与Spring集成&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;RedisClient&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;redisClient&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RedisClient&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;redis://password@localhost:6379/0&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;StatefulRedisConnection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;redisClient&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;connect&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;//获取同步API&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;RedisCommands&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;syncCommands&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;sync&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;syncCommands&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;key&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Hello, Redis!&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;//获取异步API&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;RedisAsyncCommands&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;asyncCommands&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;async&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;asyncCommands&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;key&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;thenAccept&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;the result is &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;  
&lt;span class=&quot;c1&quot;&gt;//获取reactive API&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;RedisReactiveCommands&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reactiveCommands&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;reactive&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;reactiveCommands&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;key&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;subscribe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;the result is &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;//关闭连接和客户端&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;redisClient&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;shutdown&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;statefulredisconnection&quot;&gt;StatefulRedisConnection&lt;/h2&gt;

&lt;p&gt;StatefulRedisConnection是一个接口，它代表客户端与服务端的一个连接，同步、异步和reactive三种api底层都是通过StatefulRedisConnection与服务端通信。&lt;/p&gt;

&lt;h3 id=&quot;连接的创建&quot;&gt;连接的创建&lt;/h3&gt;

&lt;p&gt;连接的创建由RedisClient的connect方法执行，connect方法需要传入编解码所需的codec与服务端的URI。codec可以不需要指定，默认是StringCodec.UTF8。connect方法最终会调用到connectStandaloneAsync方法，在这里执行真正的连接创建逻辑，主要包含三个部分：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;创建DefaultEndpoint对象，DefaultEndpoint包含了连接自身的管理以及向对端发送命令的逻辑。&lt;/li&gt;
  &lt;li&gt;创建StatefulRedisConnectionImpl对象，它是StatefulRedisConnection的实际实现。&lt;/li&gt;
  &lt;li&gt;调用connectStatefulAsync方法进行实际的连接。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;connectStatefulAsync方法中首先会创建ConnectionBuilder对象，其中包含了连接的各个选项，比如连接地址、netty相关的配置等等。接着会通过解析redis uri获得一个socketAddress，然后触发initializeChannelAsync0方法中的netty的connect操作，于是连接就建立了。&lt;/p&gt;

&lt;h3 id=&quot;连接状态的管理&quot;&gt;连接状态的管理&lt;/h3&gt;

&lt;p&gt;lettuce是基于netty实现的，所以它在netty的channel上添加了几个自定义的handler，其中具体包括：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;ChannelGroupHandler，用于维护channelGroup的状态，主要逻辑就是在连接建立或者断开时将对应的连接添加到channelGroup或者从channelGroup中移除&lt;/li&gt;
  &lt;li&gt;CommandEncoder，用于redis command的编码&lt;/li&gt;
  &lt;li&gt;CommandHandler，lettuce的核心handler，主要负责redis command发送以及服务端响应的解析&lt;/li&gt;
  &lt;li&gt;ConnectionWatchdog，用于监控连接状态和连接的自动重连&lt;/li&gt;
  &lt;li&gt;ConnectionEventTrigger，用于转发channel的状态事件，对外暴露出listener，用户可以借此监听连接变化&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;连接状态的管理主要有ChannelGroupHandler与ConnectionWatchDog实现，其中ChannelGroupHandler的逻辑比较简单，ConnectionWatchDog的实现主要是监听channelInactive事件，然后提交一个重连任务到线程池。&lt;/p&gt;

&lt;p&gt;此外，DefaultEndpoint也会监听连接状态的变化，实际上整个通信过程中与服务端的连接由DefaultEndpoint对象持有，当发生连接变化（比如断开或者重连）时，DefaultEndpoint会根据连接的状态执行不同的逻辑，比如：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;当连接建立时，将持有的连接对象替换成新建立的连接&lt;/li&gt;
  &lt;li&gt;当发送数据时，先判断连接的状态，如果连接正常则直接通过连接发送；否则将数据添加到自身的缓冲里&lt;/li&gt;
  &lt;li&gt;当连接重连时，将缓冲的数据发送出去，并将持有的连接对象替换成新建立的连接&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;commandhandler&quot;&gt;CommandHandler&lt;/h2&gt;

&lt;p&gt;CommandHandler是lettuce的核心部分，它自身也是一个channel handler，拦截了redis command的write和服务端响应的read等逻辑。&lt;/p&gt;

&lt;h3 id=&quot;发送command&quot;&gt;发送command&lt;/h3&gt;

&lt;p&gt;CommandHandler内部维护了一个command queue，所有发出的command都在这个queue中保存。当执行命令时，CommandHandler会拦截write方法，将其保存到queue中。&lt;/p&gt;

&lt;h3 id=&quot;读取响应&quot;&gt;读取响应&lt;/h3&gt;

&lt;p&gt;当收到来自服务端的响应时，CommandHandler就从queue head取出command然后进行解析，解析成功后command就执行完毕，这时将其从queue中移除即可，如果还有剩余的数据，就继续从queue中取出command进行解析。值得注意的是CommandHandler采用了buffer的方案来处理可能的TCP粘包拆包问题，其内部维护了一个单独的buffer，每次接收到服务端的数据时并不直接进行解析，而是先将其写入到自身的buffer中，然后对buffer的数据进行解析，而每次解析时会尽可能的消费buffer中的所有数据。&lt;/p&gt;

&lt;h2 id=&quot;command的编解码&quot;&gt;command的编解码&lt;/h2&gt;

&lt;p&gt;command的编码在CommandEncoder中触发，其逻辑也比较简单，就是直接调用了command的encode方法。对于服务端响应的解析则直接位于CommandHandler的channelRead方法中。command的编解码也就是直接实现了redis的通信协议，具体可以参考：&lt;a href=&quot;http://redisdoc.com/topic/protocol.html&quot;&gt;redis通信协议&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;api的实现&quot;&gt;API的实现&lt;/h2&gt;

&lt;h3 id=&quot;异步api&quot;&gt;异步API&lt;/h3&gt;

&lt;p&gt;在对外API的实现方面，主要逻辑由以下几步构成：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;组装命令对应的Command对象，包括命令的类型、命令的参数、命令的返回类型等&lt;/li&gt;
  &lt;li&gt;通过StatefulRedisConnection的dispatch方法发送命令，dispatch方法实际上也就是调用了channel的write方法&lt;/li&gt;
  &lt;li&gt;返回对应的Command对象&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在异步API的实现中，返回的是一个AsyncCommand对象，AsyncCommand继承了CompletableFuture类型，所以可以作为一个Future返回。AsyncCommand内维护了原始的命令以及返回值的占位符，当解析完成后AsyncCommand就会调用其自身的compelte方法，调用就完成了。&lt;/p&gt;

&lt;h3 id=&quot;同步api&quot;&gt;同步API&lt;/h3&gt;

&lt;p&gt;同步API的实现实际上就是封装了异步调用的API，在返回前阻塞调用Future的get方法。&lt;/p&gt;

&lt;h3 id=&quot;reactive-api&quot;&gt;reactive API&lt;/h3&gt;

&lt;p&gt;reactive API的实现底层也是调用了StatefulRedisConnection的dispatch方法发送命令，然后将返回包装成一个Publisher。&lt;/p&gt;</content><author><name></name></author><summary type="html">Lettuce简介</summary></entry><entry><title type="html">quatrz</title><link href="http://localhost:4000/2019/09/01/Schedule-Task.html" rel="alternate" type="text/html" title="quatrz" /><published>2019-09-01T00:00:00+08:00</published><updated>2019-09-01T00:00:00+08:00</updated><id>http://localhost:4000/2019/09/01/Schedule%20Task</id><content type="html" xml:base="http://localhost:4000/2019/09/01/Schedule-Task.html">&lt;h2 id=&quot;quartz简介&quot;&gt;quartz简介&lt;/h2&gt;

&lt;p&gt;quartz是一个基于java的开源的任务调度框架。&lt;/p&gt;

&lt;h2 id=&quot;quatrz-api&quot;&gt;quatrz API&lt;/h2&gt;

&lt;h3 id=&quot;scheduler&quot;&gt;Scheduler&lt;/h3&gt;

&lt;p&gt;一个Scheduler通过SchedulerFactory创建和销毁。Scheduler提供对Job和Trigger的创建、销毁、暂停等一切调度相关的操作。要启动一个Scheduler，需要显式地调用它的start方法。&lt;/p&gt;

&lt;h3 id=&quot;job&quot;&gt;Job&lt;/h3&gt;

&lt;p&gt;Job是实际的任务执行逻辑的接口，它只有一个方法execute：&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;kn&quot;&gt;package&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;org&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;quartz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;interface&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Job&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;JobExecutionContext&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;kd&quot;&gt;throws&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;JobExecutionException&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;当一个Job被触发时，Scheduler会重新创建一个Job，然后通过一个worker线程执行Job的execute方法，所以实际的Job实现类中无法维护状态字段。JobExecutionContext包含了一些“运行时”的信息，比如触发Job的Scheduler、触发的Trigger、Job对应的JobDetail等等。JobDetail是在创建Job时构建的包含Job的各种详细信息的对象，其中还包括一个JobDataMap的对象，JobDataMap中存储了Job的各种状态信息。&lt;/p&gt;

&lt;h3 id=&quot;trigger&quot;&gt;Trigger&lt;/h3&gt;

&lt;p&gt;Trigger的作用就是触发Job的执行，其中包含了触发时间、触发周期等相关信息。Trigger也有对应的JobDataMap对象，可以用于向Job传递某些参数。Trigger有不同的实现类型，比如SimpleTrigger和CronTrigger。SimpleTrigger的触发类似Java中的Schedule线程池，可以支持单次执行和定时重复执行等；CronTrigger则支持通过cron表达式指定触发时机。&lt;/p&gt;

&lt;p&gt;Trigger包含优先级的概念，当有不同的Trigger在同一时间触发时，quartz会根据其优先级决定触发的顺序，因为在资源不足的情况下，&lt;/p&gt;

&lt;h3 id=&quot;将job和trigger定义分开的原因&quot;&gt;将Job和Trigger定义分开的原因&lt;/h3&gt;

&lt;p&gt;第一个原因，Job和Trigger可以独立存储，并支持多对多的关联，减少重复定义。第二个原因是这样可以降低耦合程度，可以方便后续对Job或者Trigger的编辑和替换。&lt;/p&gt;

&lt;h3 id=&quot;标识&quot;&gt;标识&lt;/h3&gt;

&lt;p&gt;Job和Triiger都有自己的身份标识，身份标识由name和group两部分组成，并规定group+name的组合必须保证在Scheduler范围内唯一。&lt;/p&gt;

&lt;h3 id=&quot;jobdatamap&quot;&gt;JobDataMap&lt;/h3&gt;

&lt;p&gt;每个JobDetail和Trigger都有与其对应的JobDataMap。用户可以在定义JobDetail和Trigger时传入指定的key-value，Job在执行时就可以从JobExecutionContext中获取对应的key-value。此外quartz还支持将JobDataMap中的key-value与Job的属性字段对应起来，只要Job实现类中定义了与JobDataMap中key-value同名的字段，quartz就会通过setter方法将其设置到创建出来的Job实例中。&lt;/p&gt;

&lt;p&gt;quartz还支持Job的持久化，所以在选择要将什么数据存到JobDataMap时要考虑结构变化带来的序列化兼容问题。在Job实现类上加上@PersistJobDataAfterExecution就可以使quartz在任务执行后将JobDataMap更新。@PersistJobDataAfterExecution通常建议和@DisallowConcurrentExecution配合使用，后者会通知quartz不允许并发执行任务，以避免状态同步问题。&lt;/p&gt;

&lt;h3 id=&quot;triggerlistener-和-joblistener&quot;&gt;TriggerListener 和 JobListener&lt;/h3&gt;

&lt;p&gt;quartz提供TriggerLisenter和JobListener，可以在任务触发时获得通知甚至是阻止任务执行。&lt;/p&gt;

&lt;h2 id=&quot;具体逻辑&quot;&gt;具体逻辑&lt;/h2&gt;

&lt;h3 id=&quot;scheduler初始化&quot;&gt;Scheduler初始化&lt;/h3&gt;

&lt;p&gt;Scheduler是通过SchedulerFactory创建的，默认的SchedulerFactory有两种实现：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;StdSchedulerFactory 标准实现，基于properties文件进行初始化&lt;/li&gt;
  &lt;li&gt;DirectSchedulerFactory 轻量级实现，可以直接通过代码初始化&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;stdschedulerfactory&quot;&gt;StdSchedulerFactory&lt;/h4&gt;

&lt;p&gt;StdSchedulerFactory默认会从&lt;em&gt;当前工作路径&lt;/em&gt;查找名为”quartz.properties”的文件进行初始化；如果没有找到，则会用自带的默认配置文件进行初始化，此外用户还可以通过系统属性”org.quartz.properties”额外指定要加载的配置文件的文件名。除了在配置文件中指定具体的配置，用户还可以通过环境变量以及jvm参数(通过-D指定)覆盖默认的选项。下面是默认的配置文件的内容：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;org.quartz.scheduler.instanceName: DefaultQuartzScheduler
org.quartz.scheduler.rmi.export: false
org.quartz.scheduler.rmi.proxy: false
org.quartz.scheduler.wrapJobExecutionInUserTransaction: false
org.quartz.threadPool.class: org.quartz.simpl.SimpleThreadPool
org.quartz.threadPool.threadCount: 10
org.quartz.threadPool.threadPriority: 5
org.quartz.threadPool.threadsInheritContextClassLoaderOfInitializingThread: true
org.quartz.jobStore.misfireThreshold: 60000
org.quartz.jobStore.class: org.quartz.simpl.RAMJobStore
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;可以看到主要有scheduler、threadPool和jobStore相关的配置，更多的配置可以参考quartz的官方文档。&lt;/p&gt;

&lt;p&gt;StdSchedulerFactory返回的Scheduler对象时StdScheduler，而StdScheduler是对QuartzScheduler的简单封装，QuartzScheduler就是quartz中schedule的核心逻辑了。&lt;/p&gt;

&lt;h4 id=&quot;quartzscheduler&quot;&gt;QuartzScheduler&lt;/h4&gt;

&lt;h5 id=&quot;start&quot;&gt;start&lt;/h5&gt;

&lt;p&gt;start方法会启动一个QuartzSchedulerThread，QuartzScheduler所有的触发操作都发生在这个线程。&lt;/p&gt;

&lt;h5 id=&quot;schedulejob&quot;&gt;scheduleJob&lt;/h5&gt;

&lt;p&gt;scheduleJob会将传入的JobDetai和Trigger存放到JobStore中然后触发相应的Listener以及唤醒scheudle线程。&lt;/p&gt;

&lt;h4 id=&quot;quartzschedulerthread&quot;&gt;QuartzSchedulerThread&lt;/h4&gt;

&lt;p&gt;QuartzScheduleThread相当于一个事件循环，它会在循环中执行以下几个任务：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;阻塞等待worker线程池可用&lt;/li&gt;
  &lt;li&gt;从JobStore中获取所有满足触发条件的Trigger&lt;/li&gt;
  &lt;li&gt;调用JobStore的triggersFired方法触发Trigger，注意这里只是触发Trigger，并没有执行对应的Job。这一步的目的主要是在任务执行前给Trigger一个更新自身状态的机会。&lt;/li&gt;
  &lt;li&gt;JobStore触发Trigger后会读取出Trigger对应的Job信息，根据返回的Job信息构建可执行的JobRunShell对象。&lt;/li&gt;
  &lt;li&gt;使用worker线程池执行JobRunShell，也就是执行真正的Job逻辑。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;job持久化jobstore&quot;&gt;Job持久化（JobStore）&lt;/h3&gt;

&lt;p&gt;quartz中的JobStore负责储存Job和Trigger定义，它主要与QuartzScheduler交互。按照约定，Job和Trigger的存储都使用group+name的组合作为标识。&lt;/p&gt;

&lt;p&gt;目前quartz提供基于内存的实现以及基于JDBC的实现。基于内存的实现把所有Job和Trigger定义存在内存中，一旦重启所有数据就会丢失；基于JDBC的实现将数据存放在数据库中，同时还支持事务。&lt;/p&gt;</content><author><name></name></author><summary type="html">quartz简介</summary></entry><entry><title type="html">Linux磁盘缓存机制</title><link href="http://localhost:4000/2019/08/18/Linux-Disk-Cache.html" rel="alternate" type="text/html" title="Linux磁盘缓存机制" /><published>2019-08-18T00:00:00+08:00</published><updated>2019-08-18T00:00:00+08:00</updated><id>http://localhost:4000/2019/08/18/Linux%20Disk%20Cache</id><content type="html" xml:base="http://localhost:4000/2019/08/18/Linux-Disk-Cache.html">&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;最近遇到了一起跟磁盘IO相关的线上故障，借此总结一下之前不太了解的Linux磁盘缓存相关的知识。&lt;/p&gt;

&lt;p&gt;总的来说磁盘缓存出现的原因大概有两个：第一是访问磁盘的速度远慢于访问内存的速度，通过在内存中缓存磁盘内容可以提高访问速度；第二是根据程序的局部性原理，数据一旦被访问过，就很有可能在短时间内再次被访问，所以在内存中缓存磁盘内容可以提高程序运行速度。&lt;/p&gt;

&lt;h3 id=&quot;局部性原理&quot;&gt;局部性原理&lt;/h3&gt;

&lt;p&gt;程序局部性原理：程序在执行时呈现出局部性规律，即在一段时间内，整个程序的执行仅限于程序中的某一部分。相应地，执行所访问的存储空间也局限于某个内存区域，具体来说，局部性通常有两种形式：时间局部性和空间局部性。&lt;/p&gt;

&lt;p&gt;时间局部性：被引用过一次的存储器位置在未来会被多次引用（通常在循环中）。&lt;/p&gt;

&lt;p&gt;空间局部性：如果一个存储器的位置被引用，那么将来他附近的位置也会被引用。&lt;/p&gt;

&lt;h2 id=&quot;页缓存&quot;&gt;页缓存&lt;/h2&gt;

&lt;p&gt;Linux系统中为了减少对磁盘的IO操作，会将打开的磁盘内容进行缓存，而缓存的地方则是物理内存，进而将对磁盘的访问转换成对内存的访问，有效提高程序的速度。Linux的缓存方式是利用物理内存缓存磁盘上的内容，称为页缓存（page cache）。&lt;/p&gt;

&lt;p&gt;页缓存是由内存中的物理页面组成的，其内容对应磁盘上的物理块。页缓存的大小会根据系统的内存空闲大小进行动态调整，它可以通过占用内存以扩张大小，也可以自我收缩以缓解内存使用压力。&lt;/p&gt;

&lt;p&gt;在虚拟内存机制出现以前，操作系统使用块缓存系列，但是在虚拟内存出现以后，操作系统管理IO的粒度更大，因此采用了页缓存机制，页缓存是基于页的、面向文件的缓存机制。&lt;/p&gt;

&lt;h3 id=&quot;页缓存的读取&quot;&gt;页缓存的读取&lt;/h3&gt;

&lt;p&gt;Linux系统在读取文件时，会优先从页缓存中读取文件内容，如果页缓存不存在，系统会先从磁盘中读取文件内容更新到页缓存中，然后再从页缓存中读取文件内容并返回。大致过程如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;进程调用库函数read发起读取文件请求&lt;/li&gt;
  &lt;li&gt;内核检查已打开的文件列表，调用文件系统提供的read接口&lt;/li&gt;
  &lt;li&gt;找到文件对应的inode，然后计算出要读取的具体的页&lt;/li&gt;
  &lt;li&gt;通过inode查找对应的页缓存，1）如果页缓存节点命中，则直接返回文件内容；2）如果没有对应的页缓存，则会产生一个缺页异常（page fault）。这时系统会创建新的空的页缓存并从磁盘中读取文件内容，更新页缓存，然后重复第4步&lt;/li&gt;
  &lt;li&gt;读取文件返回&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;所以说，所有的文件内容的读取，无论最初有没有命中页缓存，最终都是直接来源于页缓存。&lt;/p&gt;

&lt;h3 id=&quot;页缓存的写入&quot;&gt;页缓存的写入&lt;/h3&gt;

&lt;p&gt;因为页缓存的存在，当一个进程调用write时，对文件的更新仅仅是被写到了文件的页缓存中，让后将对应的页标记为dirty，整个过程就结束了。Linux内核会在周期性地将脏页写回到磁盘，然后清理掉dirty标识。&lt;/p&gt;

&lt;p&gt;由于写操作只会把变更写入页缓存，因此进程并不会因此为阻塞直到磁盘IO发生，如果此时计算机崩溃，写操作的变更可能并没有发生在磁盘上。所以对于一些要求比较严格的写操作，比如数据系统，就需要主动调用fsync等操作及时将变更同步到磁盘上。读操作则不同，read通常会阻塞直到进程读取到数据，而为了减少读操作的这种延迟，Linux系统还是用了“预读”的技术，即从磁盘中读取数据时，内核将会多读取一些页到页缓存中。&lt;/p&gt;

&lt;h4 id=&quot;回写线程&quot;&gt;回写线程&lt;/h4&gt;

&lt;p&gt;页缓存的回写是由内核中的单独的线程来完成的，回写线程会在以下3种情况下进行回写：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;空闲内存低于阈值时。当空闲内存不足时，需要释放掉一部分缓存，由于只有不脏的页才能被释放，所以需要把脏页都回写到磁盘，使其变为可回收的干净的页。&lt;/li&gt;
  &lt;li&gt;脏页在内存中处理时间超过阈值时。这是为了确保脏页不会无限期的留在内存中，减少数据丢失的风险。&lt;/li&gt;
  &lt;li&gt;当用户进程调用sync和fsync系统调用时。这是为了给用户进程提供强制回写的方法，满足回写要求严格的使用场景。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;回写线程的实现
|    名称    | 版本 |  说明  |
| ———- | — | —– |
| bdflush |  2.6版本以前 | bdflush 内核线程在后台运行，系统中只有一个 bdflush 线程，当内存消耗到特定阀值以下时，bdflush 线程被唤醒。kupdated 周期性的运行，写回脏页。 但是整个系统仅仅只有一个 bdflush 线程，当系统回写任务较重时，bdflush 线程可能会阻塞在某个磁盘的I/O上，导致其他磁盘的I/O回写操作不能及时执行。|
| pdflush       |  2.6版本引入 | pdflush 线程数目是动态的，取决于系统的I/O负载。它是面向系统中所有磁盘的全局任务的。 但是由于 pdflush 是面向所有磁盘的，所以有可能出现多个 pdflush 线程全部阻塞在某个拥塞的磁盘上，同样导致其他磁盘的I/O回写不能及时执行。|
| flusher线程       |  2.6.32版本以后引入 | flusher 线程的数目不是唯一的，同时flusher线程不是面向所有磁盘的，而是每个flusher线程对应一个磁盘|&lt;/p&gt;

&lt;h3 id=&quot;页缓存的回收&quot;&gt;页缓存的回收&lt;/h3&gt;

&lt;p&gt;Linux中页缓存的替换逻辑是一个修改过的LRU实现，也称为双链策略。和以前不同，Linux维护的不再是一个LRU链表，而是维护两个链表：活跃链表和非活跃链表。处于活跃链表上的页面被认为是“热”的且不会被换出，而在非活跃链表上的页面则是可以被换出的。在活跃链表中的页面必须在其被访问时就处于非活跃链表中。两个链表都被伪LRU规则维护：页面从尾部加入，从头部移除，如同队列。两个链表需要维持平衡–如果活跃链表变得过多而超过了非活跃链表，那么活跃链表的头页面将被重新移回到非活跃链表中，一遍能再被回收。双链表策略解决了传统LRU算法中对仅一次访问的窘境。而且也更加简单的实现了伪LRU语义。这种双链表方式也称作LRU/2。更普遍的是n个链表，故称LRU/n。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;在这次遇到的线上故障中，根本原因在于在业务逻辑中使用了临时文件做缓存，一个临时文件创建后如果在短时间内删除，这时候对这个文件的操作都是在页缓存内进行，不会实际回写到磁盘。当程序出现问题响应变慢时，临时文件存活时间变长，就可能会使其被回写到磁盘上，导致磁盘压力过大，进而影响整个系统。&lt;/p&gt;</content><author><name></name></author><summary type="html">前言</summary></entry><entry><title type="html">netty中的epoll实现</title><link href="http://localhost:4000/2019/08/03/Netty-Epoll.html" rel="alternate" type="text/html" title="netty中的epoll实现" /><published>2019-08-03T00:00:00+08:00</published><updated>2019-08-03T00:00:00+08:00</updated><id>http://localhost:4000/2019/08/03/Netty%20Epoll</id><content type="html" xml:base="http://localhost:4000/2019/08/03/Netty-Epoll.html">&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;在java中，IO多路复用的功能通过nio中的&lt;code class=&quot;highlighter-rouge&quot;&gt;Selector&lt;/code&gt;提供，在不同的操作系统下jdk会通过spi的方式加载不同的实现，比如在macos下是&lt;code class=&quot;highlighter-rouge&quot;&gt;KQueueSelectorProvider&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;KQueueSelectorProvider&lt;/code&gt;底层使用了kqueue来进行IO多路复用；在linux 2.6以后的版本则是&lt;code class=&quot;highlighter-rouge&quot;&gt;EPollSelectorProvider&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;EPollSelectorProvider&lt;/code&gt;底层使用的是epoll。虽然jdk自身提供了selector的epoll实现，netty仍实现了自己的epoll版本，根据&lt;a href=&quot;https://stackoverflow.com/a/23465481&quot;&gt;netty开发者在StackOverflow的回答&lt;/a&gt;，主要原因有两个：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;支持更多socket option，比如TCP_CORK和SO_REUSEPORT&lt;/li&gt;
  &lt;li&gt;使用了边缘触发（ET）模式&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;接下来就来看看netty自己实现的epoll版本的大概逻辑。&lt;/p&gt;

&lt;h2 id=&quot;总体介绍&quot;&gt;总体介绍&lt;/h2&gt;

&lt;h3 id=&quot;使用方式&quot;&gt;使用方式&lt;/h3&gt;

&lt;p&gt;在netty中，如果需要使用netty自己的epoll实现，需要在项目中添加netty-transport-native-epoll依赖，然后将代码中的&lt;code class=&quot;highlighter-rouge&quot;&gt;NioEvnetLoop&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;NioSocketChannel&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;NioServerSocketChannel&lt;/code&gt;等替换为Epoll开头的类即可。具体参考&lt;a href=&quot;https://netty.io/wiki/native-transports.html&quot;&gt;Using the Linux native transport&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;与jdk原生实现的区别&quot;&gt;与jdk原生实现的区别&lt;/h3&gt;

&lt;p&gt;总的来说，不管是jdk还是netty的版本，都是直接调用了linux的epoll来提供IO多路复用，netty的epoll实现与jdk的区别主要有两个：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;使用了边缘触发（可以参考我的&lt;a href=&quot;https://juejin.im/post/5cdaa67f518825691b4a5cc0&quot;&gt;另一篇文章&lt;/a&gt;）&lt;/li&gt;
  &lt;li&gt;使用了eventfd和timerfd来实现唤醒和超时控制，而jdk的实现则是使用了pipe和epoll自带的超时机制&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;具体实现&quot;&gt;具体实现&lt;/h2&gt;

&lt;h3 id=&quot;初始化&quot;&gt;初始化&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;EpollEventLoop&lt;/code&gt;在初始化时会创建三个fd：epollFd、eventFd、timerFd。epollFd用于进一步调用epoll_wait，而另外两个fd的作用前面已经提到了。除此之外，&lt;code class=&quot;highlighter-rouge&quot;&gt;EpollEventLoop&lt;/code&gt;内部还维护了一个&lt;code class=&quot;highlighter-rouge&quot;&gt;selectStrategy&lt;/code&gt;变量，&lt;code class=&quot;highlighter-rouge&quot;&gt;selectStrategy&lt;/code&gt;用于决定当前的loop中的行为，内容不算复杂，具体的就不再展开了。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;EpollEventLoop&lt;/code&gt;还维护了一个&lt;code class=&quot;highlighter-rouge&quot;&gt;EpollEventArray&lt;/code&gt;类型的对象events，events就是epoll调用时的第二个参数，表示感兴趣的描述符集合，这个变量会被传递到native方法中。&lt;/p&gt;

&lt;p&gt;此外&lt;code class=&quot;highlighter-rouge&quot;&gt;EpollEventLoop&lt;/code&gt;还有一个&lt;code class=&quot;highlighter-rouge&quot;&gt;IntObjectMap&amp;lt;AbstractEpollChannel&amp;gt;&lt;/code&gt;类型的channels字段，表示当前EventLoop注册的所有Channel对象，其中key是channel对应的fd（文件描述符），因为epoll中接受的参数和返回的结果都是以整数形式的文件描述符表示的，value就是一个Channel对象，后续对Channel进行读写都会从这里查找（注：这里使用的IntObjectMap是netty自己实现的集合，主要目的是提升使用原生类型作为key或者value时的集合的性能，类似的实现还有hppc、FastUtil等等）。&lt;/p&gt;

&lt;h3 id=&quot;注册感兴趣的连接&quot;&gt;注册感兴趣的连接&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;EpollEventLoop&lt;/code&gt;的&lt;code class=&quot;highlighter-rouge&quot;&gt;doRegister&lt;/code&gt;方法中实现了注册连接的逻辑，就是调用&lt;code class=&quot;highlighter-rouge&quot;&gt;EpollEventLoop&lt;/code&gt;的&lt;code class=&quot;highlighter-rouge&quot;&gt;add&lt;/code&gt;方法：&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AbstractEpollChannel&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;throws&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IOException&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;inEventLoop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;socket&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;intValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Native&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;epollCtlAdd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epollFd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;intValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;AbstractEpollChannel&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;可以看到这里调用了&lt;code class=&quot;highlighter-rouge&quot;&gt;Native.epolCtlAdd&lt;/code&gt;，从名字就可以看出来，底层是调用了epoll_ctl方法，然后op参数为EPOLL_ADD。&lt;/p&gt;

&lt;h3 id=&quot;事件循环&quot;&gt;事件循环&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;EpollEventLoop&lt;/code&gt;的主体就在它的&lt;code class=&quot;highlighter-rouge&quot;&gt;run&lt;/code&gt;方法里，在&lt;code class=&quot;highlighter-rouge&quot;&gt;run&lt;/code&gt;方法的主循环中会先通过&lt;code class=&quot;highlighter-rouge&quot;&gt;selectStrategy&lt;/code&gt;决定要进行的操作是epollWait还是epollBusyWait。epollWait和epollBusyWait的区别就在于前者会计算出适合的超时时间然后调用一次epoll_wait直到有描述符就绪或超时，而后者会循环调用epoll_wait并将超时时间设置为0（也就是立即返回）直到有连接就绪为止。&lt;/p&gt;

&lt;p&gt;通过epollWait或者epollBusyWait获得的结果会保存在events当中，所以接下来就是调用&lt;code class=&quot;highlighter-rouge&quot;&gt;processReady&lt;/code&gt;处理events中的各个就绪的fd。处理的过程就是根据fd从channels查到对应的channel然后进行读写等操作，详细的读写就不再展开介绍了。&lt;/p&gt;

&lt;h3 id=&quot;超时和唤醒&quot;&gt;超时和唤醒&lt;/h3&gt;

&lt;p&gt;前面提到了，netty的epoll逻辑中使用了eventfd和timerfd来实现唤醒和超时控制，evnetfd和timerfd从linux 2.6.22版本开始加入内核，其主要功能就是提供事件通知机制。eventfd可以创建一个文件描述符，在这个描述符上可以传递无符号整数，可以用来作为控制信息。timerfd也是创建一个文件描述符，在这个描述符上可以读取定时器事件，timerfd可以支持到纳秒级别。由于eventfd和timerfd都是基于描述符的，所以和select/poll/epoll这些api都比较契合。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;EpollEventLoop&lt;/code&gt;在初始化时会首先创建epollfd、eventfd和timerfd，然后把eventfd和timerfd都加入到epoll的监听队列当中。eventfd用来做唤醒的支持，当需要唤醒&lt;code class=&quot;highlighter-rouge&quot;&gt;EpollEventLoop&lt;/code&gt;时，就往eventfd写入一个数，这时eventfd就会变得可读，epoll就会及时返回。timerfd则作为epoll的超时控制，当需要超时的时候就在timerfd上设置一个时间间隔，超时时间到了之后timerfd就会变得可读，epoll也就会及时返回。这里使用timerfd作为超时控制而不是使用epoll自带的超时的原因大概有两个，一是使用timerfd可以用统一的处理方式对待超时事件和IO事件，二是timerfd支持的超时时间精度更高。&lt;/p&gt;

&lt;p&gt;顺便提一下，在jdk原生的实现中，唤醒是通过pipe实现的，&lt;code class=&quot;highlighter-rouge&quot;&gt;Selector&lt;/code&gt;内部维护了一个pipe，初始化时将pipe的read端加入epoll的监听队列，当需要唤醒时就在pipe的write端写入数据，这样epoll就会及时返回。epoll返回后如果发现pipe可读，则将pipe中的数据读取完。&lt;/p&gt;

&lt;h2 id=&quot;其他&quot;&gt;其他&lt;/h2&gt;

&lt;p&gt;在之前的文章中提到过，将fd注册到epoll时如果采用了边缘触发，那么建议的使用方式是将fd设置为非阻塞模式，并且在描述符就绪时需要将就绪数据全部读取完（遇到EAGAIN）为止，否则可能会出现再也无法收到就绪通知的情况。&lt;/p&gt;

&lt;p&gt;而在netty的epoll实现中，所有的socket都是以ET模式注册的，而eventfd和timerfd则稍有不同。在netty 4.1.38.Final以前的版本，eventfd在注册到epollfd时使用时LT而不是ET，在每次processReady时如果eventfd可读则都会对其调用一次read。timerfd在注册到epollfd时使用的时ET，但是在每次processReady时如果timerfd可读也会对其调用一次read。而在4.1.38.Final版本，eventfd和timerfd都使用了ET，但是并不在processReady方法中读取这两个fd。对于eventfd，会在每次write返回EAGAIN时调用一次read，因为eventfd内部只能存储一个整数，所以当write出现EAGAIN时就说明目前有数据需要读取。而对于timerfd则只会在epollWait出现超时的时候调用一次read，其他情况下不会对timerfd调用read。因为在netty的实现中，每次进行epoll_wait时都会重新设置timerfd的超时时间，而每次更新timerfd的超时时间时，timerfd就会重新变为不可读状态，也就不用对其调用read了。&lt;/p&gt;</content><author><name></name></author><summary type="html">前言</summary></entry><entry><title type="html">linux 线程机制</title><link href="http://localhost:4000/2019/07/28/LinuxThread&NPTL.html" rel="alternate" type="text/html" title="linux 线程机制" /><published>2019-07-28T00:00:00+08:00</published><updated>2019-07-28T00:00:00+08:00</updated><id>http://localhost:4000/2019/07/28/LinuxThread&amp;NPTL</id><content type="html" xml:base="http://localhost:4000/2019/07/28/LinuxThread&amp;NPTL.html">&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;前面提到进程和线程的区别，进程是资源分配的基本单位，线程是程序执行的基本单位。线程都属于某个进程，而同一个进程下的不同线程分别有共享和独享的数据，这里在列举一下：&lt;/p&gt;

&lt;p&gt;同一进程内的所有线程除了共享全局变量外还共享：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;进程指令&lt;/li&gt;
  &lt;li&gt;大多数数据&lt;/li&gt;
  &lt;li&gt;打开的文件（即描述符）&lt;/li&gt;
  &lt;li&gt;信号处理函数和信号处置&lt;/li&gt;
  &lt;li&gt;当前工作目录&lt;/li&gt;
  &lt;li&gt;用户ID和组ID&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;不过每个线程有各自的：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;线程ID&lt;/li&gt;
  &lt;li&gt;寄存器集合，包括程序计数器和栈指针&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;errno&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;信号掩码&lt;/li&gt;
  &lt;li&gt;优先级&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;linux是遵循POSIX标准的操作系统，所以linux也需要提供遵循POSIX标准的线程实现。而最初linux系统中的线程机制则是LinuxThreads，在2.6版本之后又增加了NPTL（Native POSIX Thread Library）。&lt;/p&gt;

&lt;h3 id=&quot;内核线程和用户线程&quot;&gt;内核线程和用户线程&lt;/h3&gt;

&lt;p&gt;对于线程的实现机制来说，通常可以选择在内核内或者内核外实现，这两种方式的区别在于线程是在核内还是核外调度。核内调度更利于并发使用多处理器的资源，内核可以将同一个进程的不同线程调度到不同处理器上执行，当某个线程阻塞时，内核可以将处理器调度到同一个进程的另一个线程。而核外调度的上下文切换开销更低，因为线程的切换不用陷入内核态。&lt;/p&gt;

&lt;h3 id=&quot;进程-线程模型&quot;&gt;进程-线程模型&lt;/h3&gt;

&lt;p&gt;当内核既支持进程也支持线程时，就可以实现线程-进程的”多对多”模型，即一个进程的某个线程由核内调度，而同时它也可以作为用户级线程池的调度者，选择合适的用户级线程在其空间中运行。这样既可满足多处理机系统的需要，也可以最大限度的减小调度开销。&lt;/p&gt;

&lt;p&gt;在内核外实现的线程又可以分为”一对一”、”多对一”两种模型，前者用一个内核进程对应一个线程，将线程调度等同于进程调度，交给内核完成，而后者则完全在核外实现多线程，调度也在用户态完成。后者就是前面提到的单纯的用户级线程模型的实现方式，显然，这种核外的线程调度器实际上只需要完成线程运行栈的切换，调度开销非常小，但同时因为内核信号都是以进程为单位的，因而无法定位到线程，所以这种实现方式不能用于多处理器系统。&lt;/p&gt;

&lt;h3 id=&quot;linux的轻量级进程&quot;&gt;linux的轻量级进程&lt;/h3&gt;

&lt;p&gt;linux内核只提供了轻量进程的支持，限制了更高效的线程模型的实现，但linux着重优化了进程的调度开销，一定程度上也弥补了这一缺陷。目前linux的线程机制都采用的线程-进程”一对一”模型，调度交给内核，而在用户级实现一个包括信号处理在内的线程管理机制。&lt;/p&gt;

&lt;p&gt;linux内核在2.0.x版本就已经实现了轻量进程，应用程序可以通过一个统一的&lt;code class=&quot;highlighter-rouge&quot;&gt;clone&lt;/code&gt;系统调用接口，用不同的参数指定创建轻量进程还是普通进程。在内核中，&lt;code class=&quot;highlighter-rouge&quot;&gt;clone&lt;/code&gt;调用经过参数传递和解释后会调用&lt;code class=&quot;highlighter-rouge&quot;&gt;do_fork&lt;/code&gt;，这个核内函数同时也是&lt;code class=&quot;highlighter-rouge&quot;&gt;fork&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;vfork&lt;/code&gt;系统调用的最终实现：&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;intdo_fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsignedlongclone_flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsignedlongstack_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;structpt_regs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;regs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsignedlongstack_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;在do_fork&lt;/code&gt;中，不同的clone_flags将导致不同的行为（共享不同的资源），下面列举几个flag的作用。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CLONE_VM&lt;/strong&gt;
如果&lt;code class=&quot;highlighter-rouge&quot;&gt;do_fork&lt;/code&gt;时指定了&lt;code class=&quot;highlighter-rouge&quot;&gt;CLONE_VM&lt;/code&gt;开关，创建的轻量级进程的内存空间将会和父进程指向同一个地址，即创建的轻量级进程将与父进程共享内存地址空间。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CLONE_FS&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如果&lt;code class=&quot;highlighter-rouge&quot;&gt;do_fork&lt;/code&gt;时指定了&lt;code class=&quot;highlighter-rouge&quot;&gt;CLONE_FS&lt;/code&gt;开关，对于轻量级进程则会与父进程共享相同的所在文件系统的根目录和当前目录信息。也就是说，轻量级进程没有独立的文件系统相关的信息，进程中任何一个线程改变当前目录、根目录等信息都将直接影响到其他线程。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CLONE_FILES&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如果&lt;code class=&quot;highlighter-rouge&quot;&gt;do_fork&lt;/code&gt;时指定了&lt;code class=&quot;highlighter-rouge&quot;&gt;CLONE_FILES&lt;/code&gt;开关，创建的轻量级进程与父进程将会共享已经打开的文件。这一共享使得任何线程都能访问进程所维护的打开文件，对它们的操作会直接反映到进程中的其他线程。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CLONE_SIGHAND&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如果&lt;code class=&quot;highlighter-rouge&quot;&gt;do_fork&lt;/code&gt;时指定了&lt;code class=&quot;highlighter-rouge&quot;&gt;CLONE_FILES&lt;/code&gt;开关，轻量级进程与父进程将会共享对信号的处理方式。也就是说，子进程与父进程的信号处理方式完全相同，而且可以相互更改。&lt;/p&gt;

&lt;p&gt;尽管linux支持轻量级进程，但并不能说它就支持内核线程，因为linux的”线程”和”进程”实际上处于一个调度层次，共享一个进程标识符空间，这种限制使得不可能在linux上实现完全意义上的POSIX线程机制，因此众多的linux线程库实现尝试都只能尽可能实现POSIX的绝大部分语义，并在功能上尽可能逼近。&lt;/p&gt;

&lt;h2 id=&quot;linuxthreads的线程机制&quot;&gt;LinuxThreads的线程机制&lt;/h2&gt;

&lt;p&gt;LinuxThreads是linux平台上使用过的一个线程库。它所实现的就是基于内核轻量级进程的”一对一”线程模型，一个线程实体对应一个核心轻量级进程，而线程之间的管理在核外函数库中实现。对于LinuxThreads，它使用&lt;code class=&quot;highlighter-rouge&quot;&gt;(CLONE_VM|CLONE_FS|CLONE_FILES|CLONE_SIGHAND)&lt;/code&gt;参数来调用&lt;code class=&quot;highlighter-rouge&quot;&gt;clone&lt;/code&gt;创建”线程”，表示共享内存、共享文件系统访问计数、共享文件描述符表，以及共享信号处理方式。&lt;/p&gt;

&lt;h3 id=&quot;管理线程&quot;&gt;管理线程&lt;/h3&gt;

&lt;p&gt;LinuxThreads最初的设计相信相关进程之间的上下文切换速度很快，因此每个内核线程足以处理很多相关的用户级线程。LinuxThreads非常出名的一个特性就是管理线程（manager thread）。在LinuxThreads中，专门为每一个进程构造了一个管理线程，负责处理线程相关的管理工作。当进程第一次调用&lt;code class=&quot;highlighter-rouge&quot;&gt;pthread_create&lt;/code&gt;创建一个线程的时候就会创建并启动管理线程。&lt;/p&gt;

&lt;p&gt;在一个进程空间内，管理线程与其他线程之间通过一对”管理管道（manager_pipe[2]）”来通讯，该管道在创建管理线程之前创建，在成功启动了管理线程之后，管理管道的读端和写端分别赋给两个全局变量&lt;code class=&quot;highlighter-rouge&quot;&gt;__pthread_manager_reader&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;__pthread_manager_request&lt;/code&gt;，之后，每个用户线程都通过&lt;code class=&quot;highlighter-rouge&quot;&gt;__pthread_manager_request&lt;/code&gt;向管理线程发请求，但管理线程本身并没有直接使用&lt;code class=&quot;highlighter-rouge&quot;&gt;__pthread_manager_reader&lt;/code&gt;，管道的读端（manager_pipe[0]）是作为&lt;code class=&quot;highlighter-rouge&quot;&gt;__clone()&lt;/code&gt;的参数之一传给管理线程的，管理线程的工作主要就是监听管道读端，并对从中取出的请求作出反应。&lt;/p&gt;

&lt;p&gt;管理线程在进行一系列初始化工作后，进入while(1)循环。在循环中，线程以2秒为timeout查询（__poll()）管理管道的读端。在处理请求前，检查其父线程是否已退出，如果已退出就退出整个进程。如果有退出的子线程需要清理，则进行清理。然后才是读取管道中的请求，根据请求类型执行相应操作（switch-case）。&lt;/p&gt;

&lt;p&gt;每个LinuxThreads线程都同时具有线程id和进程id，其中进程id就是内核所维护的进程号，而线程id则由LinuxThreads分配和维护。&lt;/p&gt;

&lt;h3 id=&quot;linuxthreads的局限性&quot;&gt;LinuxThreads的局限性&lt;/h3&gt;

&lt;p&gt;LinuxThreads的设计通常都可以很好地工作；但是在压力很大的应用程序中，它的性能、可伸缩性和可用性都会存在问题。下面让我们来看一下LinuxThreads设计的一些局限性：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;进程id问题：linux内核并不支持真正意义上的线程，LinuxThreads是用与普通进程具有同样内核调度视图的轻量级进程来实现线程支持的。这些轻量级进程拥有独立的进程id，在进程调度、信号处理、IO等方面享有与普通进程一样的能力。在源码阅读者看来，就是linux内核的&lt;code class=&quot;highlighter-rouge&quot;&gt;clone&lt;/code&gt;没有实现对CLONE_PID参数的支持。按照POSIX定义，同一进程的所有线程应该共享一个进程id和父进程id，这在目前的”一对一”模型下是无法实现的。&lt;/li&gt;
  &lt;li&gt;管理线程容易成为瓶颈，这是这种结构的通病；同时，管理线程又负责用户线程的清理工作，因此，尽管管理线程已经屏蔽了大部分的信号，但一旦管理线程死亡，用户线程就不得不手工清理了，而且用户线程并不知道管理线程的状态，之后的线程创建等请求将无人处理。&lt;/li&gt;
  &lt;li&gt;信号用来实现同步原语，这会影响操作的响应时间。另外，将信号发送到主进程的概念也并不存在。因此，这并不遵守POSIX中处理信号的方法。&lt;/li&gt;
  &lt;li&gt;LinuxThreads中对信号的处理是按照每线程的原则建立的，而不是按照每进程的原则建立的，这是因为每个线程都有一个独立的进程ID。由于信号被发送给了一个专用的线程，因此信号是串行化的——也就是说，信号是透过这个线程再传递给其他线程的。这与POSIX标准对线程进行并行处理的要求形成了鲜明的对比。例如，在LinuxThreads中，通过kill()所发送的信号被传递到一些单独的线程，而不是集中整体进行处理。这意味着如果有线程阻塞了这个信号，那么LinuxThreads就只能对这个线程进行排队，并在线程开放这个信号时在执行处理，而不是像其他没有阻塞信号的线程中一样立即处理这个信号。&lt;/li&gt;
  &lt;li&gt;由于LinuxThreads中的每个线程都是一个进程，因此用户和组ID的信息可能对单个进程中的所有线程来说都不是通用的。例如，一个多线程的setuid()/setgid()进程对于不同的线程来说可能都是不同的。&lt;/li&gt;
  &lt;li&gt;由于每个线程都是一个单独的进程，因此/proc目录中会充满众多的进程项，而这实际上应该是线程。&lt;/li&gt;
  &lt;li&gt;由于每个线程都是一个进程，因此对每个应用程序只能创建有限数目的线程。&lt;/li&gt;
  &lt;li&gt;由于计算线程本地数据的方法是基于堆栈地址的位置的，因此对于这些数据的访问速度都很慢。另外一个缺点是用户无法可信地指定堆栈的大小，因为用户可能会意外地将堆栈地址映射到本来要为其他目的所使用的区域上了。按需增长（growondemand）的概念（也称为浮动堆栈的概念）是在2.4.10版本的linux内核中实现的。在此之前，LinuxThreads使用的是固定堆栈。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;nptl&quot;&gt;NPTL&lt;/h2&gt;

&lt;p&gt;NPTL（Native POSIX Thread Library）是linux线程的一个新实现，它克服了LinuxThreads的缺点，同时也符合POSIX的需求。与LinuxThreads相比，它在性能和稳定性方面都提供了重大的改进。与LinuxThreads一样，NPTL也实现了一对一的模型。&lt;/p&gt;

&lt;p&gt;NPTL出现的一部分原因是对LinuxThreads进行改进，它设计目标如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;这个新线程库应该兼容POSIX标准。&lt;/li&gt;
  &lt;li&gt;这个线程实现应该在具有很多处理器的系统上也能很好地工作。&lt;/li&gt;
  &lt;li&gt;为一小段任务创建新线程应该具有很低的启动成本。&lt;/li&gt;
  &lt;li&gt;NPTL线程库应该与LinuxThreads是二进制兼容的。&lt;/li&gt;
  &lt;li&gt;这个新线程库应该可以利用NUMA支持的优点。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;nptl的优点&quot;&gt;NPTL的优点&lt;/h3&gt;

&lt;p&gt;NPTL总的来说采用了LinuxThreads类似的解决办法，内核看到的依然是一个进程，新线程是通过&lt;code class=&quot;highlighter-rouge&quot;&gt;clone()&lt;/code&gt;系统调用产生的。与LinuxThreads相比，NPTL具有很多优点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;NPTL没有使用管理线程。管理线程的一些需求，例如向作为进程一部分的所有线程发送终止信号，是并不需要的；因为内核本身就可以实现这些功能。内核还会处理每个线程堆栈所使用的内存的回收工作。它甚至还通过在清除父线程之前进行等待，从而实现对所有线程结束的管理，这样可以避免僵尸进程的问题。&lt;/li&gt;
  &lt;li&gt;由于NPTL没有使用管理线程，因此其线程模型在NUMA和SMP系统上具有更好的可伸缩性和同步机制。&lt;/li&gt;
  &lt;li&gt;使用NPTL线程库与新内核实现，就可以避免使用信号来对线程进行同步了。为了这个目的，NPTL引入了一种名为&lt;code class=&quot;highlighter-rouge&quot;&gt;futex&lt;/code&gt;的新机制。&lt;code class=&quot;highlighter-rouge&quot;&gt;futex&lt;/code&gt;在共享内存区域上进行工作，因此可以在进程之间进行共享，这样就可以提供进程间POSIX同步机制。我们也可以在进程之间共享一个&lt;code class=&quot;highlighter-rouge&quot;&gt;futex&lt;/code&gt;。这种行为使得进程间同步成为可能。实际上，NPTL包含了一个&lt;code class=&quot;highlighter-rouge&quot;&gt;PTHREAD_PROCESS_SHARED&lt;/code&gt;宏，使得开发人员可以让用户级进程在不同进程的线程之间共享互斥锁。&lt;/li&gt;
  &lt;li&gt;由于NPTL是POSIX兼容的，因此它对信号的处理是按照每进程的原则进行的；getpid()会为所有的线程返回相同的进程ID。例如，如果发送了SIGSTOP信号，那么整个进程都会停止；使用LinuxThreads，只有接收到这个信号的线程才会停止。这样可以在基于NPTL的应用程序上更好地利用调试器，例如GDB。&lt;/li&gt;
  &lt;li&gt;由于在NPTL中所有线程都具有一个父进程，因此对父进程汇报的资源使用情况（例如CPU和内存百分比）都是对整个进程进行统计的，而不是对一个线程进行统计的。&lt;/li&gt;
  &lt;li&gt;NPTL线程库所引入的一个实现特性是对ABI（应用程序二进制接口）的支持。这帮助实现了与LinuxThreads的向后兼容性。这个特性是通过使用&lt;code class=&quot;highlighter-rouge&quot;&gt;LD_ASSUME_KERNEL&lt;/code&gt;实现的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;futex&quot;&gt;futex&lt;/h3&gt;

&lt;p&gt;futex（Fast Userspace muTexes）意为快速用户区互斥，它是linux提供的一种同步（互斥）机制，特点是对于条件的判断是发生在用户空间的，在竞争不激烈的情况下能有更好的性能表现。futex在2.6.x系列稳定版内核中出现。&lt;/p&gt;

&lt;p&gt;futex由一块能够被多个进程共享的内存空间（一个对齐后的整型变量）组成；这个整型变量的值能够通过汇编语言调用CPU提供的原子操作指令来增加或减少，并且一个进程可以等待直到那个值变成正数。Futex 的操作几乎全部在用户空间完成；只有当操作结果不一致从而需要仲裁时，才需要进入操作系统内核空间执行。这种机制允许使用 futex 的锁定原语有非常高的执行效率：由于绝大多数的操作并不需要在多个进程之间进行仲裁，所以绝大多数操作都可以在应用程序空间执行，而不需要使用（相对高代价的）内核系统调用。&lt;/p&gt;</content><author><name></name></author><summary type="html">前言</summary></entry><entry><title type="html">redis热点</title><link href="http://localhost:4000/2019/07/21/Redis-Hot-Key.html" rel="alternate" type="text/html" title="redis热点" /><published>2019-07-21T00:00:00+08:00</published><updated>2019-07-21T00:00:00+08:00</updated><id>http://localhost:4000/2019/07/21/Redis%20Hot%20Key</id><content type="html" xml:base="http://localhost:4000/2019/07/21/Redis-Hot-Key.html">&lt;h2 id=&quot;redis简介&quot;&gt;redis简介&lt;/h2&gt;

&lt;p&gt;redis是一个用C语言编写的开源的、基于内存的键值对存储数据库。redis支持多种数据结构如string、hash、list、set等等，同时还支持原子操作。相比起其他kv类型的数据库产品，redis有两个突出的特点：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;redis支持多种数据结构，并且可以在复杂的数据结构上进行原子操作&lt;/li&gt;
  &lt;li&gt;redis支持数据的持久化，包括RDB和AOF格式&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;除此之外，redis的另一个着重宣传的优势就是快，因为redis对数据的操作都是基于内存的，同时redis采用单线程模型，所有的命令都在同一个线程中执行，避免了线程切换的影响。&lt;/p&gt;

&lt;h2 id=&quot;redis使用的限制&quot;&gt;redis使用的限制&lt;/h2&gt;

&lt;p&gt;由于redis独特的单线程模型，在使用redis时需要注意几个地方，避免出现性能下降。&lt;/p&gt;

&lt;h3 id=&quot;避免耗时的操作&quot;&gt;避免耗时的操作&lt;/h3&gt;

&lt;p&gt;前面提到，redis只用一个线程处理所有的命令，所以应避免出现耗时的操作，否则会阻塞整个redis实例，影响性能。&lt;/p&gt;

&lt;h3 id=&quot;避免大key&quot;&gt;避免大key&lt;/h3&gt;

&lt;p&gt;大key指的是一个key对应的value过大的情况，比如string的长度非常大，或者hash、list、set、zset类型中元素个数非常多等等。redis中的大key会导致查询、删除等操作变慢，网卡带宽占满等问题，影响其他查询。&lt;/p&gt;

&lt;h3 id=&quot;避免keys遍历&quot;&gt;避免KEYS遍历&lt;/h3&gt;

&lt;p&gt;redis中的KEYS命令可以根据给定的正则表达式匹配所有符合的key，时间复杂度为O(N)，在key的数量特别多时耗时会变长，直接导致其他命令阻塞甚至实例崩溃。&lt;/p&gt;

&lt;h2 id=&quot;redis热点产生的原因&quot;&gt;redis热点产生的原因&lt;/h2&gt;

&lt;p&gt;在实际的生产中，我们通常会通过集群的模式部署redis来保证性能和可用性。redis集群的原理是通过分片来扩展服务能力，但不支持同时处理多个key的命令，来自客户端的请求会根据一定的规则路由到集群中的某个实例，而实例与实例之间不进行数据交换。&lt;/p&gt;

&lt;p&gt;热点通常是由实际业务中的热门商品、热点新闻或者突发事件引起的，这是用户会大量而且集中的访问某个数据。而在服务端访问数据时，就会根据数据分片规则访问某个redis实例，这时由于请求过于集中，所有的请求都会落到同一个实例，就产生了redis热点。&lt;/p&gt;

&lt;h2 id=&quot;redis热点的危害&quot;&gt;redis热点的危害&lt;/h2&gt;

&lt;p&gt;redis热点实际上就是某个redis实例的负载过高，进而导致以下几个问题：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;主机网卡被打满，影响主机上的其他服务&lt;/li&gt;
  &lt;li&gt;请求量过大，导致redis实例崩溃&lt;/li&gt;
  &lt;li&gt;redis失效导致业务降级到读取DB，进而导致DB也崩溃，业务雪崩&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;redis热点的解决方法&quot;&gt;redis热点的解决方法&lt;/h2&gt;

&lt;h3 id=&quot;避免直接读db&quot;&gt;避免直接读DB&lt;/h3&gt;

&lt;p&gt;这是比较简单的方法，业务的服务端维护一个本地缓存，当redis负载过高或者崩溃时不降级到DB，而是返回本地缓存中的数据。但是这个方案有以下几个问题：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;需要一个判断是否返回本地缓存的机制&lt;/li&gt;
  &lt;li&gt;需要维护本地缓存&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;读写分离&quot;&gt;读写分离&lt;/h3&gt;

&lt;p&gt;读写分离的方案需要引入一个代理层，在redis客户端与redis集群之间增加一个代理，来自客户端的请求会先经过代理层做负载均衡和路由。然后redis集群中的实例分为读和写两类，读节点只负责读取数据。当出现redis请求热点时，读节点负载就会上升，这是我们可以扩容出更多的读节点来分担负载，避免redis实例崩溃。但是这种方案由以下几个要求：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;需要维护代理层和负载均衡&lt;/li&gt;
  &lt;li&gt;需要实时监控redis负载&lt;/li&gt;
  &lt;li&gt;需要支持节点扩缩容&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;热点数据缓存&quot;&gt;热点数据缓存&lt;/h3&gt;

&lt;p&gt;另一种可以选择的方案就是对热点数据进行缓存，这里可以选择在代理层进行缓存或者是在客户端缓存。当发现有redis热点时，可以及时从缓存读取，避免直接向redis实例发起请求。这个方案有以下几个要求：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;需要维护本地缓存&lt;/li&gt;
  &lt;li&gt;需要redis热点发现机制&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;热点的发现&quot;&gt;热点的发现&lt;/h3&gt;

&lt;p&gt;热点的发现可以通过统计的方式实现。我们可以在代理层周期统计每个key的访问情况，当超过指定阈值时key就成为了热点key。&lt;/p&gt;</content><author><name></name></author><summary type="html">redis简介</summary></entry></feed>