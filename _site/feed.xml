<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-04-19T18:47:46+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">熊纪元的博客</title><subtitle>think digital and be human</subtitle><entry><title type="html">k8s的调度</title><link href="http://localhost:4000/2020/04/12/k8s-scheduler.html" rel="alternate" type="text/html" title="k8s的调度" /><published>2020-04-12T00:00:00+08:00</published><updated>2020-04-12T00:00:00+08:00</updated><id>http://localhost:4000/2020/04/12/k8s%20scheduler</id><content type="html" xml:base="http://localhost:4000/2020/04/12/k8s-scheduler.html">&lt;h2 id=&quot;资源模型与资源管理&quot;&gt;资源模型与资源管理&lt;/h2&gt;
&lt;h3 id=&quot;资源限额配置&quot;&gt;资源限额配置&lt;/h3&gt;
&lt;p&gt;k8s中资源调度的单位是pod，对资源的需求都设置在pod的属性里，其中主要有cpu和memory等。在k8s中，cpu属于“可压缩”的资源，意思是当cpu资源不足时，pod只会产生“饥饿”，而不会退出；memory属于“不可压缩”的资源，当memory资源不足时，pod就会因为OOM而被kill掉。同时，pod内可以包含多个container，所以在包含多个container的pod中，对资源的需求实际上是配置在每个container里的，这样一来pod的资源配置就是其包含的所有container的资源配置的总和。&lt;/p&gt;

&lt;p&gt;k8s中pod对资源的配置，包含requests和limits两种类型：在调度的时候，调度器只会按照requests的值进行计算。而在真正设置cgroups限制的时候，kubelet则会按照limits的值来进行设置。这个设计的思路在于：用户在提交pod时，可以声明一个相对较小的requests值供调度器使用，而k8s真正设置给容器cgroups的，则是相对较大的limits值。&lt;/p&gt;

&lt;p&gt;这样的设计实际上源于Borg论文中的“动态资源边界”的定义，即：容器化作业在提交时所设置的资源边界，并不一定是调度系统所必须严格遵守的，这是因为在实际场景中，大多数作业使用到的资源其实远小于它所请求的资源限额。基于这种假设，Borg在作业被提交后，会主动减小它的资源限额配置，以便容纳更多的作业、提升资源利用率。而当作业资源使用量增加到一定阈值时，Borg会通过“快速恢复”过程，还原作业原始的资源限额，防止出现异常情况。&lt;/p&gt;

&lt;p&gt;###pod的Qos级别
根据pod对requests和limits的不同的设置方式，k8s会将pod划分为三个不同的QoS级别：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;guaranteed：当pod中每个container都同时设置了requests和limits，并且requests和limits值相等的时候，这个pod就属于guaranteed类别。当pod仅设置了limits没有设置requests的时候，k8s会自动为它设置与limits相同的requests值，这也属于guaranteed&lt;/li&gt;
  &lt;li&gt;burstable：当pod不满足guaranteed的条件，但至少有一个container设置了requests。那么这个pod就会被划分到burstable类别&lt;/li&gt;
  &lt;li&gt;besteffort：如果一个pod既没有设置requests，也没有设置limits，那么它的QoS类别就是besteffort&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;###pod的eviction
QoS 划分的主要应用场景，是当宿主机资源紧张的时候，kubelet对pod进行eviction（即资源回收）时需要用到的。具体地说，当k8s所管理的宿主机上不可压缩资源短缺时，就有可能触发eviction。比如，可用内存（memory.available）、可用的宿主机磁盘空间（nodefs.available），以及容器运行时镜像存储空间（imagefs.available）等等。&lt;/p&gt;

&lt;p&gt;pod的eviction有两种类型：soft和hard；soft类型表示在资源不足时，先等待一段时间（可配置）再进行eviction，hard类型表示当资源不足时就立刻进行eviction。kubelet在对pod进行eviction时，会根据pod的QoS级别来选择要被驱逐的pod：besteffort -&amp;gt; burstable -&amp;gt; guaranteed。同时k8s会保证只有当guaranteed类别的pod的资源使用量超过了其limits的限制，或者宿主机本身正处于Memory Pressure状态时，guaranteed的pod才可能被选中进行eviction操作。&lt;/p&gt;

&lt;h2 id=&quot;默认调度器&quot;&gt;默认调度器&lt;/h2&gt;
&lt;p&gt;k8s中的默认调度器的职责，就是为新创建的pod分配一个合适的node。而在筛选合适的node的过程中，主要包含两个步骤：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;从集群的所有node中，挑选出可以运行这个pod的node；这个步骤会运行一个叫做predicate的算法来检查每个node&lt;/li&gt;
  &lt;li&gt;从第1步筛选的结果中，挑选出一个最符合条件的node作为最终结果；这个步骤会运行一个叫做priority的算法来为每个node打分，然后选出得分最高的node
而对一个pod调度成功，实际上就是将pod的spec.nodeName字段填上合适的node的名字；随后对应node上的kubelet会监听到这个更新，然后创建出实际的node。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;默认调度器的工作原理&quot;&gt;默认调度器的工作原理&lt;/h3&gt;
&lt;p&gt;k8s的调度器的核心，实际上就是两个相互独立的控制循环，称为Informer Path和Scheduling Path。
&lt;img src=&quot;/img/scheduler.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Informer Path的主要内容就是监听etcd中各种资源类型的变化（Pod、Node、Service等等）以及与调度相关的API对象的变化。当一个待调度的pod被创建之后，Informer Path就会感知到并将其加入调度队列中，默认情况下，调度队列是一个优先级队列。&lt;/p&gt;

&lt;p&gt;此外，默认调度器还要负责对调度器缓存（scheduler cache）进行更新。事实上，k8s调度部分进行性能优化的一个最根本原则，就是尽最大可能将集群信息Cache化，以便从根本上提高Predicate和Priority调度算法的执行效率。&lt;/p&gt;

&lt;p&gt;Scheduling Path的主要逻辑，就是不断地从调度队列里出队一个pod。然后，调用predicates算法进行“过滤”。这一步“过滤”得到的一组node，就是所有可以运行这个pod的宿主机列表。当然，predicates算法需要的node信息，都是从scheduler cache里直接拿到的，这是调度器保证算法执行效率的主要手段之一。接下来，调度器就会再调用priorities算法为上述列表里的node打分，得分最高的node，就会作为这次调度的结果。调度算法执行完成后，调度器就需要将pod对象的nodeName字段的值，修改为上述node的名字。这个步骤称作 Bind。但是，为了不在关键调度路径里远程访问 APIServer，默认调度器在Bind阶段，只会更新scheduler cache里的pod和node的信息。这种基于“乐观”假设的API对象更新方式被称作Assume。&lt;/p&gt;

&lt;p&gt;Assume之后，调度器才会创建一个goroutine来异步地向APIServer发起更新pod的请求，来真正完成bind操作。如果这次异步的bind过程失败了，scheduler cache 同步之后一切就会恢复正常。由于调度器的“乐观”绑定的设计，当一个新的pod完成调度需要在某个节点上运行起来之前，该节点上的kubelet还会通过一个叫作Admit的操作来再次验证该pod是否确实能够运行在该节点上。这一步Admit操作，实际上就是把一组叫作GeneralPredicates的最基本的调度算法，比如：“资源是否可用”“端口是否冲突”等再执行一遍，作为kubelet端的二次确认。&lt;/p&gt;

&lt;h3 id=&quot;默认调度策略&quot;&gt;默认调度策略&lt;/h3&gt;
&lt;h4 id=&quot;predicate&quot;&gt;predicate&lt;/h4&gt;
&lt;p&gt;predicates在调度过程中的作用，可以理解为Filter，即：它按照调度策略，从当前集群的所有节点中，“过滤”出一系列符合条件的节点。这些节点，都是可以运行待调度pod的宿主机。在k8s中，默认的调度策略有如下三种：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;GeneralPredicates：负责的是最基础的调度策略，比如资源是否足够，node的名字是否满足pod指定的名字，端口是否有冲突等等。&lt;/li&gt;
  &lt;li&gt;Volume相关的过滤规则&lt;/li&gt;
  &lt;li&gt;宿主机相关的过滤规则
+pod相关的过滤规则&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在具体执行的时候，当开始调度一个pod时，k8s调度器会同时启动16个goroutine，来并发地为集群里的所有node计算predicates，最后返回可以运行这个pod的宿主机列表。需要注意的是，在为每个node执行predicates时，调度器会按照固定的顺序来进行检查。这个顺序，是按照predicates本身的含义来确定的。比如，宿主机相关的predicates会被放在相对靠前的位置进行检查。要不然的话，在一台资源已经严重不足的宿主机上，上来就开始计算PodAffinityPredicate是没有实际意义的。&lt;/p&gt;
&lt;h4 id=&quot;priority&quot;&gt;priority&lt;/h4&gt;
&lt;p&gt;完成了节点的“过滤”之后，priority阶段的工作就是为这些节点打分。这里打分的范围是0-10分，得分最高的节点就是最后被pod绑定的最佳节点。k8s中有很多打分规则，比如：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;LeastRequestedPriority：打分公式为&lt;code class=&quot;highlighter-rouge&quot;&gt;score = (cpu((capacity-sum(requested))10/capacity) + memory((capacity-sum(requested))10/capacity))/2&lt;/code&gt;，即选择空闲资源最多的宿主机&lt;/li&gt;
  &lt;li&gt;BalancedResourceAllocation：打分公式为&lt;code class=&quot;highlighter-rouge&quot;&gt;score = 10 - variance(cpuFraction,memoryFraction,volumeFraction)*10&lt;/code&gt;，其中每种资源的Fraction的定义是 ：请求的资源/节点上的可用资源。而variance算法的作用，则是计算每两种资源fraction之间的“距离”。而最后选择的，则是资源Fraction差距最小的节点。BalancedResourceAllocation选择的其实是调度完成后，所有节点里各种资源分配最均衡的那个节点，从而避免一个节点上 CPU 被大量分配、而 Memory 大量剩余的情况。
+nodeAffinityPriority：根据node亲和性打分&lt;/li&gt;
  &lt;li&gt;TaintTolerationPriority：根据污点容忍打分&lt;/li&gt;
  &lt;li&gt;InterPodAffinityPriority：根据pod间亲和性打分&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;优先级和抢占机制&quot;&gt;优先级和抢占机制&lt;/h3&gt;
&lt;p&gt;优先级和抢占机制，解决的是pod调度失败时该怎么办的问题。当一个pod调度失败后，它就会被暂时“搁置”起来，直到pod被更新，或者集群状态发生变化，调度器才会对这个pod进行重新调度。但在有时候，我们希望的是这样一个场景。当一个高优先级的pod调度失败后，该pod并不会被“搁置”，而是会“挤走”某个node上的一些低优先级的pod。这样就可以保证这个高优先级pod的调度成功。&lt;/p&gt;
&lt;h4 id=&quot;优先级&quot;&gt;优先级&lt;/h4&gt;
&lt;p&gt;在k8s中，我们可以先定义一个PriorityClass对象，其中指定了优先级的值（没有指定则默认为0），然后在pod中通过priorityClassName指定其要设置的优先级。而拥有较高优先级的pod，就会在调度过程中的优先级队列中优先出队，就体现了“优先级”的概念。&lt;/p&gt;
&lt;h4 id=&quot;抢占&quot;&gt;抢占&lt;/h4&gt;
&lt;p&gt;而当一个高优先级的pod调度失败的时候，调度器的抢占能力就会被触发。这时，调度器就会试图从当前集群里寻找一个节点，使得当这个节点上的一个或者多个低优先级pod被删除后，待调度的高优先级pod就可以被调度到这个节点上。这个过程，就是“抢占”这个概念的主要体现。&lt;/p&gt;

&lt;p&gt;当上述抢占过程发生时，抢占者并不会立刻被调度到被抢占的node上。事实上，调度器只会将抢占者的spec.nominatedNodeName字段，设置为被抢占的node的名字。然后，抢占者会重新进入下一个调度周期，然后在新的调度周期里来决定是不是要运行在被抢占的节点上。这当然也就意味着，即使在下一个调度周期，调度器也不会保证抢占者一定会运行在被抢占的节点上。&lt;/p&gt;

&lt;p&gt;这样设计的一个重要原因是，调度器只会通过标准的DELETE API来删除被抢占的pod，所以，这些pod必然是有一定的“优雅退出”时间（默认是30s）的。而在这段时间里，其他的节点也是有可能变成可调度的，或者直接有新的节点被添加到这个集群中来。所以，鉴于优雅退出期间，集群的可调度性可能会发生的变化，把抢占者交给下一个调度周期再处理，是一个非常合理的选择。而在抢占者等待被调度的过程中，如果有其他更高优先级的pod也要抢占同一个节点，那么调度器就会清空原抢占者的 spec.nominatedNodeName 字段，从而允许更高优先级的抢占者执行抢占，并且，这也就使得原抢占者本身，也有机会去重新抢占其他节点。这些都是设置 nominatedNodeName 字段的主要目的。&lt;/p&gt;

&lt;p&gt;而k8s调度器实现抢占算法的一个最重要的设计，就是在调度队列的实现里，使用了两个不同的队列。第一个队列，叫作activeQ。凡是在activeQ里的pod，都是下一个调度周期需要调度的对象。所以，当集群里新创建一个pod的时候，调度器会将这个pod入队到activeQ里面。而调度器不断从队列里出队（Pop）一个pod进行调度，实际上都是从activeQ里出队的。第二个队列，叫作unschedulableQ，专门用来存放调度失败的pod。而当一个unschedulableQ里的pod被更新之后，调度器会自动把这个pod移动到activeQ里。&lt;/p&gt;

&lt;p&gt;当调度失败之后，抢占者就会被放进unschedulableQ里面。然后，这次失败事件就会触发调度器为抢占者寻找牺牲者的流程。第一步，调度器会检查这次失败事件的原因，来确认抢占是不是可以帮助抢占者找到一个新节点。这是因为有很多predicate的失败是不能通过抢占来解决的。比如PodFitsHost算法（负责的是检查pod的nodeSelector与node的名字是否匹配）。第二步，如果确定抢占可以发生，那么调度器就会把自己缓存的所有节点信息复制一份，然后使用这个副本来模拟抢占过程。这里的抢占过程很容易理解。调度器会检查缓存副本里的每一个节点，然后从该节点上最低优先级的pod开始，逐一“删除”这些pod。而每删除一个低优先级pod，调度器都会检查一下抢占者是否能够运行在该node上。一旦可以运行，调度器就记录下这个node的名字和被删除pod的列表，这就是一次抢占过程的结果了。当遍历完所有的节点之后，调度器会在上述模拟产生的所有抢占结果里做一个选择，找出最佳结果。而这一步的判断原则，就是尽量减少抢占对整个系统的影响。比如，需要抢占的pod越少越好，需要抢占的pod的优先级越低越好等等。在得到了最佳的抢占结果之后，这个结果里的node，就是即将被抢占的 Node；被删除的pod列表，就是牺牲者。所以接下来，调度器就可以真正开始抢占的操作了，这个过程可以分为三步。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;第一步，调度器会检查牺牲者列表，清理这些pod所携带的 nominatedNodeName 字段。&lt;/li&gt;
  &lt;li&gt;第二步，调度器会把抢占者的nominatedNodeName，设置为被抢占的node的名字。&lt;/li&gt;
  &lt;li&gt;第三步，调度器会开启一个goroutine，同步地删除牺牲者。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;对于任意一个待调度pod来说，因为有上述抢占者的存在，它的调度过程其实是有一些特殊情况需要特殊处理的。具体来说，在为某一对pod和node执行predicate算法的时候，如果待检查的node是一个即将被抢占的节点，即调度队列里有nominatedNodeName字段值是该node名字的pod存在（可以称之为：“潜在的抢占者”）。那么调度器就会对这个node将同样的 predicate算法运行两遍。&lt;/p&gt;

&lt;p&gt;第一遍，调度器会假设上述“潜在的抢占者”已经运行在这个节点上，然后执行predicate算法；第二遍，调度器会正常执行predicates算法，即不考虑任何“潜在的抢占者”。而只有这两遍predicate算法都能通过时，这个pod和node才会被认为是可以绑定（bind）的。&lt;/p&gt;</content><author><name></name></author><summary type="html">资源模型与资源管理 资源限额配置 k8s中资源调度的单位是pod，对资源的需求都设置在pod的属性里，其中主要有cpu和memory等。在k8s中，cpu属于“可压缩”的资源，意思是当cpu资源不足时，pod只会产生“饥饿”，而不会退出；memory属于“不可压缩”的资源，当memory资源不足时，pod就会因为OOM而被kill掉。同时，pod内可以包含多个container，所以在包含多个container的pod中，对资源的需求实际上是配置在每个container里的，这样一来pod的资源配置就是其包含的所有container的资源配置的总和。</summary></entry><entry><title type="html">cgroups</title><link href="http://localhost:4000/2020/04/06/cgroups.html" rel="alternate" type="text/html" title="cgroups" /><published>2020-04-06T00:00:00+08:00</published><updated>2020-04-06T00:00:00+08:00</updated><id>http://localhost:4000/2020/04/06/cgroups</id><content type="html" xml:base="http://localhost:4000/2020/04/06/cgroups.html">&lt;h2 id=&quot;简介&quot;&gt;简介&lt;/h2&gt;
&lt;p&gt;cgroups是control groups的缩写，它是linux内核提供的一种可以管理进程对资源（cpu、内存、网络设备等）的使用的机制。cgroups被LXC（Linux Containers）用于实现虚拟化的资源管理手段，可以说是LXC的实现基础。cgroups主要的作用针对一组进程（进程组）声明一个控制组，在这个控制组下可以设置各种控制参数，然后通过不同的子系统（资源控制器）来解析各自的控制参数来对进程组内的进程的资源使用进行控制。&lt;/p&gt;

&lt;p&gt;cgroups主要提供以下几个方面的功能：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;限制资源使用。比如：memory子系统可以为进程组设定一个memory使用上限，一旦进程组使用的内存达到限额再申请内存，就会出发OOM（out of memory）&lt;/li&gt;
  &lt;li&gt;优先级控制。比如：可以使用cpu子系统为某个进程组分配特定cpu share&lt;/li&gt;
  &lt;li&gt;资源使用审计。比如：可以使用cpuacct子系统记录某个进程组使用的cpu时间&lt;/li&gt;
  &lt;li&gt;进程隔离。比如：使用ns子系统可以使不同的进程组使用不同的namespace，以达到隔离的目的，不同的进程组有各自的进程、网络、文件系统挂载空间&lt;/li&gt;
  &lt;li&gt;进程控制。比如：使用freezer子系统可以将进程组挂起和恢复&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;概念&quot;&gt;概念&lt;/h3&gt;
&lt;p&gt;cgroups中有许多概念，这里集中列举如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;任务（task），也就是系统中的一个进程&lt;/li&gt;
  &lt;li&gt;控制组（control group），控制组就是一组进程。cgroups中的资源控制都以控制组为单位。一个进程可以加入某个控制组，也可以从某个控制组转移到另一个控制组。一个进程组的进程可以使用cgroups以控制组为单位分配的资源，同时受到cgroups以控制组为单位设定的限制&lt;/li&gt;
  &lt;li&gt;层级（hierarchy），各个控制组实际上是通过树形的结构关联起来的，控制组树上的子节点会继承父节点的属性。&lt;/li&gt;
  &lt;li&gt;子系统（subsystem），一个子系统实际上就是一个资源控制器。前面提到cgroup会声明各种资源的使用限制，而具体负责执行这些限制的就是各个子系统，比如CPU子系统就是控制CPU时间分配的一个控制器。cgroups的子系统有很多，并且也可以支持新增。子系统必须附加（attach）到一个层级上才能起作用，一个子系统attach到某个层级以后，这个层级上的所有控制族群都受到这个子系统的控制。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;各个子系统&quot;&gt;各个子系统&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;cpu 子系统，主要限制进程的 cpu 使用率。&lt;/li&gt;
  &lt;li&gt;cpuacct 子系统，可以统计 cgroups 中的进程的 cpu 使用报告。&lt;/li&gt;
  &lt;li&gt;cpuset 子系统，可以为 cgroups 中的进程分配单独的 cpu 节点或者内存节点。&lt;/li&gt;
  &lt;li&gt;memory 子系统，可以限制进程的 memory 使用量。&lt;/li&gt;
  &lt;li&gt;blkio 子系统，可以限制进程的块设备 io。&lt;/li&gt;
  &lt;li&gt;devices 子系统，可以控制进程能够访问某些设备。&lt;/li&gt;
  &lt;li&gt;net_cls 子系统，可以标记 cgroups 中进程的网络数据包，然后可以使用 tc 模块（traffic control）对数据包进行控制。&lt;/li&gt;
  &lt;li&gt;freezer 子系统，可以挂起或者恢复 cgroups 中的进程。&lt;/li&gt;
  &lt;li&gt;ns 子系统，可以使不同 cgroups 下面的进程使用不同的 namespace。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;使用示例&quot;&gt;使用示例&lt;/h2&gt;

&lt;p&gt;说了这么多概念和介绍，接下来看一下如果查看和操作cgroup的相关配置。&lt;/p&gt;

&lt;h3 id=&quot;查看cgroup文件系统&quot;&gt;查看cgroup文件系统&lt;/h3&gt;

&lt;p&gt;cgroup在实现上把用户接口实现为文件系统的形式，我们可以通过执行&lt;code class=&quot;highlighter-rouge&quot;&gt;mount -t cgroup&lt;/code&gt;来查看cgroup挂载的文件系统：&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;~&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;mount &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; cgroup
cgroup on /sys/fs/cgroup/systemd &lt;span class=&quot;nb&quot;&gt;type &lt;/span&gt;cgroup &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;rw,nosuid,nodev,noexec,relatime,xattr,release_agent&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/lib/systemd/systemd-cgroups-agent,name&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;systemd&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
cgroup on /sys/fs/cgroup/pids &lt;span class=&quot;nb&quot;&gt;type &lt;/span&gt;cgroup &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;rw,nosuid,nodev,noexec,relatime,pids&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
cgroup on /sys/fs/cgroup/cpu,cpuacct &lt;span class=&quot;nb&quot;&gt;type &lt;/span&gt;cgroup &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;rw,nosuid,nodev,noexec,relatime,cpu,cpuacct&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
cgroup on /sys/fs/cgroup/hugetlb &lt;span class=&quot;nb&quot;&gt;type &lt;/span&gt;cgroup &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;rw,nosuid,nodev,noexec,relatime,hugetlb&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
cgroup on /sys/fs/cgroup/devices &lt;span class=&quot;nb&quot;&gt;type &lt;/span&gt;cgroup &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;rw,nosuid,nodev,noexec,relatime,devices&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
cgroup on /sys/fs/cgroup/net_cls,net_prio &lt;span class=&quot;nb&quot;&gt;type &lt;/span&gt;cgroup &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;rw,nosuid,nodev,noexec,relatime,net_cls,net_prio&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
cgroup on /sys/fs/cgroup/perf_event &lt;span class=&quot;nb&quot;&gt;type &lt;/span&gt;cgroup &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;rw,nosuid,nodev,noexec,relatime,perf_event&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
cgroup on /sys/fs/cgroup/blkio &lt;span class=&quot;nb&quot;&gt;type &lt;/span&gt;cgroup &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;rw,nosuid,nodev,noexec,relatime,blkio&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
cgroup on /sys/fs/cgroup/memory &lt;span class=&quot;nb&quot;&gt;type &lt;/span&gt;cgroup &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;rw,nosuid,nodev,noexec,relatime,memory&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
cgroup on /sys/fs/cgroup/freezer &lt;span class=&quot;nb&quot;&gt;type &lt;/span&gt;cgroup &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;rw,nosuid,nodev,noexec,relatime,freezer&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
cgroup on /sys/fs/cgroup/cpuset &lt;span class=&quot;nb&quot;&gt;type &lt;/span&gt;cgroup &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;rw,nosuid,nodev,noexec,relatime,cpuset&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;可以看到，在/sys/fs下有一个cgroup的目录，这是cgroup默认挂载的位置，下边有许多子目录，比如cpu、cpuset等等，这些就是不同的子系统的挂载点。每个挂载点即表示一棵与对应的子系统关联的cgroup树。&lt;/p&gt;

&lt;p&gt;在一棵cgroup树里面，会包含系统中的所有进程，但每个进程只能属于树的其中一个节点（进程组）。系统中可以有很多cgroup树，每棵树都和不同的subsystem关联，一个进程可以属于多棵树，即一个进程可以属于多个进程组，只是这些进程组和不同的subsystem关联。&lt;/p&gt;

&lt;p&gt;可以通过查看/proc/cgroups文件来确认当前系统支持的子系统，以及各个子系统绑定的cgroup树，比如：&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;~&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; /proc/cgroups
&lt;span class=&quot;c&quot;&gt;#subsys_name    hierarchy       num_cgroups     enabled&lt;/span&gt;
cpuset          11              1               1
cpu             3               64              1
cpuacct         3               64              1
blkio           8               64              1
memory          9               104             1
devices         5               64              1
freezer         10              4               1
net_cls         6               1               1
perf_event      7               1               1
net_prio        6               1               1
hugetlb         4               1               1
pids            2               68              1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;输出结果的第一列是子系统的名称，比如cpuset等等，第二列是子系统绑定的cgroup树id，可以注意到cpu和cpuacct绑定到了同一个cgroup树，也就是说它们在/sys/fs/cgroup下挂载的路径是相同的，实际上是/sys/fs/cgroup/cpu,cpuacct，然后cgroup目录下还会有两个名为cpu和cpuacct的链接，链接到它们共同的挂载点。第三列和第四列的输出分别表示cgroup树下有多少控制组（也就是树的节点个数）以及是否开启（可以通过设置内核的启动参数“cgroup_disable”来控制子系统的开启）。&lt;/p&gt;

&lt;h3 id=&quot;更新cgroup配置&quot;&gt;更新cgroup配置&lt;/h3&gt;

&lt;p&gt;假如我们想要控制某个进程的cpu使用不能超过30%，我们就可以这样操作：&lt;/p&gt;

&lt;p&gt;1、在/sys/fs/cgroup/cpu下新建一个名为“demo”的目录，这个操作实际上就是在cpu这个子系统绑定的cgroup树下面创建了一个新的节点，也就是控制组。&lt;/p&gt;

&lt;p&gt;创建完demo目录后，我们可以看到demo目录下会自动新增很多文件，实际上这些文件在上一层的cpu目录下也存在。这些自动生成的文件实际上就是cgroup的各个控制属性，而前面提到过，作为cgroup树的子节点会继承父节点的特定属性，所以这些自动生成的文件也就是从cpu目录继承过来的。&lt;/p&gt;

&lt;p&gt;各个文件的作用简单介绍如下：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;cgroup.clone_children：这个文件只对cpuset（subsystem）有影响，当该文件的内容为1时，新创建的cgroup将会继承父cgroup的配置，即从父cgroup里面拷贝配置文件来初始化新cgroup&lt;/li&gt;
  &lt;li&gt;cgroup.procs：当前cgroup中的所有进程ID，系统不保证ID是顺序排列的，且ID有可能重复&lt;/li&gt;
  &lt;li&gt;release_agent：里面包含了cgroup退出时将会执行的命令，系统调用该命令时会将相应cgroup的相对路径当作参数传进去。 注意：这个文件只会存在于root cgroup下面，其他cgroup里面不会有这个文件。&lt;/li&gt;
  &lt;li&gt;tasks当前cgroup中的所有线程ID，系统不保证ID是顺序排列的&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、将cpu限额设置为20%，这里需要修改cpu.cfs_quota_us文件里的内容：&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; /sys/fs/cgroup/cpu/demo/cpu.cfs_quota_us 
&lt;span class=&quot;nt&quot;&gt;-1&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;echo &lt;/span&gt;20000 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; /sys/fs/cgroup/cpu/demo/cpu.cfs_quota_us
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;3、将要限制的进程id（比如3529）添加到cgroup下，这里需要修改tasks文件的内容：&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;echo &lt;/span&gt;3529 &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; /sys/fs/cgroup/cpu/haoel/tasks
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;这样一来3529这个进程就会被cgroup的cpu子系统限制cpu的使用百分比为20%了。&lt;/p&gt;

&lt;p&gt;值得注意的是这里将进程id加入了tasks文件而不是cgroup.procs下，这两者的区别在于tasks支持线程级别的控制，而cgroup.proc是进程级别的控制。&lt;/p&gt;

&lt;h2 id=&quot;cgroups的实现&quot;&gt;cgroups的实现&lt;/h2&gt;

&lt;p&gt;cgroups通过实现cgroup文件系统来为用户提供管理cgroup的工具，而cgroup文件系统是基于Linux VFS实现的。相应地，cgroups为控制文件定义了相应的数据结构cftype，对其操作由cgroup文件系统定义的通过操作捕获，再调用cftype定义的具体实现。所以说上述的例子中我们对相关的文件进行操作时，实际上被cgroup捕获并转换成了对内核中cgroup相应的数据结构的操作，也就完成了用户态操作到内核态操作的映射。&lt;/p&gt;</content><author><name></name></author><summary type="html">简介 cgroups是control groups的缩写，它是linux内核提供的一种可以管理进程对资源（cpu、内存、网络设备等）的使用的机制。cgroups被LXC（Linux Containers）用于实现虚拟化的资源管理手段，可以说是LXC的实现基础。cgroups主要的作用针对一组进程（进程组）声明一个控制组，在这个控制组下可以设置各种控制参数，然后通过不同的子系统（资源控制器）来解析各自的控制参数来对进程组内的进程的资源使用进行控制。</summary></entry><entry><title type="html">k8s NetworkPolicy和Service</title><link href="http://localhost:4000/2020/03/29/k8s-service-ingress.html" rel="alternate" type="text/html" title="k8s NetworkPolicy和Service" /><published>2020-03-29T00:00:00+08:00</published><updated>2020-03-29T00:00:00+08:00</updated><id>http://localhost:4000/2020/03/29/k8s%20service%20ingress</id><content type="html" xml:base="http://localhost:4000/2020/03/29/k8s-service-ingress.html">&lt;h2 id=&quot;networkpolicy&quot;&gt;NetworkPolicy&lt;/h2&gt;

&lt;p&gt;k8s默认情况下，所有pod都是连通的。如果有隔离的需求，可以通过设置NetworkPolicy来实现。NetworkPolicy在namespace范围下工作，它可以根据podSelector选中某些pod。而被NetworkPolicy选中的pod就进入了“默认隔离”的状态，只有符合NetworkPolicy中规则的流量才会被放行。值得注意的是NetworkPolicy需要对应的CNI插件支持，也就是说NetworkPolicy只提供了隔离规则的声明，而实际根据声明进行隔离操作的则是具体的网络实现方案，比如Calico、Weave、kube-router等，但是Flannel并不支持。&lt;/p&gt;

&lt;h3 id=&quot;配置的格式&quot;&gt;配置的格式&lt;/h3&gt;

&lt;p&gt;一个完整的NetworkPolicy的定义如下所示：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;NetworkPolicy&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; 
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;test-network-policy&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;default&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;podSelector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;role&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;db&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;policyTypes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Ingress&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Egress&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;ingress&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;ipBlock&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;cdir&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;172.17.0.0/16&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;expect&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;172.17.1.0/24&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;namespaceSelector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;project&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;myproject&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;podSelector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;role&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;frontend&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;6379&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;egress&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;ipBlock&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;cidr&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;10.0.0.0/24&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;5978&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;首先spec中的podSelector字段表示这个配置只影响default namespace下的带role=db标签的pod。policyTypes字段中声明了这个配置既影响流入（Ingress）的流量也影响（Egress）流出的流量，意思是被选中的pod的流入流量和流出流量都变成了默认拒绝，只有符合下面规则的流量才会被放行。如果没有声明policyTypes，则默认只影响流入的流量；如果没有声明policyTypes但是规则中声明了egress规则，则也会影响流出的流量。&lt;/p&gt;

&lt;p&gt;规则定义方面，示例配置中声明了目标端口是6379的TCP协议的流入流量，同时满足以下任意条件的流量才会被放行：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;源地址属于172.17.0.0/16网段但不属于172.17.1.0/24&lt;/li&gt;
  &lt;li&gt;来源pod的namespace中有project=myproject标签&lt;/li&gt;
  &lt;li&gt;来源pod有role=frontend标签&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;值得注意的是规则声明中from和to部分下的内容以&lt;code class=&quot;highlighter-rouge&quot;&gt;-&lt;/code&gt;开头表示“或”的关系，没有&lt;code class=&quot;highlighter-rouge&quot;&gt;-&lt;/code&gt;开头的规则则表示“与”的关系。&lt;/p&gt;

&lt;p&gt;流出规则的作用类似，就不叙述了。&lt;/p&gt;

&lt;h3 id=&quot;配置的实现&quot;&gt;配置的实现&lt;/h3&gt;

&lt;p&gt;前面提到了，NetworkPolicy需要配套的网络方案实现，而在具体实现中，所有支持NetworkPolicy的插件都维护着一个NetworkPolicy Controller，通过控制循环的方式来NetworkPolicy对象的变化，然后在宿主机上进行iptables的配置。iptables的规则大概有几条：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;拦截同一台宿主机上通过CNI网桥进行通信的流量&lt;/li&gt;
  &lt;li&gt;拦截跨宿主机通信的流量&lt;/li&gt;
  &lt;li&gt;将拦截的流量转交给NetworkPolicy的规则进行匹配&lt;/li&gt;
  &lt;li&gt;匹配失败的流量则直接拒绝&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;service&quot;&gt;Service&lt;/h2&gt;

&lt;p&gt;Service是k8s内置的资源类型，它的作用就是将一组pod通过一个单一的ip和dns记录暴露出去，同时提供负载均衡。&lt;/p&gt;

&lt;h3 id=&quot;配置格式&quot;&gt;配置格式&lt;/h3&gt;

&lt;p&gt;一个简单的Service配置如下所示：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;hostnames&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;hostnames&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;default&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;80&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;targetPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;9376&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;示例中的Service表示它只代理携带了app=hostnames标签的pod，同时这个Service对外的端口是80，被代理的pod的端口则是9376。被代理的pod也被称为Endpoint，只有处于Running状态同时readinessProbe检查通过的Pod才会被Service纳入Endpoint列表中，而且当一个pod出现问题时，k8s会将其自动摘掉。当成功创建一个Service时，k8s会为其分配一个vip（也称为cluster ip），通过Service的vip就能顺利访问其代理的后端pod了。&lt;/p&gt;

&lt;p&gt;此外，k8s还会为Service的vip创建一条DNS记录，格式为..svc.cluster.local，当访问其对应的DNS记录时，解析到的实际是Service对应的vip地址。然而对于制定了clusterIP=None的Headless Service来说，其DNS对应的记录解析到的结果是其代理的pod的ip的集合。对于ClusterIP模式的Service来说，它代理的pod会自动分配一个DNS记录，格式是..pod.cluster.local，这条记录则指向pod的ip地址。&lt;/p&gt;

&lt;p&gt;除了通过selector指定pod以外，k8s还支持显示指定Service和Endpoint的绑定关系，这需要首先声明一个不带selector字段的Service，比如：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;my-service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;80&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;targetPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;9376&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;然后再通过声明一个Endpoint类型对象，通过metadata中的name字段把它和Service关联起来：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Endpoints&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;my-service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;subsets&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;addresses&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.0.2.42&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;9376&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这样一来，当我们通过vip访问my-service时，请求就会直接转发到192.0.2.42:9376&lt;/p&gt;

&lt;h3 id=&quot;具体实现&quot;&gt;具体实现&lt;/h3&gt;

&lt;p&gt;Service实际上是通过kube-proxy组件加上iptables来实现的。Service提供多种代理模式：用户空间代理、iptables代理以及IPVS代理。&lt;/p&gt;

&lt;h4 id=&quot;本地进程代理&quot;&gt;本地进程代理&lt;/h4&gt;

&lt;p&gt;这个模式下，kube-proxy会监听Service和Endpoint的变化，然后在宿主机上启动一个代理端口（随机选择），当pod中发起连接到代理端口的连接时，这个连接会被桥接到Service实际的被代理pod上，在进行负载均衡时，还会参考&lt;code class=&quot;highlighter-rouge&quot;&gt;SessionAffinity&lt;/code&gt;相关的配置。&lt;/p&gt;

&lt;p&gt;然后kube-proxy会设置一系列的iptables规则，将发往Service vip的流量转发到代理端口上。&lt;/p&gt;

&lt;p&gt;默认情况下，kube-proxy使用round-robin的规则来进行负载均衡。&lt;/p&gt;

&lt;h4 id=&quot;iptables代理&quot;&gt;iptables代理&lt;/h4&gt;

&lt;p&gt;这个模式下，kube-proxy会监听Service和Endpoint的变化，然后在宿主机上设置iptables规则，将发往Service vip的流量转发到实际的被代理的pod上，这个过程中会随机选择一个pod。这个模式比起本地代理性能会更高，也更为可靠。&lt;/p&gt;

&lt;p&gt;当被选中的pod无法响应时，本次请求就会直接失败；而本地代理模式下，代理进程会检测到失败并将连接切换到新的pod上。当然在这里可以通过pod的readinessProbe来进行pod的健康检查。&lt;/p&gt;

&lt;h4 id=&quot;ipvs代理&quot;&gt;ipvs代理&lt;/h4&gt;

&lt;p&gt;这个模式下，kube-proxy会监听Service和Endpoint的变化，然后通过&lt;code class=&quot;highlighter-rouge&quot;&gt;netlink&lt;/code&gt;创建和定期维护ipvs规则。当pod需要访问Service时，ipvs会直接将流量转发到对应的目标pod。ipvs模式比起前两个模式性能更好，同时对大流量的场景支持的也更好（iptables会占用很多宿主机的CPU资源）。除此之外，ipvs模式还支持多种负载均衡策略比如round-robin、least connection、destination hash、source hash等等。&lt;/p&gt;

&lt;p&gt;值得注意的是要使用ipvs模式需要宿主机支持ipvs功能，如果设置了ipvs模式但是宿主机不支持ipvs功能，则会fallback到iptables代理模式。&lt;/p&gt;

&lt;h3 id=&quot;集群外访问service&quot;&gt;集群外访问Service&lt;/h3&gt;

&lt;p&gt;很明显Service相关的路由信息等只有在k8s集群内的才能访问到，而在k8s外想要访问Service，则需要一些额外的配置。&lt;/p&gt;

&lt;h4 id=&quot;nodeport&quot;&gt;NodePort&lt;/h4&gt;

&lt;p&gt;如果将Service的type字段设置为NodePort，k8s就会在每个node将那个端口代理到指定的Service中。可以在spec.ports.nodePort字段中要求分配的端口，如果不指定则会分配一个随机的端口（具体范围是–service-node-port-range标志指定的范围内，默认值：30000-32767）。比如：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;my-nginx&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;my-nginx&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;NodePort&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;nodePort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;8080&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;targetPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;80&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;http&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;nodePort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;443&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;https&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;my-nginx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;采用NodePort方式部署Service，我们可以在集群内任意node通过指定端口访问实际的Service了。&lt;/p&gt;

&lt;h4 id=&quot;loadbalancer&quot;&gt;LoadBalancer&lt;/h4&gt;

&lt;p&gt;除了NodePort方式，我们还可以利用一些公有云提供的负载均衡能力来对外暴露Service。通过把Service的type设置为LoadBalancer，可以触发外部负载均衡的创建，来自外部的流量就能被转发到实际的pod了。&lt;/p&gt;

&lt;h4 id=&quot;externalip&quot;&gt;ExternalIP&lt;/h4&gt;

&lt;p&gt;k8s还支持为Service指定ExternalIP属性，可以将一个ip直接绑定到Service，这样访问这个ip时就可以直接访问到Service代理的pod中。但这个特性要求ExternalIP中设置的ip必须能够路由到k8s中的一个node上。&lt;/p&gt;

&lt;h4 id=&quot;externalname&quot;&gt;ExternalName&lt;/h4&gt;

&lt;p&gt;k8s 1.7版本之后，支持了一个叫做ExternalName的新特性，将Service的type设置为ExternalName并设置了externalName属性之后，就可以将Service和externalName直接关联起来。这个方式实际上是将Service的DNS解析结果设置为externalName的解析结果，所以k8s要求externalName必须是一个DNS域名格式。&lt;/p&gt;

&lt;p&gt;ExternalName的作用更类似于将外部的服务包装成一个k8s的Service对象，比如我有一个数据库集群并不部署在k8s中，但是我可以将数据库集群的vip绑定到一个Service，这样k8s内的pod就可以通过这个Service访问这个数据库集群了。&lt;/p&gt;

&lt;h3 id=&quot;ingress&quot;&gt;Ingress&lt;/h3&gt;

&lt;p&gt;除了上述的将Service对外暴露的方式之外，k8s还提供了一个统一的抽象–Ingress。简单来说，Ingress就是多个Service的上层路由，可以将不同的流量路由到不同的Service上，而这只需要配置一个Ingress对象即可。举个例子：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Ingress&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cafe-ingress&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;tls&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;hosts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cafe.example.com&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;secretName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cafe-secret&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cafe.example.com&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;http&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;paths&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/tea&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;serviceName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;tea-svc&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;servicePort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;80&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/coffee&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;serviceName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;coffee-svc&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;servicePort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;上面的配置中，spec.hosts下配置了Ingress的入口，也就是cafe.example.com，也就是当访问cafe.example.com这个host时，实际上就会访问到这个Ingress对象。rules下配置了两个规则，表示根据访问的路径不同，Ingress需要把流量转发到不同的Service下。&lt;/p&gt;

&lt;p&gt;和其他资源类型类似，Ingress类型也需要对应的controller配合使用，Ingress的定义在k8s中创建之后，需要有具体的controller来创建具体的代理。目前Ingress controller的实现有很多，包括 Google Cloud Load Balancer， Nginx，Contour，Istio等等。他们都会根据Ingress的配置创建出实际的可工作的服务代理。&lt;/p&gt;

&lt;p&gt;通过配置Ingress对象配合实际的Ingress controller实现，我们就可以省去将每个Service对外暴露的步骤，直接将Service配置到Ingress的下游即可。&lt;/p&gt;</content><author><name></name></author><summary type="html">NetworkPolicy</summary></entry><entry><title type="html">k8s网络模型</title><link href="http://localhost:4000/2020/03/15/k8s-network.html" rel="alternate" type="text/html" title="k8s网络模型" /><published>2020-03-15T00:00:00+08:00</published><updated>2020-03-15T00:00:00+08:00</updated><id>http://localhost:4000/2020/03/15/k8s%20network</id><content type="html" xml:base="http://localhost:4000/2020/03/15/k8s-network.html">&lt;h2 id=&quot;k8s对网络连通性的约定&quot;&gt;k8s对网络连通性的约定&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;所有的Pod之间可以在不使用NAT网络地址转换的情况下相互通信&lt;/li&gt;
  &lt;li&gt;所有的Nodes之间可以在不使用NAT网络地址转换的情况下相互通信&lt;/li&gt;
  &lt;li&gt;每个Pod自己看到的自己的ip和其他Pod看到的一致&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;同一个pod下的容器是如何通信的&quot;&gt;同一个pod下的容器是如何通信的&lt;/h3&gt;

&lt;p&gt;每个pod拥有独立的ip，而同一个pod下的多个容器则共享用一个网络namespace，它们之间的通信可以直接使用localhost。而这个网络namespace的初始化是在pause容器（infra容器）当中初始化的，同一个pod下的其他容器都通过&lt;code class=&quot;highlighter-rouge&quot;&gt;–net=container:NAMEorID&lt;/code&gt;的形式加入pause容器的网络namespace。&lt;/p&gt;

&lt;h4 id=&quot;network-namespace&quot;&gt;network namespace&lt;/h4&gt;

&lt;p&gt;网络namespace是linux中提供网络资源隔离的机制，其包括网络设备、IP协议栈（IPv4和IPv6）、IP路由表、防火墙规则等等。linux同时也提供了veth的机制，使得不同的namespace之间也可以进行数据传输。&lt;/p&gt;

&lt;h3 id=&quot;同一台node上的pod之间是如何通信的&quot;&gt;同一台node上的pod之间是如何通信的&lt;/h3&gt;

&lt;p&gt;默认情况下，容器的网络模式为bridge模式，即在宿主机上创建名为”docker0”的网桥，每个启动的pod都将自己的eth0网卡通过veth设备连接到宿主机的docker0网桥上。也就是说，对于每个pod来说，它的eth0网卡都是一个veth pair的一端，而另一端则连接在宿主机的docker0网桥上。&lt;/p&gt;

&lt;p&gt;当一个pod向同一个node上的另一个pod发送数据时，数据包会通过自身的eth0网卡传递到veth pair的另一端，也就是宿主机的docker0网桥上。网桥在收到数据包时，则会根据数据包的目标MAC地址进行查询，确认目标是否是连接在自身的其他pod。当网桥发现了目标pod时，网桥就会把数据包转发到连接在自身的目标pod的veth pair，然后这些数据就会出现在目标pod的eth0网卡上了。具体示意图如下：
&lt;img src=&quot;/img/podonnode.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;veth-和-bridge&quot;&gt;veth 和 bridge&lt;/h4&gt;

&lt;p&gt;veth是linux系统中提供的一种在不同网络namespace之间通信的机制。当创建一个veth设备时，系统会返回两个互相连通的端点，称为veth pair。从任意一端发送的数据会被立刻传输到另一端上。&lt;/p&gt;

&lt;p&gt;当一个veth pair连接到网桥上时，它就会变为网桥的一个“端口”，从端口传入的数据会由网桥处理，而网桥会判断数据包的转发或者丢弃，然后将数据包发送到某个端口上。&lt;/p&gt;

&lt;p&gt;当docker0网桥被创建时，同时还会在系统的路由表中添加路由信息，指定将目标为该宿主机上的pod的数据包交给网桥处理。这样通过IP向宿主机上的pod发送数据时，数据就会交给网桥处理并转发到指定pod了。&lt;/p&gt;

&lt;h4 id=&quot;node上的进程到pod是如何通信的&quot;&gt;node上的进程到pod是如何通信的&lt;/h4&gt;

&lt;p&gt;根据前面提到的内容，node上的其他进程（比如agent）也可以通过ip向pod发送数据时，数据包也会先到达网桥，然后由网桥转发到对应的pod。&lt;/p&gt;

&lt;h3 id=&quot;不同node上的pod之间是如何通信的&quot;&gt;不同node上的pod之间是如何通信的&lt;/h3&gt;

&lt;p&gt;既然网桥可以处理同一宿主机上的不同pod，那么把所有node上的所有pod都连接到同一个网桥上就可以实现互相访问了。或者其实不一定要网桥，只要能够让宿主机直到如何转发pod发送的数据包就可以了，宿主机通过某种机制再转发到实际的pod就可以了。k8s下许多网络方案都是在这个思路下进行的。&lt;/p&gt;

&lt;h2 id=&quot;flannel&quot;&gt;flannel&lt;/h2&gt;

&lt;p&gt;flannel是coreos提供的跨宿主机的容器网络方案的框架，而flannel目前提供三种后端实现：UDP、VXLAN和host gw。&lt;/p&gt;

&lt;p&gt;flannel的工作原理就是在每台宿主机上部署一个agent进程（称为flanneld），flanneld负责在宿主机上配置各种路由信息，来协助宿主机判断数据包的转发目标。每台宿主机都由flanneld划分一个子网，具体的配置信息都存储在etcd中。子网划分的目的在于将同一宿主机上的pod都划给同一个子网，那么数据路由就可以以node为粒度，pod跨node通信时就可以先转发到node，再由node转发到pod。&lt;/p&gt;

&lt;h3 id=&quot;udp模式&quot;&gt;UDP模式&lt;/h3&gt;

&lt;p&gt;UDP模式是最初的实现，但因为其性能等因素目前已经不建议采用了，但是却是最简单直观的方案。&lt;/p&gt;

&lt;p&gt;当node1上的pod1根据ip向node2上的pod2上发送数据时，会发生以下的事情：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;数据会首先出现在node1的docker0网桥上&lt;/li&gt;
  &lt;li&gt;由于node1的docker0并不处理发到pod2的数据，所以数据包会按照node1上配置的路由信息进行处理&lt;/li&gt;
  &lt;li&gt;flanneld会在node1上增加额外的路由信息，指定将发往pod2所在网段的数据转发到flannel0的设备上&lt;/li&gt;
  &lt;li&gt;flannel0实际上是一个TUN设备（Tunnel设备），它会将收到的数据包传递到flanneld进程&lt;/li&gt;
  &lt;li&gt;flanneld收到数据包后，发现是发给pod2的，于是根据其维护的路由信息将数据包转发到node2上的flanneld进程&lt;/li&gt;
  &lt;li&gt;node2上的flanneld进程收到数据包之后，再将数据发送到node2上的flannel0设备上&lt;/li&gt;
  &lt;li&gt;node2上的flannel0收到数据之后，会根据数据包的目标地址转发到docker0网桥上&lt;/li&gt;
  &lt;li&gt;docker0网桥收到数据后，将其转发到pod2的eth0上，数据包的传递就完成了&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/img/flannel_udp.png&quot; alt=&quot;flannel udp&quot; /&gt;
之所以叫UDP模式，是因为第5步中flanneld之间是通过UDP通信的，每个flanneld都监听指定端口（8285），而发送时则将数据发送到目标node的指定端口即可。这里使用UDP的原因，大概是因为网络层本来就不保证传输的可靠性，使用UDP更加简单直接，节省资源。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TUN设备&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在linux中，TUN设备是一种工作在三层（Network Layer）的虚拟网络设备。TUN设备的功能就是在操作系统内核和用户应用程序之间传递IP包。&lt;/p&gt;

&lt;h3 id=&quot;vxlan&quot;&gt;VXLAN&lt;/h3&gt;

&lt;p&gt;VXLAN 即Virtual Extensible LAN（虚拟可扩展局域网），是linux内核本身就支持的一种网络虚似化技术。所以说，VXLAN可以完全在内核态实现上述UDP模式封装和解封装的工作，从而通过与前面相似的“隧道”机制，构建出覆盖网络（Overlay Network）。&lt;/p&gt;

&lt;p&gt;VXLAN的思路是在现有的三层（网络层）网络上增加一层虚拟的二层（数据链路层）网络，连接到这个虚拟的二层网络上的主机都可以像同一个局域网内的主机一样互相通信。为了让不同主机在这个虚拟的网络上连通，VXLAN在宿主机上创建了一种VTEP（VXLAN Tunnel Endpoint）设备。VTEP的作用实际上和UDP模式中的flanneld进程类似，都是将数据包进行封装和拆封。但是VTEP封装的对象是一个数据帧（对应数据链路层），整个逻辑都可以在内核态完成。&lt;/p&gt;

&lt;p&gt;为了组成虚拟的局域网，发送端的VTEP设备需要将原始的IP数据包封装成一个二层网络的数据帧。而一个数据帧则需要知道目标VTEP设备的MAC地址，这里的MAC地址则由flanneld进程维护，VTEP能够顺利地拿到目标MAC地址，并组装一个数据帧。后边的过程就跟UDP模式非常类似了，随后这个数据帧会被添加一个VXLAN header标识，表明这是一个VXLAN数据帧，接收端在看到VXLAN header标识之后就能够判断应该将数据交给VTEP设备处理。随后这个携带了VXLAN header的数据帧就会被通过UDP发送到目标node上，同样，目标node的地址等信息也由flanneld维护。&lt;/p&gt;

&lt;h3 id=&quot;host-gw模式&quot;&gt;host gw模式&lt;/h3&gt;

&lt;p&gt;host gw模式的工作原理就比较简单了，它不需要额外的手段将数据包封装后再转发，而是通过在路由信息中指定每个子网的下一跳为对应宿主机的ip。比如某个pod2的ip是10.244.1.3，其所在的宿主机node2的ip为10.168.0.3，对应的子网为10.244.1.0/24; 那么host gw模式下flannel会在每台宿主机上都添加这样一条路由规则：&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;
10.244.1.0/24 via 10.168.0.3 dev eth0
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;这条规则表示目的ip地址属于10.244.1.0/24网段的数据包，应该经过本机的eth0设备发出去（即：dev eth0）；并且它下一跳地址（next-hop）是10.168.0.3（即：via 10.168.0.3）。下一跳信息的作用在于当ip数据包从网络层进入链路层封装成帧的时候，eth0设备就会使用下一跳地址对应的MAC地址作为该数据帧的目的MAC地址。这样一来，所有其他pod发往pod1的数据包都会在其所属的宿主机上，通过路由信息发送到pod1对应的宿主机上，宿主机在收到数据包后，就能转发给pod1，这样数据的传输就完成了。host gw模式的工作原理示意图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/host_gw.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可想而知，所有的子网和宿主机之间的对应关系以及路由信息也有flannel管理，最终保存在etcd中，每个宿主机只要通过api关注etcd的变化，并及时更新路由就可以了。&lt;/p&gt;

&lt;p&gt;host gw模式通过ip路由的下一跳来转发ip数据包，不需要额外的数据封装，所以性能也会由于UDP模式和VXVLAN模式。但是这个模式有一个要求，那就是所有宿主机必须本来就是在第二层（数据链路层）互通的。&lt;/p&gt;

&lt;h2 id=&quot;calico&quot;&gt;Calico&lt;/h2&gt;

&lt;p&gt;Calico与flannel类似，也是k8s中网络方案的一种实现。其工作原理几乎和flannel的host gw模式一样，但是Calico并不通过etcd和每台宿主机上运行的agent进程来管理路由信息，而是使用了BGP来在整个集群中自动分发路由信息。&lt;/p&gt;

&lt;p&gt;Calico主要有以下几个组件组成：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Felix：运行在每台宿主机上的agent进程，负责维护路由信息等。&lt;/li&gt;
  &lt;li&gt;etcd：数据存储&lt;/li&gt;
  &lt;li&gt;BIRD：BGP client，主要负责分发路由信息，默认每台宿主机上的BIRD都要和其他所有宿主机进行通信&lt;/li&gt;
  &lt;li&gt;BGP Route Reflector：大规模集群下用到的组件，主要用于减少宿主机BIRD之间的通信。通过制定某几个节点作为中心，其他节点则只与中心进行通信。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;BGP（边界网关协议）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;BGP协议主要用于在大规模网络中实现路由节点信息的同步和共享，具体介绍则可以参考&lt;a href=&quot;https://zh.wikipedia.org/wiki/%E8%BE%B9%E7%95%8C%E7%BD%91%E5%85%B3%E5%8D%8F%E8%AE%AE&quot;&gt;这个链接&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;除了BGP，Calico还有另一个地方与flannel不同，那就是Calico没有使用网桥，而是直接在容器和宿主机之间用veth pair进行连接。Calico的工作原理示意图如下：
&lt;img src=&quot;/img/calico.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Calico会针对每个容器都创建一个veth pair用于连接宿主机和容器，从容器发出的数据会直接出现在宿主机的对应veth设备上，最终通过路由信息转到eth0设备然后发送出去；而由于没有网桥，Calico还需要添加额外的路由信息来用于接收传入到容器的数据包。比如node2的container4对应的配置：&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;
10.233.2.3 dev cali5863f3 scope link
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;这个配置表示发往10.233.2.3的数据都发送到cali5863f3设备上，也就进入了container4。&lt;/p&gt;

&lt;h3 id=&quot;ipip模式&quot;&gt;IPIP模式&lt;/h3&gt;

&lt;p&gt;对于flannel中host gw模式下要求宿主机必须在二层连通的限制，Calico也同样有这个限制，所以Calico提供了IPIP模式来解决这个问题。在IPIP模式下，容器发出的数据包会通过一个特殊的设备“tul0”发出。Calico的IPIP模式的工作原理示意图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/calico_ipip.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这个tunl0设备实际上是一个 IP隧道（IP tunnel）设备。数据包通过这个设备发送时，会在原始数据包的基础上在封装一个IP包头，同时将目标地址设置为原始ip数据包的下一跳地址。这样从容器到宿主机的数据包就被封装成了一个从宿主机到宿主机的数据包，也就可以通过宿主机之间的转发顺利到达目标宿主机了。而目标宿主机收到数据包后会先进行拆封，再将原始的数据包通过veth设备发送到目标容器。&lt;/p&gt;

&lt;p&gt;很显然，当Calico开启IPIP模式时，也会有因为封装和拆封导致的性能损耗，所以实际场景下推荐尽量让所有宿主机都位于同一子网下。&lt;/p&gt;

&lt;h2 id=&quot;k8s-cni插件&quot;&gt;k8s CNI插件&lt;/h2&gt;

&lt;p&gt;k8s中对于网络容器的操作主要通过CNI插件实现。k8s中通过CNI插件维护一个名为“cni0”的网桥来代替docker0网桥。cni0的作用就是接管宿主机下所有pod的网络。CNI的工作原理就是k8s在infra容器（pause容器）启动时，通过插件来初始化容器的网络栈。&lt;/p&gt;

&lt;h3 id=&quot;cni插件的部署&quot;&gt;CNI插件的部署&lt;/h3&gt;

&lt;p&gt;在部署k8s的过程中，有一个步骤是安装kubernetes-cni包，它的目的就是在宿主机上安装CNI插件所需的基础可执行文件，包括bridge、dhcp、portmap等等。这些 CNI 的基础可执行文件，按照功能可以分为三类：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;第一类，叫作Main插件，它是用来创建具体网络设备的二进制文件&lt;/li&gt;
  &lt;li&gt;第二类，叫作 IPAM（IP Address Management）插件，它是负责分配 IP 地址的二进制文件&lt;/li&gt;
  &lt;li&gt;第三类，是由 CNI 社区维护的内置 CNI 插件。比如：flannel，就是专门为 Flannel 项目提供的 CNI 插件；tuning，是一个通过 sysctl 调整网络设备参数的二进制文件；portmap，是一个通过 iptables 配置端口映射的二进制文件；bandwidth，是一个使用 Token Bucket Filter (TBF) 来进行限流的二进制文件。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;cni的实现&quot;&gt;CNI的实现&lt;/h3&gt;

&lt;p&gt;要实现一个k8s下的容器网络方案，主要的工作内容分为两个部分：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;实现网络方案本身，比如对应flannel来说，主要指实现flanneld进程的逻辑，比如创建和配置网络设备和路由信息等等。&lt;/li&gt;
  &lt;li&gt;实现对应的CNI插件，这部分主要指配置 Infra 容器里面的网络栈，并把它连接在 CNI 网桥上。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;cni的工作流程&quot;&gt;CNI的工作流程&lt;/h3&gt;

&lt;p&gt;当k8s在处理容器网络相关的逻辑时，主要工作都通过CRI（Container Runtime Interface，容器运行时接口）来完成，CRI对于docker来说，就是dockershim。&lt;/p&gt;

&lt;p&gt;dockershim在初始化网络配置时，会宿主机的cni配置目录下（/etc/cni/net.d) 找到对应第一个配置文件，然后加载其内容并执行。 在执行插件前，dockershim会初始化一系列CNI插件的参数，这些参数主要分为两部分：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;CNI环境变量，其中包括CNI_COMMAND。CNI_COMMAND的取值有ADD和DEL，分别表示将容器添加到CNI网络中和将容器从CNI网络中移除。这两个操作也是CNI插件要实现的两个方法，对于网桥类型的CNI插件来说，这两个操作意味着把容器以veth pair的方式“插”到 CNI 网桥上，或者从网桥上“拔”掉。&lt;/li&gt;
  &lt;li&gt;CNI配置文件里指定的默认插件的配置信息。dockershim会以JSON数据的格式，通过标准输入（stdin）的方式传递给CNI插件。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;值得注意的是，k8s目前不支持多个CNI插件混用。如果在CNI配置目录（/etc/cni/net.d）里放置了多个CNI配置文件的话，dockershim 只会加载按字母顺序排序的第一个插件。但另一方面，CNI允许在一个CNI配置文件里通过plugins字段定义多个插件进行协作，比如flannel的配置文件中就指定了flannel和portmap两个插件。&lt;/p&gt;</content><author><name></name></author><summary type="html">k8s对网络连通性的约定</summary></entry><entry><title type="html">k8s client-go informer机制简介</title><link href="http://localhost:4000/2020/03/08/k8s-informer.html" rel="alternate" type="text/html" title="k8s client-go informer机制简介" /><published>2020-03-08T00:00:00+08:00</published><updated>2020-03-08T00:00:00+08:00</updated><id>http://localhost:4000/2020/03/08/k8s%20informer</id><content type="html" xml:base="http://localhost:4000/2020/03/08/k8s-informer.html">&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;client-go&lt;/code&gt; 是k8s提供的用于和k8s集群交互的sdk，根据k8s集群版本不同分为两个维护分支：&lt;code class=&quot;highlighter-rouge&quot;&gt;v0.x.y&lt;/code&gt;版本（对应高于或等于0.17.0版本的k8s集群）和&lt;code class=&quot;highlighter-rouge&quot;&gt;kubernetes-1.x.y&lt;/code&gt;版本（对应低于0.17.0版本的k8s集群）。其主要功能包括以下几个部分：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kubernetes&lt;/code&gt; 与k8s api通信&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;discovery&lt;/code&gt; 服务发现相关功能&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dynamic&lt;/code&gt; 构造并向k8s api发送动态请求&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;plugin/pkg/client/auth&lt;/code&gt; 鉴权相关&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;transport&lt;/code&gt; 建立连接相关&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tools/cache&lt;/code&gt; 客户端缓存的实现，主要在编写controller时用到，本文的介绍重点&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;由于client-go根据k8s服务端版本的不同区分了两个主要版本，具体安装时需要根据实际情况进行区分，具体安装方式可以参考这个&lt;a href=&quot;https://github.com/kubernetes/client-go/blob/master/INSTALL.md&quot;&gt;链接&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;k8s中的controller&quot;&gt;k8s中的controller&lt;/h2&gt;

&lt;p&gt;controller在k8s中是一个重要的概念，一个controller主要负责追踪某个（或多个）资源的状态，并努力使其追踪的资源的状态达到预期的值。controller内部的逻辑主体是一个无限循环，称为“控制循环”。控制循环的大致流程如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;循环开始&lt;/li&gt;
  &lt;li&gt;获取目标资源的当前状态和预期状态&lt;/li&gt;
  &lt;li&gt;如果当前状态和预期状态有差异，尝试通过各种手段使当前状态向预期状态靠拢&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;当controller发现资源的当前状态和预期状态不同时，它可以直接操作资源使其状态发生变化，但是实际中更常见的做法是向k8s api server发送请求，触发其他事件来使资源发生变化。后者更值得考虑的原因在于，一个controller不应该完成许多工作，因为操作可能会失败，而复杂的控制循环则会使系统复杂度上升，难以维护。举个例子：k8s中的job controller，当它发现需要创建新的job实例时，并不会直接创建新的pod，而是向api server发送创建pod的请求，随后。。。&lt;/p&gt;

&lt;h2 id=&quot;informer&quot;&gt;informer&lt;/h2&gt;

&lt;p&gt;一个controller的逻辑主体就是在循环中进行”获取状态 -&amp;gt; 进行处理”的处理，所以client-go提供了从k8s api server获取资源状态的支持，也称为informer。&lt;/p&gt;

&lt;p&gt;informer指的是client-go/tools/cache包下的逻辑的代称，它的主要功能就是是帮助controller与k8s api server交互，主要包括获取资源对象的信息以及根据资源对象的变更事件触发自定义的回调。它实际上包含几个组件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;reflector: reflector主要负责监听（watch）具体的资源类型，而watch具体是通过&lt;code class=&quot;highlighter-rouge&quot;&gt;ListAndWatch&lt;/code&gt;实现。被监听的资源不限于内置的类型，也可以是自定义的资源类型。当reflector通过watch api接收到有新的资源实例被创建的通知时，它会主动通过list api获取到新的资源实例，然后将其放入到&lt;code class=&quot;highlighter-rouge&quot;&gt;Delta Fifo&lt;/code&gt;队列中。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;informer: informer主要的工作是循环地从&lt;code class=&quot;highlighter-rouge&quot;&gt;Delta Fifo&lt;/code&gt;队列中取出对象，随后通过回调传递给controller，以及交给indexer缓存下来。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;indexer: indexer主要提供索引的功能，而通常对象的索引是根据对象的label创建的。indexer通过线程安全的结构来存储对象，同时cache包下定义了默认的根据对象label生成索引的函数&lt;code class=&quot;highlighter-rouge&quot;&gt;MetaNamespaceKeyFunc&lt;/code&gt;，具体格式为&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;namespace&amp;gt;/&amp;lt;name&amp;gt;&lt;/code&gt;。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;controller中使用informer的场景&quot;&gt;controller中使用informer的场景&lt;/h2&gt;

&lt;p&gt;下图是controller在实际中通过client-go与k8s api server交互的示意图：
&lt;img src=&quot;/img/client-go-controller-interaction.jpeg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从示意图中我们可以看出client-go主要提供的两个机制：第6步的事件注册和触发机制以及第9步的根据key获取对象。同时我们可以看到，在controller中实际有两个reference：informer reference和index reference，它们分别对应前面提到的两个功能。&lt;/p&gt;

&lt;p&gt;而informer内部的流程主要包括几个关键点：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;reflector 通过list &amp;amp; watch 从api server获取资源的信息，其中初始化时通过list获取最新信息，随后依赖watch监听资源变化，同时伴随可配置的定时resync机制。&lt;/li&gt;
  &lt;li&gt;reflector 获取到的资源变更事件都通过&lt;code class=&quot;highlighter-rouge&quot;&gt;Delta Fifo&lt;/code&gt;队列传递到informer&lt;/li&gt;
  &lt;li&gt;informer 从队列中获取到事件及对应的对象时，会触发对应的controller回调并将对象传递到indexer进行索引和缓存。&lt;/li&gt;
  &lt;li&gt;controller 获取对象时，会直接从indexer返回缓存，提高性能&lt;/li&gt;
  &lt;li&gt;对象在controller内部传递时也通过key传递，而不是直接传递对象本身，在实际处理时才从indexer获取最新状态的对象&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;另外示意图中的几个组件也需要介绍一下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Resource Event Handlers: controller注册的事件回调函数，informer会在触发这些回调时将对应的资源对象也传递给controller，推荐的做法是根据对象获取其对应的key，然后将key放入work queue中（而不是直接把对象放进queue），随后再由其他组件处理。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Work queue: 用于存放在事件回调中收到的对象的key，通过队列来解耦对象的接受和处理。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Process Item: 实际处理从队列中获取的对象的逻辑，在做实际操作前往往需要从队列中获得的key从indexer reference获取资源对象。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">前言</summary></entry><entry><title type="html">TCP/IP的诞生（二）</title><link href="http://localhost:4000/2020/03/01/TCPIP2.html" rel="alternate" type="text/html" title="TCP/IP的诞生（二）" /><published>2020-03-01T00:00:00+08:00</published><updated>2020-03-01T00:00:00+08:00</updated><id>http://localhost:4000/2020/03/01/TCPIP2</id><content type="html" xml:base="http://localhost:4000/2020/03/01/TCPIP2.html">&lt;h2 id=&quot;tcpv1的提出&quot;&gt;TCPv1的提出&lt;/h2&gt;

&lt;p&gt;随着论文《&lt;a href=&quot;https://www.cs.princeton.edu/courses/archive/fall08/cos561/papers/cerf74.pdf&quot;&gt;A Protocol for Packet Network Intercommunication&lt;/a&gt;》在1974年五月发表，Vinton Cerf在1974年十二月发布了&lt;a href=&quot;https://tools.ietf.org/html/rfc675&quot;&gt;RFC675&lt;/a&gt;。在RFC中作者根据论文和其他人的建议提出了TCP的第一个版本，只不过这时候TCP还是transmission control program的缩写。根据论文的描述，TCP的目标是提供一个在不同的网络间进行数据交换的机制，而要达到这个目标TCP将目标分解为两个子问题：提供一个端到端的数据传输控制程序，以及提供一个在网络间进行寻址和路由的机制。&lt;/p&gt;

&lt;p&gt;作者在RFC中描述了TCP的地址规范、用户接口定义、与TCP配合使用的部分上层协议（包括日志、FTP等）、TCP包头协议格式、以及TCP的部分实现思路（包括三次握手、滑动窗口机制、输入输出处理、数据包分段等等）。&lt;/p&gt;

&lt;h2 id=&quot;ien2&quot;&gt;IEN2&lt;/h2&gt;

&lt;p&gt;而在1977年，TCP已经发展到v2版本，在这个时候TCP的发展出现了重要的转折，也就是Jon Postel在1977年八月发表的一些针对TCP的评论，后来被称为&lt;a href=&quot;https://www.rfc-editor.org/ien/ien2.txt&quot;&gt;IEN2&lt;/a&gt;（Internet Engineering Note number 2）。在这个评论中Jon指出:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;We are screwing up in our design of internet protocols by violating the principle of layering. Specifically we are trying to use TCP to do two things: serve as a host level end to end protocol, and to serve as an internet packaging and routing protocol. These two things should be provided in a layered and modular way. I suggest that a new distinct internetwork protocol is needed, and that TCP be used strictly as a host level end to end protocol.&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;简单来说，Jon Postel认为TCP想做的事情太多，应该把TCP要解决的两个问题拆分成两层协议，显然这个建议被TCP的发表者采纳了，这促使了TCP/IP协议栈的诞生，TCP（transmission control program）被拆分成了两个协议：IP（internet protol）和TCP（transmission control protocol），它们分别工作在TCP/IP协议栈的第三层和第四层（OSI七层协议中）。然后TCP更名为”TCP/IP”。这项拆分工作最开始从1978年的TCPv3版本中体现，最终发表于1980的TCPv4则是第一个正式的标准，而相应的RFC则由Jon Postel发布于1981年。&lt;/p&gt;

&lt;p&gt;此外，Jon Postel还在IEN2中简要描述了拆分出来的寻址协议（作者称之为Internet Hop Protocol）所需的各个字段以及格式（identifier、TOS、fragmentation，其中一个比较重要的就是TOS（type of service）。TOS字段标识了上层协议希望IP协议如何处理其携带的数据，这主要是为了而支持不同上层协议对传输可靠性以及速度等各方面的不同要求。IEN2中并没有详细描述TOS的规范和定义，而是在1981年的&lt;a href=&quot;https://tools.ietf.org/html/rfc791&quot;&gt;RFC791&lt;/a&gt;中对TOS进行了更详细的描述和规范。&lt;/p&gt;

&lt;h2 id=&quot;tcpv4&quot;&gt;TCPv4&lt;/h2&gt;

&lt;p&gt;在1981年发表的TCP/IP v4的各个RFC中详细描述了第一个正式版TCP/IP协议栈的规范。其中提出了我们后来都很熟悉的四层TCP/IP协议栈，TCP协议工作在第三层（传输层），同样在第三层的还有UDP协议，而IP协议工作在第二层（网络层），同时还有一个ICMP协议。ICMP协议的全称是互联网控制消息协议（Internet Control Message Protocol），它实际上运行于IP协议之上，主要用于在IP协议中发送控制消息。&lt;img src=&quot;/img/protocol_relationships.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而除了TCP/IP协议栈之外，还有一个OSI七层模型，网上有很多比较两者的资料，这里就不详细列举了。而值得注意的是OSI七层模型实际上只是一个参考标准，实际上我们遇到的更多的是TCP/IP四层协议。&lt;/p&gt;

&lt;h3 id=&quot;ip协议&quot;&gt;IP协议&lt;/h3&gt;

&lt;p&gt;IP协议的介绍位于RFC791，RCF791描述了IP协议的适用场景：为通过不同网络连接的两台主机交换二进制数据包（称为internet datagram）提供必要的功能。IP协议并不提供数据传输可用性的保障，也不提供流量控制等端到端通信的常用功能（这些功能由TCP协议提供）。同时也规范了我们现在熟悉的IP数据包头的格式。&lt;/p&gt;

&lt;p&gt;为了进行网络间数据传输，IP协议主要提供两个重要功能：addressing和fragmentation。在寻址方面，有三个重要的概念：name、address以及routes。IP协议主要面向的是address，而name和routes则主要交给上层和下层协议处理。&lt;/p&gt;

&lt;p&gt;RFC中提出了IP地址的规范：由固定长度（32bit）的整数标识，其中IP地址由两个部分组成：标识网络的网络段和标识主机的主机段，而根据地址前几位的数值不同，IP地址分为ABC三类。&lt;/p&gt;

&lt;p&gt;由于数据包在不同网络间传递，很可能因为某些网络的限制不得不将数据包分割成不同的分段，RFC中详细描述了分段功能所需的协议支持，主要包括数据包标识（identification）、分段flag（DF和MF）以及fragment offset字段。&lt;/p&gt;

&lt;p&gt;IP数据报的包头中有一个TTL字段，标识数据包存活的剩余时间，其目的是为了可以中间设备可以丢弃过期的数据包。这个字段最初的设计是表示剩余存活的秒数，而现在已经变成表示数据包还能经过多少个路由器转发，俗称hop数。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TOS&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;RFC中详细介绍了TOS字段的组成和具体的取值，支持上层协议自行指定数据的优先级、延迟、吞吐和可靠性方面的要求，具体格式这里就不再列举了。&lt;/p&gt;

&lt;h3 id=&quot;tcp协议&quot;&gt;TCP协议&lt;/h3&gt;

&lt;p&gt;TCP协议所在的RFC是RFC793，其中描述了TCP协议的目标是为由不同网络连接的主机提供进程间可靠通信。RFC中规范了现在我们熟悉的TCP包头格式，寻址相关信息只保留了源端口和目标端口，也去掉了fragmentation的相关信息，更加专注于可靠传输本身的实现。&lt;/p&gt;

&lt;h3 id=&quot;udp协议&quot;&gt;UDP协议&lt;/h3&gt;

&lt;p&gt;UDP的正式规范也随着TCP/IP协议族的提出一起发表，具体位于RFC768。RFC中描述了UDP运行在IP协议之上，提供了简单的消息发送和接受机制而不需要其他不必要的准备步骤，因此UDP也不保证消息的顺序以及可靠性。&lt;/p&gt;

&lt;p&gt;UDP和TCP协议都运行在TCP/IP协议栈的第三层，也是现在人们最熟知的两个传输层协议。具体的对比这里就不做介绍了。&lt;/p&gt;

&lt;h2 id=&quot;其他&quot;&gt;其他&lt;/h2&gt;

&lt;p&gt;TCP/IP协议并不是如今唯一的网络协议，但绝对是最重要的那个。TCP/IP的成功可以说是不可复制的，其中也有很多不同原因，包括技术上的（充分考虑了路由的复杂、扩展性等）和历史原因（趁着互联网的东风、开放的标准制定和开发流程等）。&lt;/p&gt;</content><author><name></name></author><summary type="html">TCPv1的提出</summary></entry><entry><title type="html">TCP/IP的诞生</title><link href="http://localhost:4000/2020/02/16/TCPIP.html" rel="alternate" type="text/html" title="TCP/IP的诞生" /><published>2020-02-16T00:00:00+08:00</published><updated>2020-02-16T00:00:00+08:00</updated><id>http://localhost:4000/2020/02/16/TCPIP</id><content type="html" xml:base="http://localhost:4000/2020/02/16/TCPIP.html">&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;简单总结了《&lt;a href=&quot;https://www.cs.princeton.edu/courses/archive/fall08/cos561/papers/cerf74.pdf]&quot;&gt;A Protocol for Packet Network Intercommunication&lt;/a&gt;》，这篇论文由 VINTON G. CERF 和 ROBERT E. KAHN 发表于 1974 年，论文主要论述了如何在不同的系统之间进行通信。论文中提出的机器之间的寻址以及传输控制协议等等，可以说是奠定了整个以 TCP/IP 为核心的互联网的基础。&lt;/p&gt;

&lt;h2 id=&quot;问题的引入以及各个概念的提出&quot;&gt;问题的引入以及各个概念的提出&lt;/h2&gt;

&lt;p&gt;在计算机诞生之初，科学家们为了让计算机之间能够互相交换信息制定了用于交换信息的网络协议，并将多台计算机连接到同一个内部网络中。而在这个过程中诞生了很多种不同的网络协议以及大大小小的计算机内部网络，论文的两个作者的目标就是提出一个能够让使用不同网络协议的各个计算机内部网络能够互相交换信息。&lt;/p&gt;

&lt;h3 id=&quot;网关的概念与作用&quot;&gt;网关的概念与作用&lt;/h3&gt;

&lt;p&gt;为了在不同系统之间交互，引入中间层实际上是一个很常见的设计了。论文中提出在不同计算机网络之间通过“网关”连接，而网关主要的功能包括两个：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;兼容两端的网络协议：为了在运行不同协议的网络间交换和传递信息，必须要有一个“翻译”的角色存在，它需要了解两端的协议并支持互相转换。&lt;/li&gt;
  &lt;li&gt;网络间寻址：为了把消息从一个网络内部的一台主机投递到另一个网络内部的另一台主机，网关需要根据发送端主机给定的地址确定目标主机所在的网络以及其在网络中的地址，如果目标主机所在的网络和自身连接，则根据其在网络中的地址投递，如果目标主机所在的网络不和自身连接，网关就需要把数据重新组织成下游网络能理解的格式并递归地传递到下一个网关。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;协议头&quot;&gt;协议头&lt;/h3&gt;

&lt;p&gt;为了使网关能够明确消息的源主机和目标主机，寻址相关的信息就必须随着数据一起传输，这就要求数据传输过程中必须添加一些额外的“控制”相关的信息，论文中称为“internetwork header”，包括：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;源地址和目标地址&lt;/li&gt;
  &lt;li&gt;序列号和字节数&lt;/li&gt;
  &lt;li&gt;flag 部分，主要用于传递一些特定的控制信息，比如 SYN、FIN 等&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;从这就提出了 TCP/IP 协议中各个字段的设计的雏形，在随后的 RFC 中有了更明确的定义和规范。&lt;/p&gt;

&lt;h3 id=&quot;数据分片&quot;&gt;数据分片&lt;/h3&gt;

&lt;p&gt;一个可以预见的问题就是网关在不同网络之间传递数据时，可能会将一个完整的数据包分割成几个小的数据包，因为不同网络中能够处理的数据包的尺寸可能会不同。这就需要目标主机在接收到被分割过后的数据包时有能力将其重新组装回来。&lt;/p&gt;

&lt;p&gt;而一旦发生数据分片，更多的问题就会随之产生，为了保证可靠传输，则需要更多的额外机制。&lt;/p&gt;

&lt;h2 id=&quot;从进程间通信到-tcp&quot;&gt;从进程间通信到 TCP&lt;/h2&gt;

&lt;p&gt;论文中基于进程间通信的场景提出了传输控制程序（transmission control program, TCP)的概念。假设每台主机内部都存在一个 TCP，进程间的数据发送和接收都经过 TCP 来处理。进程和 TCP 之间交换的都是完整的数据，而 TCP 则有可能在内部将数据分割成若干个分段（Segment)，同样是因为接收端可能会限制单次数据传输的最大尺寸（与网关的数据分片场景类似）。&lt;/p&gt;

&lt;p&gt;这种场景下，就要求 TCP 支持多路复用。TCP 会接收来自不同发送端的数据分段，并将各个分段发送到不同的接收端。为了区别同一台主机下的各个发送端和接收端，数据分段同样也需要附加一些额外的控制信息，论文中称为“process header”，这个信息最终演化成了后来的端口（port）。&lt;/p&gt;

&lt;h3 id=&quot;地址的格式&quot;&gt;地址的格式&lt;/h3&gt;

&lt;p&gt;随后论文中提出了完整的地址的格式，为了在不同网络间定位一个进程，一个 TCP 地址应包括三个部分：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;网络标识符：标识主机所在的网络，网关能够根据这个信息决定是将数据直接送到目标主机还是继续转发到其他网络&lt;/li&gt;
  &lt;li&gt;主机标识符：标识一个网络内部的一台主机&lt;/li&gt;
  &lt;li&gt;端口标识符：标识一台主机内部的一个进程&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;数据包的格式&quot;&gt;数据包的格式&lt;/h3&gt;

&lt;p&gt;因为进程间传递的消息可能会在传输途中被分割成若干个分段，所以一个分段应该包括以下信息：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;源端口和目标端口&lt;/li&gt;
  &lt;li&gt;窗口大小和 ack&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;序列号机制&quot;&gt;序列号机制&lt;/h2&gt;

&lt;p&gt;TCP 接收端进行分端重组时，需要知道每个分段的序列号。分段的序列号必须是单调递增（或者递减）的，因为接收端需要根据序列号来判断收到的分段是否发生了失序、重复或者丢失。很显然，序列号不可能是无限递增的，而有限的序列号则会导致接收端可能无法判断一个数据包是重传的还是新的，这个问题可以通过引入接收窗口（滑动窗口）来解决。&lt;/p&gt;

&lt;p&gt;根据设计，每个分段都需要分配序列号，而关于序列号的分配，论文中提出了一个方法：假设网络两端的进程交换的是一个无限长的字节流（所以 TCP 连接是面向字节流的连接），而每个字节都分配一个序号，其序号就是它相对于流的最开端的位置。当 TCP 创建新的分段时，则将其携带的数据的第一个字节的序号作为整个分段的序号，同时将携带的字节数也设置到协议头中。&lt;/p&gt;

&lt;h3 id=&quot;分段重传与重复检测&quot;&gt;分段重传与重复检测&lt;/h3&gt;

&lt;p&gt;为了尽可能做到可靠传输，论文中引入了超时重传和确认机制。当数据包发出后一定时间内没有收到确认，发送端会重新发送这个分段。&lt;/p&gt;

&lt;p&gt;而接收端维护一个接收窗口，在收到分段时返回预期下一次收到的分端序列号作为确认，同时更新自身的接收窗口。而窗口的初始尺寸则通过建立连接时两边协商确定。&lt;/p&gt;

&lt;h2 id=&quot;操作实践&quot;&gt;操作实践&lt;/h2&gt;

&lt;p&gt;论文中还针对实际实践做了简单的建议，包括用缓冲处理输入输出，用户进程如何与 TCP 交互等等。&lt;/p&gt;

&lt;p&gt;TCP 应如何处理输入/输出的数据做了简单的建议。当收到数据时，做完了必要的校验之后就可以将数据放入缓冲。当接收缓冲满时，可以将接收到的数据丢弃同时不发送确认，这样依赖发送端就会重传。当发送数据时，可以维护一个小的发送缓冲，因为发送进程的缓冲会持有完整的数据。&lt;/p&gt;

&lt;p&gt;当用户进程需要发送数据时，可以先向待发送的数据插入控制信息（transmit control block，TCB），然后通过指针传递给 TCP；同理要接收数据时，可以先创建好对应的接收缓冲以及控制信息（receive control block， RCB），然后传递给 TCP。&lt;/p&gt;

&lt;p&gt;论文中提到，如果出现缓冲不够用时，可以直接丢弃数据包等待重传，没有着重描述拥塞控制相关的部分，直到 1986 年，从 LBL 到 UC Berkeley 的网络吞吐因为拥塞出现了从 32Kbps 到 40bps 的急剧下降，Van Jacobson 在 1988 年发表论文《Congestion Avoidance and Control》，才对 TCP 的拥塞控制做出了进一步的完善。&lt;/p&gt;

&lt;h3 id=&quot;连接的概念&quot;&gt;连接的概念&lt;/h3&gt;

&lt;p&gt;论文中提出：当双方都准备好进行数据交互时，双方就建立了连接。但有可能直到真正进行数据交互时，才能真正确认连接是否已经建立。&lt;/p&gt;

&lt;p&gt;而要双方建立连接，则需要几个要素：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;地址，至少有一方能够通过地址定位另一方&lt;/li&gt;
  &lt;li&gt;TCP 控制信息，包括起始序号、窗口大小等，否则双方无法确认接收到的数据是否有意义&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;握手和挥手&quot;&gt;握手和挥手&lt;/h4&gt;

&lt;p&gt;要在两个进程间创建连接，就必须确定与进程关联的端口，而很显然一台主机的端口不能是无限的，也就是说端口会被复用。所以为了保证连接状态的正确，在双方交换数据之前需要进行初始化和校验等工作，也就是进行握手。&lt;/p&gt;

&lt;p&gt;为了发送或者接收数据，TCP 必须先初始化各种控制信息（TCB 和 RCB、窗口等），所以发送端发送的第一个数据包应有特殊的标记，同时携带一些需要协商的控制信息（比如窗口大小），这样才能触发接收端的控制信息初始化（也就是 SYN 请求）。接收端可以在接收到初始化请求之后进行校验，决定是否接收这个请求。因此接收端应该明确表明它是否愿意接收某个端口上的数据请求，也就是后来的 listen 某个端口。&lt;/p&gt;

&lt;p&gt;当发送端决定不再发送数据之后，需要发送一个携带特殊标记的请求标识将连接关闭（也就是 FIN 请求），为了确保两端都明确知道连接要进行关闭，接收端也要返回一个特殊请求作为确认，也就是进行挥手。当接收端根据控制信息判断所有的数据接收完毕之后，连接就关闭了。&lt;/p&gt;</content><author><name></name></author><summary type="html">前言</summary></entry><entry><title type="html">JVM G1垃圾回收</title><link href="http://localhost:4000/2020/01/12/G1.html" rel="alternate" type="text/html" title="JVM G1垃圾回收" /><published>2020-01-12T00:00:00+08:00</published><updated>2020-01-12T00:00:00+08:00</updated><id>http://localhost:4000/2020/01/12/G1</id><content type="html" xml:base="http://localhost:4000/2020/01/12/G1.html">&lt;h2 id=&quot;简介&quot;&gt;简介&lt;/h2&gt;

&lt;p&gt;G1（Garbage-First）垃圾回收器是在jdk7版本开始被引进的，它的特性在于能够尽可能的满足用户对停顿时间的要求同时还保持较高的吞吐。G1的定位是取代CMS，相比CMS，G1能够更有效的避免碎片化，同时可以让用户指定预期的停顿时间。&lt;/p&gt;

&lt;h2 id=&quot;g1的机制&quot;&gt;G1的机制&lt;/h2&gt;

&lt;p&gt;G1同样是分代的垃圾回收，但是不同的是G1把整个堆分成了大小相等的块（称为region），每个region可以被分配为不同的角色（young、eden、old等等），这意味着不同代的内存大小是不固定的，可以灵活地调整。&lt;/p&gt;

&lt;p&gt;G1会在jvm启动时确定region size（最小1M，最大32M），通常G1会尽量把堆分为2048个相同大小的region，具体的region size由此计算，也可以在jvm参数里显示指定。不同的region会被分配到不同的逻辑角色，比如eden/old，同一个逻辑角色的不同region也不一定是连续的。&lt;/p&gt;

&lt;p&gt;G1的运行机制与CMS类似，G1会进行并发的全局标记来判断对象的存活与否，在标记结束后，G1就能得知哪些region中的垃圾最多，然后就先回收这部分region，这就是G1的名字的由来。G1采用了一个停顿时间预测的模型来尽可能满足用户指定的停顿时间，根据用户指定的停顿时间来选择要回收哪些region。&lt;/p&gt;

&lt;p&gt;G1在进行垃圾回收时采用的是复制算法，G1会把各个region中残留的存活对象复制到单独的region中，这样在回收过程中就完成了内存的整理。为了降低复制过程中停顿的时间，整个复制过程是并行的，而CMS并不会进行内存整理，ParallelOld则是会直接整理整个堆，显然会明显增加停顿时间。&lt;/p&gt;

&lt;h2 id=&quot;g1的各个阶段&quot;&gt;G1的各个阶段&lt;/h2&gt;

&lt;h3 id=&quot;young-gc&quot;&gt;young gc&lt;/h3&gt;

&lt;p&gt;发生young gc时，存活的对象被复制到survivor区，如果对象的年龄超过阈值，那么会把它晋升到old区。整个young gc过程中是STW的，同时也会重新计算出下一次GC时的eden区和survivor区的大小，计算过程中也会考虑用户指定的目标停顿时间。因为region的设计，要调整各个分区的大小实际上非常容易。&lt;/p&gt;

&lt;h3 id=&quot;concurrent-marking-cycle&quot;&gt;concurrent marking cycle&lt;/h3&gt;

&lt;p&gt;并发标记是G1中的一个重要阶段，这个阶段包括若干个步骤，通过并发标记来收集各个region的使用情况等信息，协助达到用户指定的停顿时间。&lt;/p&gt;

&lt;h4 id=&quot;initial-markstw&quot;&gt;initial mark（STW）&lt;/h4&gt;

&lt;p&gt;这一步是和young gc一起顺带着执行的，首先标记出gc roots直接可达的对象，&lt;/p&gt;

&lt;h4 id=&quot;root-region-scanning&quot;&gt;root region scanning&lt;/h4&gt;

&lt;p&gt;young gc过后，survivor中的对象都被标记为root region，这时扫描由survivor区直接可达的old区并标记。这一阶段必须在新一轮的young gc前执行完毕。如果这时又需要young gc，那么会等待扫描完成才会进行。&lt;/p&gt;

&lt;h4 id=&quot;concurrent-marking&quot;&gt;concurrent marking&lt;/h4&gt;

&lt;p&gt;扫描整个堆，标记存活的对象，整个阶段是与应用程序并行的，可能被young gc打断。这个阶段下会不断从扫描栈取出引用，递归地扫描整个堆里的对象图。每扫描到一个对象就会对其标记，并将其字段压入扫描栈。重复扫描过程直到扫描栈清空。过程中还会扫描SATB write barrier所记录下的引用。&lt;/p&gt;

&lt;h4 id=&quot;remarkstw&quot;&gt;remark（STW）&lt;/h4&gt;

&lt;p&gt;在完成并发标记后，每个Java线程还会有一些剩下的SATB write barrier记录的引用尚未处理。这个阶段就负责把剩下的引用处理完。同时这个阶段也进行弱引用处理（reference processing）。注意这个暂停与CMS的remark有一个本质上的区别，那就是这个暂停只需要扫描SATB buffer，而CMS的remark需要重新扫描mod-union table里的dirty card外加整个根集合，而此时整个young gen（不管对象死活）都会被当作根集合的一部分，因而CMS remark有可能会非常慢。&lt;/p&gt;

&lt;h4 id=&quot;cleanupstw&quot;&gt;cleanup（STW）&lt;/h4&gt;

&lt;p&gt;这阶段会清理各个region，同时更新Rset，如果有空的region就把它释放掉。&lt;/p&gt;

&lt;h4 id=&quot;copyingstw&quot;&gt;copying（STW）&lt;/h4&gt;

&lt;p&gt;把存活的对象拷贝到新的region。&lt;/p&gt;

&lt;h3 id=&quot;g1的老年代回收总结&quot;&gt;G1的老年代回收总结&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;concurrent mark阶段
    &lt;ul&gt;
      &lt;li&gt;存活对象的信息是在运行时并发地计算的&lt;/li&gt;
      &lt;li&gt;在复制阶段，G1会根据每个region内存活对象的信息确定哪些region优先被回收&lt;/li&gt;
      &lt;li&gt;没有类似CMS中的sweep过程（因为是直接evacuate整个region）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;remark阶段
    &lt;ul&gt;
      &lt;li&gt;SATB算法，（据说）会比CMS更快&lt;/li&gt;
      &lt;li&gt;空的region会被直接回收&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;cleanup/copying阶段
    &lt;ul&gt;
      &lt;li&gt;young和old区同时进行回收&lt;/li&gt;
      &lt;li&gt;会根据情况选择特定的old区region进行回收&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;混合gc&quot;&gt;混合GC&lt;/h3&gt;

&lt;p&gt;在经历了一个完整的标记周期过后，G1会在下一次young gc的时刻转换成混合gc，混合gc下，G1可能会把一部分old区的region加入Cset中，利用young gc的算法清理一部分old region。当G1回收了足够多的old region，又会重新回到young gc，直到下一次并发标记周期完成。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;G1的目标是要代替CMS，它把整个堆空间划分成了不同的region来进行管理，使得分配和回收更加灵活。G1的主要活动包括young gc、mixed gc以及并发标记，它会根据用户指定的目标停顿时间来决定要对哪些内存区域进行回收。&lt;/p&gt;

&lt;h3 id=&quot;remembered-sets和collection-sets&quot;&gt;Remembered Sets和Collection Sets&lt;/h3&gt;

&lt;p&gt;RSet用于记录指向某个region的引用，每个region对应一个RSet，这个数据结构里记录了哪些其他region包含了指向这个region的对象的引用。CSet记录了GC过程中会被回收的region，CSet中存活的对象在GC过程中都会被复制到新的空的region。Rset和Cset都是为了帮助GC而产生的额外的数据结构。&lt;/p&gt;

&lt;p&gt;G1的heap与HotSpot VM的其它GC一样有一个覆盖整个heap的card table。逻辑上说，G1的RSet是每个region有一份。这个RSet记录的是从别的region指向该region的card。所以这是一种“points-into”的Remembered Set。用card table实现的Remembered Set通常是points-out的，也就是说card table要记录的是从它覆盖的范围出发指向别的范围的指针。以分代式GC的card table为例，要记录old -&amp;gt; young的跨代指针，被标记的card是old gen范围内的。&lt;/p&gt;

&lt;p&gt;G1则是在points-out的card table之上再加了一层结构来构成points-into RSet：每个region会记录下到底哪些别的region有指向自己的指针，而这些指针分别在哪些card的范围内。这个RSet其实是一个hash table，key是别的region的起始地址，value是一个集合，里面的元素是card table的index。&lt;/p&gt;

&lt;h3 id=&quot;satb算法&quot;&gt;SATB算法&lt;/h3&gt;

&lt;p&gt;G1在concurrent mark阶段使用了SATB算法来避免对象的漏标记，而SATB是snapshot at the beginning的缩写。简单来说，SATB的思路就是认定在GC开始时存活的对象就是存活的，此时整个堆内的所有对象形成一个快照（snapshot）；同时认定在GC过程中新产生的对象也都是存活的，而剩下的不可达的对象则都是垃圾了。&lt;/p&gt;

&lt;p&gt;而G1是如何确定哪些对象是在GC开始后新产生的呢，这依赖两个指针：prevTAMS和nextTAMS。TAMS是top at mark start的缩写，这里就要再介绍一下region的几个指针了：
&lt;img src=&quot;/img/g1_region.webp&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;[bottom, top)部分为该region中已经使用的部分&lt;/li&gt;
  &lt;li&gt;[top, end)部分为该region中未使用的部分&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在每次concurrent mark开始时，将当前top赋值给nextTAMS，那么在concurrent mark过程中，该region上新分配的对象都落在nextTAMS和top之间，G1保证这部分对象都不会被漏标，默认都是存活的。&lt;/p&gt;

&lt;p&gt;当concurrent mark结束时，将当前的nextTAMS赋值给prevTAMS，同时根据mark的结果，将[bottom, prevTAMS]之间的对象的存活信息保存为一个bitmap，后续就可以通过这个bitmap确定对应的对象是否存活了。&lt;/p&gt;

&lt;p&gt;由于对象的存活标记是和应用程序并发执行的，应用程序完全有可能在标记过程中修改对象的引用，所以为了避免漏标记，G1使用了write barrier。write barrier是指在”对引用类型字段赋值”这个动作前后的一个拦截，可以在赋值的前后进行额外的工作。在赋值前的部分的write barrier叫做pre-write barrier，在赋值后的则叫做post-write barrier，G1则使用了pre-write barrier。为了避免漏标，G1会在每次引用赋值前把这个引用指向的旧的值也进行递归地标记，并默认其为存活，这样就不会漏掉任何一个snapshot中的对象了。当这个旧的值实际上不再是存活对象时，它实际上也就成为了浮动垃圾，只能留到下一轮垃圾回收了。&lt;/p&gt;

&lt;p&gt;可以看出，上面提到的barrier中的工作实际上都是在应用程序的线程中完成的。为了尽量减少write barrier对性能的影响，G1将一部分原本要在barrier里做的事情挪到别的线程上并发执行。实现这种分离的方式就是通过logging形式的write barrier：应用程序只在barrier里把要做的事情的信息记（log）到一个队列里，然后另外的线程从队列里取出信息批量完成剩余的动作。&lt;/p&gt;

&lt;p&gt;以SATB write barrier为例，每个Java线程有一个独立的、定长的SATBMarkQueue，应用程序线程在barrier里只把old_value压入该队列中。一个队列满了之后，它就会被加到全局的SATB队列集合SATBMarkQueueSet里等待处理，然后给对应的Java线程换一个新的、干净的队列继续执行下去。&lt;/p&gt;

&lt;p&gt;concurrent mark会定期检查全局SATB队列集合的大小。当全局集合中队列数量超过一定阈值后，concurrent marker就会处理集合里的所有队列：把队列里记录的每个oop都标记上，并将其引用字段压到标记栈（marking stack）上等后面做进一步标记。&lt;/p&gt;</content><author><name></name></author><summary type="html">简介</summary></entry><entry><title type="html">TCP BBR v2.0</title><link href="http://localhost:4000/2020/01/05/TCP-BBRv2.html" rel="alternate" type="text/html" title="TCP BBR v2.0" /><published>2020-01-05T00:00:00+08:00</published><updated>2020-01-05T00:00:00+08:00</updated><id>http://localhost:4000/2020/01/05/TCP%20BBRv2</id><content type="html" xml:base="http://localhost:4000/2020/01/05/TCP-BBRv2.html">&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;上次简单介绍了BBR的特点和基本的实现思路，但是BBR并不就是完美的吊打一切其他算法的存在。在2018年7月的时候google发布了BBR的相关改进的计划，目前已经有BBR v2 alpha版本的试用文档，正式版尚未发布。所以借助BBR v2的更新内容简单总结一下BBR的一些不足或者缺点。&lt;/p&gt;

&lt;h2 id=&quot;bbr-v2的计划更新内容&quot;&gt;BBR v2的计划更新内容&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;改进与其他算法共存时的公平性：调整BBR探测带宽时的时间来和CUBIC/Reno共存&lt;/li&gt;
  &lt;li&gt;降低排队压力（丢包和排队延时），在计算以下指标时将丢包和ECN考虑在内
    &lt;ul&gt;
      &lt;li&gt;in-flight 数据的安全范围&lt;/li&gt;
      &lt;li&gt;退出STARTUP的时机&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;加快min_rtt的收敛：增加PROBE_RTT的频率&lt;/li&gt;
  &lt;li&gt;降低PROBE_RTT时的极端性&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;计算in-flight数据大小的新模型&quot;&gt;计算in-flight数据大小的新模型&lt;/h3&gt;

&lt;p&gt;v2版本使用新的模型来计算in-flight数据的大小范围，其中包含三个参数：inflight_lo、inflight_high、inflight_prob。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;inflight_lo：基于丢包和ECN信号计算出来的in-flight数据包最小值&lt;/li&gt;
  &lt;li&gt;inflight_hi：出现丢包和ECN信号前的in-flight数据包最大值&lt;/li&gt;
  &lt;li&gt;inflight_prob：探测带宽时超过inflight_hi的增量&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;startup阶段&quot;&gt;STARTUP阶段&lt;/h3&gt;

&lt;p&gt;BBR v1中，STARTUP会持续增加发送速度，直到探测到的最大带宽趋于平稳然后退出。但是这个阶段并不会把丢包考虑在内，所以在STARTUP阶段可能会出现丢包严重的现象。v2版本中在STARTUP阶段的退出条件中增加了一项：当发现丢包或者ECN时，也会提前退出STARTUP阶段，同时更新inflight_hi变量。STARTUP阶段模式的另一个修改是将拥塞窗口增益由2.89改为2，这个改动反而会加重因为ACK聚合而导致失速问题，而BBR对此的解决方案则是BBR Extra-CWND，这里还没有太弄明白，后续再详细了解下。&lt;/p&gt;

&lt;h3 id=&quot;drain阶段&quot;&gt;DRAIN阶段&lt;/h3&gt;

&lt;p&gt;DRAIN阶段会降低发送速度，尝试清空中间设备的缓存，直到inflight数据少于预估的带宽（”drain to target”）。这个阶段并没有改动。&lt;/p&gt;

&lt;h3 id=&quot;probe_bw阶段&quot;&gt;PROBE_BW阶段&lt;/h3&gt;

&lt;p&gt;v2版本中PROBE_BW分为三个阶段：cruise（平稳）、up（探测更多带宽）和down（收敛到可用带宽）。同时为了与其他基于丢包的算法共存，PROBE_BW周期的时长不再是8个min_rtt，而是min(T_bbr, T_reno)，T_bbr是时间范围为2-5s，T_reno是min（BDP, 50）* RTT。BDP过期时间不在是过去的十轮，而是更长的2个PROBE_BW周期时长。&lt;/p&gt;

&lt;h4 id=&quot;cruise&quot;&gt;cruise&lt;/h4&gt;

&lt;p&gt;v1版本中平稳阶段会使inflight保持在一个恒定的值，而v2版本则会预留一部分空间（让给其他连接），使inflight在inflight_lo和inflight_hi之间，并且会根据丢包和ECN事件减小inflight_lo的值。&lt;/p&gt;

&lt;h4 id=&quot;up&quot;&gt;up&lt;/h4&gt;

&lt;p&gt;v1版本中在探测更多带宽时简单地增加1/4的发送速度，而v2版本中采用了指数增长的方式，先慢后快地探测可用带宽，直到出现丢包或者新的可用带宽大于预估带宽的1.25倍，同时会在出现丢包时更新inflight_hi。&lt;/p&gt;

&lt;h4 id=&quot;down&quot;&gt;down&lt;/h4&gt;

&lt;p&gt;v1版本中每次收敛只会降低1/4，而v2版本中则直接采用了”drain to target”的策略，会直接收敛到预估带宽。后续这个阶段可能会直接代替DRAIN阶段。&lt;/p&gt;

&lt;h3 id=&quot;probe_rtt阶段&quot;&gt;PROBE_RTT阶段&lt;/h3&gt;

&lt;p&gt;v1版本中，在进入PROBE_RTT阶段时，为了探测min_RTT会直接将窗口降到4个，同时为了尽量减小PROBE_RTT带来的吞吐降低的影响，PROBE_RTT的频率比较低（10s一次）。这就使得BBR在收敛时的速度很慢（通常需要20~30s）。v2版本中对此作了两点改进：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;窗口降低的更温和，不再是4而是0.75*BDP&lt;/li&gt;
  &lt;li&gt;探测得更频繁，不再是10s一次而是2.5s一次&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过这样的调整，使得PROBE_RTT不再那么激进，也可以有效提高收敛的速度。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;BBR v2版本总的来说更加保守了，把丢包和ECN加入了考虑范围，同时还考虑到与其他算法共存时的情况。&lt;/p&gt;

&lt;p&gt;附：ECN即显式拥塞通知（Explicit Congestion Notification），它可以通过显式的通知来告知网络两端发生了拥塞。具体可以参考维基百科。&lt;/p&gt;</content><author><name></name></author><summary type="html">前言</summary></entry><entry><title type="html">TCP BBR简介</title><link href="http://localhost:4000/2019/12/29/TCP-BBR.html" rel="alternate" type="text/html" title="TCP BBR简介" /><published>2019-12-29T00:00:00+08:00</published><updated>2019-12-29T00:00:00+08:00</updated><id>http://localhost:4000/2019/12/29/TCP%20BBR</id><content type="html" xml:base="http://localhost:4000/2019/12/29/TCP-BBR.html">&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;TCP BBR是由来自Google的 Neal Cardwell 和 Yuchung Cheng 发表的新的TCP拥塞控制算法，目前已经在Google内部大范围使用并且随着linux 4.9版本正式发布。不过我个人第一次接触到BBR却是在搭建上网工具的时候，个人体感开启了BBR之后的访问速度明显提高，在这里主要总结一下从网上了解到的相关知识。&lt;/p&gt;

&lt;h2 id=&quot;bbr简介&quot;&gt;BBR简介&lt;/h2&gt;

&lt;p&gt;BBR的名称实际上是bottleneck bandwith and round-trip propagation time的首字母缩写，表明了BBR的主要运行机制：通过检测带宽和RTT这两个指标来进行拥塞控制。 BBR算法的主要特点有以下几个：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;BBR不考虑丢包，因为丢包（在现在这个时代）并不一定是网络出现拥塞的标志了&lt;/li&gt;
  &lt;li&gt;BBR依赖实时检测的带宽和RTT来决定拥塞窗口的大小：窗口大小 = 带宽 * RTT&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;丢包不一定等于发生拥塞&quot;&gt;丢包不一定等于发生拥塞&lt;/h3&gt;

&lt;p&gt;在1988发表的论文《Congestion Avoidance and Control》中，Van Jacobson就提出丢包可以作为发生拥塞的信号，在当时的硬件性能下这个推论是成立的，后续的诸多拥塞控制算法也都是按照这个思路来实现（当然也有例外比如Vegas和Westwood），而如今网卡带宽已经从Mbps增长到了Gbps，丢包与拥塞这两者之间的关联关系也就变得微弱了。&lt;/p&gt;

&lt;p&gt;在现在网络状况下，丢包可能是由于拥塞，也有可能是因为错误。在数据中心内部，错误丢包率并不高（约在十万分之一）；而在广域网上错误丢包率则高得多。更重要的是，在有一定错误丢包率的长肥管道（带宽大、延时高的网络）中，传统的拥塞控制算法会将发送速率收敛到一个比较小的值，导致网络利用率非常低。&lt;/p&gt;

&lt;p&gt;另外，网络链路中很多设备都会有缓冲，用于吸收网络中的波动，提高转发成功率，而传统的基于丢包的拥塞控制算法感知到丢包时，这些缓冲却早已被填满了，这个问题成为bufferbloat（缓冲区膨胀）。而bufferbloat带来的影响主要有两个：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;延时会增加，同时缓冲越大延时增加得越多&lt;/li&gt;
  &lt;li&gt;共享网络瓶颈的连接较多时，可能会因为缓冲区被填满而发生丢包。但这种丢包并不意味着发生了拥塞&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;总结一句话，时代已经变了，丢包不一定等于拥塞了。&lt;/p&gt;

&lt;h3 id=&quot;基于带宽和rtt决定拥塞窗口&quot;&gt;基于带宽和RTT决定拥塞窗口&lt;/h3&gt;

&lt;p&gt;BBR既然不把丢包作为拥塞出现的信号，就需要找到其他机制来检测拥塞是否出现。Vegas算法基于时延来判断是否出现了拥塞，Westwood算法基于带宽和RTT来决定拥塞窗口的大小，但是受限于linux拥塞控制实现的原因，Westwood计算带宽和RTT的方式十分粗糙。BBR也采用了和Westwood一样的方式，但是它的作者同时改进了linux拥塞控制的实现，使得BBR能够得到更完全的控制。&lt;/p&gt;

&lt;p&gt;一个网路链路能够传输的最大吞吐取决于这条网路链路上的物理时延（Round-Trip Propagation Time，在BBR中简写为RTprop）与链路上速度最低的一段的带宽（Bottle-neck Bandwidth，在BBR中简写为BtlBw）的乘积。这个乘积叫做BDP（Bottle-neck Bandwidth Delay Production），即BDP=BltBw x RTprop，也就是将链路填满数据同时不填充中间链路设备缓冲的最大数据量。BBR追求的就是数据发送速率达到BDP这个最优点。&lt;/p&gt;

&lt;h4 id=&quot;带宽与rtt的测不准原理&quot;&gt;带宽与RTT的测不准原理&lt;/h4&gt;

&lt;p&gt;在一条网络链路上，RTprop和BtlBw实际上是互相独立的两个变量，它们都可能在对方不变的情况下增大或者减小。而要精确地测得延时的最小值，就必须保证网络设备的缓冲为空，链路上的流量越少越好，但此时的带宽就低；要测得带宽的最大值，就必须发送尽可能多的数据来把网络带宽填满，缓冲区就会有部分数据，延时就会上升；这就有点类似物理学中的不确定性原理。&lt;/p&gt;

&lt;p&gt;而BBR对于这个问题的解决方式就是取一定时间范围内的RTprop极小值与BtlBw极大值作为估计值。&lt;/p&gt;

&lt;h2 id=&quot;bbr算法的各个阶段&quot;&gt;BBR算法的各个阶段&lt;/h2&gt;

&lt;p&gt;在连接建立的时候，BBR也采用类似慢启动的方式逐步增加发送速率，然后根据收到的ack计算BDP，当发现BDP不再增长时，就进入拥塞避免阶段（这个过程完全不管有没有丢包）。在慢启动的过程中，由于几乎不会填充中间设备的缓冲区，这过程中的延迟的最小值就是最初估计的最小延迟；而慢启动结束时的最大带宽就是最初的估计的最大贷款。&lt;/p&gt;

&lt;p&gt;慢启动结束之后，为了把慢启动过程中可能填充到缓冲区中的数据排空，BBR会进入排空阶段，这期间会降低发送速率，如果缓冲区中有数据，降低发送速率就会使延时下降（缓冲区逐渐被清空），直到延时不再下降。&lt;/p&gt;

&lt;p&gt;排空阶段结束后，进入稳定状态，这个阶段会交替探测带宽和延迟。带宽探测阶段是一个正反馈系统：定期尝试增加发包速率，如果收到确认的速率也增加了，就进一步增加发包速率。具体来说，以每8个RTT为周期，在第一个RTT中，尝试以估计带宽的5/4的速度发送数据，第二个RTT中，为了把前一个RTT多发出来的包排空，以估计带宽的3/4的速度发送数据。剩下6个RTT里，使用估计的带宽发包（估计带宽可能在前面的过程中更新）。 这个机制使得BBR在带宽增加时能够迅速提高发送速率，而在带宽下降时则需要一定的时间才能降低到稳定的水平。&lt;/p&gt;

&lt;p&gt;除了带宽检测，BBR还会进行最小延时的检测。每过10s，如果最小RTT没有改变（也就是没有发现一个更低的延迟），就进入延迟探测阶段。延迟探测阶段持续的时间仅为 200 毫秒（或一个往返延迟，如果后者更大），这段时间里发送窗口固定为4个包，也就是几乎不发包。这段时间内测得的最小延迟作为新的延迟估计。也就是说，大约有2%的时间BBR会用极低的发包速率来测量延迟。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;这次主要总结了BBR的特点以及各个阶段，但是BBR并非就是完美的横扫一切的拥塞控制算法，它需要和实际的场景相结合才能发挥威力，后续再继续深入了解了。&lt;/p&gt;</content><author><name></name></author><summary type="html">前言</summary></entry></feed>